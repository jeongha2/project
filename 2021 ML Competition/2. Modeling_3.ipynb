{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "from matplotlib import font_manager, rc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "import matplotlib.font_manager as fm\n",
    "plt.rc('font', family=fm.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()) # for Windows OS user\n",
    "import math\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import klib\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sys, warnings\n",
    "if not sys.warnoptions: warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import ClassifierMixin\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "\n",
    "import datetime\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_train = pd.read_csv(os.path.abspath(\"../input\")+'/X_train.csv', encoding='cp949')\n",
    "df_x_test = pd.read_csv(os.path.abspath(\"../input\")+'/X_test.csv', encoding='cp949')\n",
    "df_y_train = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv').age\n",
    "df_y_train_mer = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv')\n",
    "y_train = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv', encoding='cp949')\n",
    "IDtrain = pd.DataFrame({'custid': df_x_train.custid.unique()})\n",
    "IDtest = pd.DataFrame({'custid': df_x_test.custid.unique()})\n",
    "\n",
    "# df = pd.concat([df_x_train, df_x_test])\n",
    "\n",
    "eda_df = pd.merge(df_x_train , df_y_train_mer, on = 'custid')\n",
    "\n",
    "ALL = pd.read_csv(os.path.abspath(\"../input\")+'/ALL피처추가060601.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.abspath(\"../input\")+'/jin_features.csv')\n",
    "dft = pd.read_csv(os.path.abspath(\"../input\")+'/jin_features_te.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainid = IDtrain.custid.to_list()\n",
    "X_train = ALL.query('custid in @trainid')\n",
    "\n",
    "testid = IDtest.custid.to_list()\n",
    "X_test = ALL.query('custid in @testid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('custid', axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop('custid', axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns2 = X_train.dtypes[X_train.dtypes != 'object'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-skew에만 처리를 하기 위해서 numeric_columns에서 skew와 non-skew로 구분\n",
    "skewlist2 = []\n",
    "numeric_columns_ns2 = []\n",
    "\n",
    "for i in numeric_columns2:\n",
    "    if 'SKEW' in i:\n",
    "        skewlist2.append(i)\n",
    "    else:\n",
    "        numeric_columns_ns2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-skew standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_columns2] = scaler.fit_transform(X_train[numeric_columns2])\n",
    "X_test[numeric_columns2] = scaler.transform(X_test[numeric_columns2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df, X_train[numeric_columns_ns2], X_train[skewlist2]], axis=1)\n",
    "\n",
    "X_test.index = dft.index\n",
    "data_te = pd.concat([dft, X_test[numeric_columns_ns2], X_test[skewlist2] ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custid</th>\n",
       "      <th>('tot_amt', '총구매액')</th>\n",
       "      <th>('tot_amt', '구매건수')</th>\n",
       "      <th>('tot_amt', '평균구매가격')</th>\n",
       "      <th>('tot_amt', '최대구매액')</th>\n",
       "      <th>('dis_amt', 'dis_sum')</th>\n",
       "      <th>('dis_amt', 'dis_mean')</th>\n",
       "      <th>('net_amt', 'net_sum')</th>\n",
       "      <th>('net_amt', 'net_mean')</th>\n",
       "      <th>('inst_mon', '평균할부개월수')</th>\n",
       "      <th>...</th>\n",
       "      <th>행사장(여성캐주얼)</th>\n",
       "      <th>행사장(여성캐쥬)</th>\n",
       "      <th>행사장(잡화)</th>\n",
       "      <th>화장품</th>\n",
       "      <th>식품팀</th>\n",
       "      <th>의류패션팀</th>\n",
       "      <th>인터넷백화점_y</th>\n",
       "      <th>잡화가용팀</th>\n",
       "      <th>방문당평균구매액</th>\n",
       "      <th>방문당평균구매상품수</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14.370545</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>11.972655</td>\n",
       "      <td>13.028055</td>\n",
       "      <td>12.067965</td>\n",
       "      <td>9.239061</td>\n",
       "      <td>14.265185</td>\n",
       "      <td>11.867296</td>\n",
       "      <td>1.339774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>1.514697</td>\n",
       "      <td>-0.346229</td>\n",
       "      <td>-0.451798</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>0.060923</td>\n",
       "      <td>0.316165</td>\n",
       "      <td>-0.391769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15.137493</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>12.368329</td>\n",
       "      <td>14.163347</td>\n",
       "      <td>12.449376</td>\n",
       "      <td>9.239061</td>\n",
       "      <td>15.067062</td>\n",
       "      <td>12.330205</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.380754</td>\n",
       "      <td>-0.346229</td>\n",
       "      <td>-0.076312</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>0.556603</td>\n",
       "      <td>1.848414</td>\n",
       "      <td>-0.391769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>14.648637</td>\n",
       "      <td>3.433987</td>\n",
       "      <td>11.247453</td>\n",
       "      <td>13.339088</td>\n",
       "      <td>11.425852</td>\n",
       "      <td>8.024971</td>\n",
       "      <td>14.607979</td>\n",
       "      <td>11.206794</td>\n",
       "      <td>1.299283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.295355</td>\n",
       "      <td>-0.310157</td>\n",
       "      <td>-0.224343</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>0.068412</td>\n",
       "      <td>-0.067703</td>\n",
       "      <td>0.702682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>13.859528</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>12.368329</td>\n",
       "      <td>13.235694</td>\n",
       "      <td>9.989711</td>\n",
       "      <td>8.603554</td>\n",
       "      <td>13.838447</td>\n",
       "      <td>12.330205</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.559885</td>\n",
       "      <td>-0.346229</td>\n",
       "      <td>-0.118915</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.559987</td>\n",
       "      <td>1.777211</td>\n",
       "      <td>0.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15.435643</td>\n",
       "      <td>3.496508</td>\n",
       "      <td>11.969913</td>\n",
       "      <td>13.432786</td>\n",
       "      <td>12.797356</td>\n",
       "      <td>9.239061</td>\n",
       "      <td>15.361476</td>\n",
       "      <td>11.895747</td>\n",
       "      <td>1.056053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>0.404504</td>\n",
       "      <td>0.134538</td>\n",
       "      <td>0.161607</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>0.736393</td>\n",
       "      <td>0.272373</td>\n",
       "      <td>-0.462554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21582</th>\n",
       "      <td>29995</td>\n",
       "      <td>16.129623</td>\n",
       "      <td>4.343805</td>\n",
       "      <td>12.278325</td>\n",
       "      <td>14.224968</td>\n",
       "      <td>12.819015</td>\n",
       "      <td>8.942196</td>\n",
       "      <td>16.093831</td>\n",
       "      <td>12.242107</td>\n",
       "      <td>1.035243</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>1.146022</td>\n",
       "      <td>-0.346229</td>\n",
       "      <td>5.361198</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>0.672847</td>\n",
       "      <td>1.224299</td>\n",
       "      <td>0.169069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>29996</td>\n",
       "      <td>14.157184</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>11.212758</td>\n",
       "      <td>12.384223</td>\n",
       "      <td>11.642207</td>\n",
       "      <td>8.697926</td>\n",
       "      <td>14.072863</td>\n",
       "      <td>11.128438</td>\n",
       "      <td>0.793231</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.601543</td>\n",
       "      <td>-0.297154</td>\n",
       "      <td>-0.476080</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.073786</td>\n",
       "      <td>-0.329461</td>\n",
       "      <td>-0.160108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>29997</td>\n",
       "      <td>14.996220</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>12.000494</td>\n",
       "      <td>14.289880</td>\n",
       "      <td>10.604132</td>\n",
       "      <td>7.608871</td>\n",
       "      <td>14.983768</td>\n",
       "      <td>11.988042</td>\n",
       "      <td>0.916291</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.622372</td>\n",
       "      <td>0.078366</td>\n",
       "      <td>-0.422218</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>0.550462</td>\n",
       "      <td>1.160984</td>\n",
       "      <td>0.988546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>29998</td>\n",
       "      <td>13.428231</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>10.863299</td>\n",
       "      <td>11.695255</td>\n",
       "      <td>9.563178</td>\n",
       "      <td>6.999071</td>\n",
       "      <td>13.407047</td>\n",
       "      <td>10.842116</td>\n",
       "      <td>1.172720</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.605709</td>\n",
       "      <td>-0.346229</td>\n",
       "      <td>-0.346326</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.496227</td>\n",
       "      <td>-0.559442</td>\n",
       "      <td>-0.312135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>29999</td>\n",
       "      <td>11.898195</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>10.799596</td>\n",
       "      <td>11.141876</td>\n",
       "      <td>8.902592</td>\n",
       "      <td>7.804251</td>\n",
       "      <td>11.846902</td>\n",
       "      <td>10.748304</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.576548</td>\n",
       "      <td>-0.346229</td>\n",
       "      <td>-0.532149</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.543726</td>\n",
       "      <td>-0.750921</td>\n",
       "      <td>-1.241194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21587 rows × 5191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       custid  ('tot_amt', '총구매액')  ('tot_amt', '구매건수')  \\\n",
       "0           0            14.370545             2.484907   \n",
       "1           2            15.137493             2.484907   \n",
       "2           3            14.648637             3.433987   \n",
       "3           4            13.859528             1.609438   \n",
       "4           5            15.435643             3.496508   \n",
       "...       ...                  ...                  ...   \n",
       "21582   29995            16.129623             4.343805   \n",
       "21583   29996            14.157184             2.995732   \n",
       "21584   29997            14.996220             3.044522   \n",
       "21585   29998            13.428231             2.639057   \n",
       "21586   29999            11.898195             1.386294   \n",
       "\n",
       "       ('tot_amt', '평균구매가격')  ('tot_amt', '최대구매액')  ('dis_amt', 'dis_sum')  \\\n",
       "0                  11.972655             13.028055               12.067965   \n",
       "1                  12.368329             14.163347               12.449376   \n",
       "2                  11.247453             13.339088               11.425852   \n",
       "3                  12.368329             13.235694                9.989711   \n",
       "4                  11.969913             13.432786               12.797356   \n",
       "...                      ...                   ...                     ...   \n",
       "21582              12.278325             14.224968               12.819015   \n",
       "21583              11.212758             12.384223               11.642207   \n",
       "21584              12.000494             14.289880               10.604132   \n",
       "21585              10.863299             11.695255                9.563178   \n",
       "21586              10.799596             11.141876                8.902592   \n",
       "\n",
       "       ('dis_amt', 'dis_mean')  ('net_amt', 'net_sum')  \\\n",
       "0                     9.239061               14.265185   \n",
       "1                     9.239061               15.067062   \n",
       "2                     8.024971               14.607979   \n",
       "3                     8.603554               13.838447   \n",
       "4                     9.239061               15.361476   \n",
       "...                        ...                     ...   \n",
       "21582                 8.942196               16.093831   \n",
       "21583                 8.697926               14.072863   \n",
       "21584                 7.608871               14.983768   \n",
       "21585                 6.999071               13.407047   \n",
       "21586                 7.804251               11.846902   \n",
       "\n",
       "       ('net_amt', 'net_mean')  ('inst_mon', '평균할부개월수')  ...  행사장(여성캐주얼)  \\\n",
       "0                    11.867296                 1.339774  ...   -0.046443   \n",
       "1                    12.330205                 1.386294  ...   -0.046443   \n",
       "2                    11.206794                 1.299283  ...   -0.046443   \n",
       "3                    12.330205                 1.386294  ...   -0.046443   \n",
       "4                    11.895747                 1.056053  ...   -0.046443   \n",
       "...                        ...                      ...  ...         ...   \n",
       "21582                12.242107                 1.035243  ...   -0.046443   \n",
       "21583                11.128438                 0.793231  ...   -0.046443   \n",
       "21584                11.988042                 0.916291  ...   -0.046443   \n",
       "21585                10.842116                 1.172720  ...   -0.046443   \n",
       "21586                10.748304                 0.693147  ...   -0.046443   \n",
       "\n",
       "       행사장(여성캐쥬)   행사장(잡화)       화장품       식품팀     의류패션팀  인터넷백화점_y     잡화가용팀  \\\n",
       "0      -0.044738 -0.031135  1.514697 -0.346229 -0.451798 -0.006806  0.060923   \n",
       "1      -0.044738 -0.031135 -0.380754 -0.346229 -0.076312 -0.006806  0.556603   \n",
       "2      -0.044738 -0.031135 -0.295355 -0.310157 -0.224343 -0.006806  0.068412   \n",
       "3      -0.044738 -0.031135 -0.559885 -0.346229 -0.118915 -0.006806 -0.559987   \n",
       "4      -0.044738 -0.031135  0.404504  0.134538  0.161607 -0.006806  0.736393   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "21582  -0.044738 -0.031135  1.146022 -0.346229  5.361198 -0.006806  0.672847   \n",
       "21583  -0.044738 -0.031135 -0.601543 -0.297154 -0.476080 -0.006806 -0.073786   \n",
       "21584  -0.044738 -0.031135 -0.622372  0.078366 -0.422218 -0.006806  0.550462   \n",
       "21585  -0.044738 -0.031135 -0.605709 -0.346229 -0.346326 -0.006806 -0.496227   \n",
       "21586  -0.044738 -0.031135 -0.576548 -0.346229 -0.532149 -0.006806 -0.543726   \n",
       "\n",
       "       방문당평균구매액  방문당평균구매상품수  \n",
       "0      0.316165   -0.391769  \n",
       "1      1.848414   -0.391769  \n",
       "2     -0.067703    0.702682  \n",
       "3      1.777211    0.245300  \n",
       "4      0.272373   -0.462554  \n",
       "...         ...         ...  \n",
       "21582  1.224299    0.169069  \n",
       "21583 -0.329461   -0.160108  \n",
       "21584  1.160984    0.988546  \n",
       "21585 -0.559442   -0.312135  \n",
       "21586 -0.750921   -1.241194  \n",
       "\n",
       "[21587 rows x 5191 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custid</th>\n",
       "      <th>('tot_amt', '총구매액')</th>\n",
       "      <th>('tot_amt', '구매건수')</th>\n",
       "      <th>('tot_amt', '평균구매가격')</th>\n",
       "      <th>('tot_amt', '최대구매액')</th>\n",
       "      <th>('dis_amt', 'dis_sum')</th>\n",
       "      <th>('dis_amt', 'dis_mean')</th>\n",
       "      <th>('net_amt', 'net_sum')</th>\n",
       "      <th>('net_amt', 'net_mean')</th>\n",
       "      <th>('inst_mon', '평균할부개월수')</th>\n",
       "      <th>...</th>\n",
       "      <th>행사장(여성캐주얼)</th>\n",
       "      <th>행사장(여성캐쥬)</th>\n",
       "      <th>행사장(잡화)</th>\n",
       "      <th>화장품</th>\n",
       "      <th>식품팀</th>\n",
       "      <th>의류패션팀</th>\n",
       "      <th>인터넷백화점_y</th>\n",
       "      <th>잡화가용팀</th>\n",
       "      <th>방문당평균구매액</th>\n",
       "      <th>방문당평균구매상품수</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30001</td>\n",
       "      <td>15.240622</td>\n",
       "      <td>3.332205</td>\n",
       "      <td>11.944791</td>\n",
       "      <td>13.607257</td>\n",
       "      <td>11.679041</td>\n",
       "      <td>8.383425</td>\n",
       "      <td>15.211817</td>\n",
       "      <td>11.915987</td>\n",
       "      <td>1.008228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.570299</td>\n",
       "      <td>0.266301</td>\n",
       "      <td>-0.017519</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>0.486510</td>\n",
       "      <td>1.207685</td>\n",
       "      <td>1.285845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30002</td>\n",
       "      <td>15.903696</td>\n",
       "      <td>4.543295</td>\n",
       "      <td>11.278736</td>\n",
       "      <td>12.923915</td>\n",
       "      <td>12.660490</td>\n",
       "      <td>8.035838</td>\n",
       "      <td>15.863875</td>\n",
       "      <td>11.238915</td>\n",
       "      <td>1.118030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>3.468470</td>\n",
       "      <td>-0.140063</td>\n",
       "      <td>1.420423</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>0.891128</td>\n",
       "      <td>0.473286</td>\n",
       "      <td>2.500668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30003</td>\n",
       "      <td>14.147416</td>\n",
       "      <td>3.871201</td>\n",
       "      <td>10.316067</td>\n",
       "      <td>11.835016</td>\n",
       "      <td>9.922849</td>\n",
       "      <td>6.111467</td>\n",
       "      <td>14.132677</td>\n",
       "      <td>10.287882</td>\n",
       "      <td>0.850333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.464695</td>\n",
       "      <td>-0.253084</td>\n",
       "      <td>-0.165801</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.393869</td>\n",
       "      <td>-0.714909</td>\n",
       "      <td>0.066920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30005</td>\n",
       "      <td>11.790081</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>10.950824</td>\n",
       "      <td>11.141876</td>\n",
       "      <td>8.160804</td>\n",
       "      <td>7.955425</td>\n",
       "      <td>11.746819</td>\n",
       "      <td>10.899532</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.566133</td>\n",
       "      <td>-0.346229</td>\n",
       "      <td>-0.553782</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.561271</td>\n",
       "      <td>-0.708207</td>\n",
       "      <td>-1.241194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30007</td>\n",
       "      <td>12.596427</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>10.804685</td>\n",
       "      <td>11.141876</td>\n",
       "      <td>9.220390</td>\n",
       "      <td>7.429125</td>\n",
       "      <td>12.561650</td>\n",
       "      <td>10.769909</td>\n",
       "      <td>0.847298</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.366174</td>\n",
       "      <td>-0.346229</td>\n",
       "      <td>-0.490869</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.520191</td>\n",
       "      <td>-0.618107</td>\n",
       "      <td>-0.497947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14375</th>\n",
       "      <td>49988</td>\n",
       "      <td>13.356646</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>11.970357</td>\n",
       "      <td>12.955130</td>\n",
       "      <td>10.360944</td>\n",
       "      <td>8.974745</td>\n",
       "      <td>13.305353</td>\n",
       "      <td>11.919064</td>\n",
       "      <td>1.178655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.480734</td>\n",
       "      <td>-0.346229</td>\n",
       "      <td>-0.344074</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.518479</td>\n",
       "      <td>0.674657</td>\n",
       "      <td>0.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14376</th>\n",
       "      <td>49990</td>\n",
       "      <td>12.269052</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>12.269052</td>\n",
       "      <td>12.269052</td>\n",
       "      <td>9.273409</td>\n",
       "      <td>9.239061</td>\n",
       "      <td>12.217759</td>\n",
       "      <td>12.217759</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.241199</td>\n",
       "      <td>-0.346229</td>\n",
       "      <td>-0.553782</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.494515</td>\n",
       "      <td>0.124715</td>\n",
       "      <td>-1.241194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14377</th>\n",
       "      <td>49992</td>\n",
       "      <td>12.305923</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>11.612780</td>\n",
       "      <td>12.031725</td>\n",
       "      <td>9.310276</td>\n",
       "      <td>8.617220</td>\n",
       "      <td>12.254629</td>\n",
       "      <td>11.561487</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.574465</td>\n",
       "      <td>-0.346229</td>\n",
       "      <td>-0.479611</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.562982</td>\n",
       "      <td>-0.422558</td>\n",
       "      <td>-1.241194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14378</th>\n",
       "      <td>49993</td>\n",
       "      <td>11.870810</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>10.484536</td>\n",
       "      <td>11.156265</td>\n",
       "      <td>8.160804</td>\n",
       "      <td>6.775366</td>\n",
       "      <td>11.846035</td>\n",
       "      <td>10.459762</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.539055</td>\n",
       "      <td>-0.272087</td>\n",
       "      <td>-0.553782</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.547149</td>\n",
       "      <td>-0.248877</td>\n",
       "      <td>3.218286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14379</th>\n",
       "      <td>49994</td>\n",
       "      <td>11.977923</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>10.879323</td>\n",
       "      <td>11.362114</td>\n",
       "      <td>9.059634</td>\n",
       "      <td>7.961254</td>\n",
       "      <td>11.922389</td>\n",
       "      <td>10.823790</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046443</td>\n",
       "      <td>-0.044738</td>\n",
       "      <td>-0.031135</td>\n",
       "      <td>-0.505729</td>\n",
       "      <td>-0.346229</td>\n",
       "      <td>-0.521465</td>\n",
       "      <td>-0.006806</td>\n",
       "      <td>-0.548861</td>\n",
       "      <td>-0.587540</td>\n",
       "      <td>-0.497947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14380 rows × 5191 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       custid  ('tot_amt', '총구매액')  ('tot_amt', '구매건수')  \\\n",
       "0       30001            15.240622             3.332205   \n",
       "1       30002            15.903696             4.543295   \n",
       "2       30003            14.147416             3.871201   \n",
       "3       30005            11.790081             1.098612   \n",
       "4       30007            12.596427             1.945910   \n",
       "...       ...                  ...                  ...   \n",
       "14375   49988            13.356646             1.609438   \n",
       "14376   49990            12.269052             1.098612   \n",
       "14377   49992            12.305923             1.098612   \n",
       "14378   49993            11.870810             1.609438   \n",
       "14379   49994            11.977923             1.386294   \n",
       "\n",
       "       ('tot_amt', '평균구매가격')  ('tot_amt', '최대구매액')  ('dis_amt', 'dis_sum')  \\\n",
       "0                  11.944791             13.607257               11.679041   \n",
       "1                  11.278736             12.923915               12.660490   \n",
       "2                  10.316067             11.835016                9.922849   \n",
       "3                  10.950824             11.141876                8.160804   \n",
       "4                  10.804685             11.141876                9.220390   \n",
       "...                      ...                   ...                     ...   \n",
       "14375              11.970357             12.955130               10.360944   \n",
       "14376              12.269052             12.269052                9.273409   \n",
       "14377              11.612780             12.031725                9.310276   \n",
       "14378              10.484536             11.156265                8.160804   \n",
       "14379              10.879323             11.362114                9.059634   \n",
       "\n",
       "       ('dis_amt', 'dis_mean')  ('net_amt', 'net_sum')  \\\n",
       "0                     8.383425               15.211817   \n",
       "1                     8.035838               15.863875   \n",
       "2                     6.111467               14.132677   \n",
       "3                     7.955425               11.746819   \n",
       "4                     7.429125               12.561650   \n",
       "...                        ...                     ...   \n",
       "14375                 8.974745               13.305353   \n",
       "14376                 9.239061               12.217759   \n",
       "14377                 8.617220               12.254629   \n",
       "14378                 6.775366               11.846035   \n",
       "14379                 7.961254               11.922389   \n",
       "\n",
       "       ('net_amt', 'net_mean')  ('inst_mon', '평균할부개월수')  ...  행사장(여성캐주얼)  \\\n",
       "0                    11.915987                 1.008228  ...   -0.046443   \n",
       "1                    11.238915                 1.118030  ...   -0.046443   \n",
       "2                    10.287882                 0.850333  ...   -0.046443   \n",
       "3                    10.899532                 1.386294  ...   -0.046443   \n",
       "4                    10.769909                 0.847298  ...   -0.046443   \n",
       "...                        ...                      ...  ...         ...   \n",
       "14375                11.919064                 1.178655  ...   -0.046443   \n",
       "14376                12.217759                 1.386294  ...   -0.046443   \n",
       "14377                11.561487                 0.693147  ...   -0.046443   \n",
       "14378                10.459762                 0.693147  ...   -0.046443   \n",
       "14379                10.823790                 0.693147  ...   -0.046443   \n",
       "\n",
       "       행사장(여성캐쥬)   행사장(잡화)       화장품       식품팀     의류패션팀  인터넷백화점_y     잡화가용팀  \\\n",
       "0      -0.044738 -0.031135 -0.570299  0.266301 -0.017519 -0.006806  0.486510   \n",
       "1      -0.044738 -0.031135  3.468470 -0.140063  1.420423 -0.006806  0.891128   \n",
       "2      -0.044738 -0.031135 -0.464695 -0.253084 -0.165801 -0.006806 -0.393869   \n",
       "3      -0.044738 -0.031135 -0.566133 -0.346229 -0.553782 -0.006806 -0.561271   \n",
       "4      -0.044738 -0.031135 -0.366174 -0.346229 -0.490869 -0.006806 -0.520191   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "14375  -0.044738 -0.031135 -0.480734 -0.346229 -0.344074 -0.006806 -0.518479   \n",
       "14376  -0.044738 -0.031135 -0.241199 -0.346229 -0.553782 -0.006806 -0.494515   \n",
       "14377  -0.044738 -0.031135 -0.574465 -0.346229 -0.479611 -0.006806 -0.562982   \n",
       "14378  -0.044738 -0.031135 -0.539055 -0.272087 -0.553782 -0.006806 -0.547149   \n",
       "14379  -0.044738 -0.031135 -0.505729 -0.346229 -0.521465 -0.006806 -0.548861   \n",
       "\n",
       "       방문당평균구매액  방문당평균구매상품수  \n",
       "0      1.207685    1.285845  \n",
       "1      0.473286    2.500668  \n",
       "2     -0.714909    0.066920  \n",
       "3     -0.708207   -1.241194  \n",
       "4     -0.618107   -0.497947  \n",
       "...         ...         ...  \n",
       "14375  0.674657    0.245300  \n",
       "14376  0.124715   -1.241194  \n",
       "14377 -0.422558   -1.241194  \n",
       "14378 -0.248877    3.218286  \n",
       "14379 -0.587540   -0.497947  \n",
       "\n",
       "[14380 rows x 5191 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display( data, data_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21587, 5191), (14380, 5191), (21587,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv', encoding='cp949').age\n",
    "data.shape, data_te.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.astype(str)\n",
    "data_te.columns = data_te.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# data = data.rename(columns = lambda x:re.sub('[^A-Za-z0-9]+', ' ', x))\n",
    "# data_te = data_te.rename(columns = lambda x:re.sub('[^A-Za-z0-9]+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.values\n",
    "data_te2 = data_te.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = data2[:, 1:]\n",
    "target = y_train\n",
    "target_log = np.log1p(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17269, 5190), (4318, 5190))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(ftr, target, test_size=0.2 , stratify = target , random_state=0)\n",
    "train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 7.99213\ttraining's l2: 63.8742\tvalid_1's rmse: 8.58504\tvalid_1's l2: 73.703\n",
      "[200]\ttraining's rmse: 7.25095\ttraining's l2: 52.5763\tvalid_1's rmse: 8.34465\tvalid_1's l2: 69.6332\n",
      "[300]\ttraining's rmse: 6.76285\ttraining's l2: 45.7362\tvalid_1's rmse: 8.29029\tvalid_1's l2: 68.7289\n",
      "[400]\ttraining's rmse: 6.38687\ttraining's l2: 40.7921\tvalid_1's rmse: 8.27331\tvalid_1's l2: 68.4476\n",
      "[500]\ttraining's rmse: 6.06472\ttraining's l2: 36.7808\tvalid_1's rmse: 8.26928\tvalid_1's l2: 68.3809\n",
      "Early stopping, best iteration is:\n",
      "[457]\ttraining's rmse: 6.20259\ttraining's l2: 38.4721\tvalid_1's rmse: 8.26796\tvalid_1's l2: 68.3591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(learning_rate=0.02, max_depth=12, n_estimators=1000,\n",
       "              num_leaves=32, silent=-1, subsample=0.8, verbose=-1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "clf = LGBMRegressor(\n",
    "        n_jobs=-1,\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=32,\n",
    "        subsample=0.8,\n",
    "        max_depth=12,\n",
    "        silent=-1,\n",
    "        verbose=-1\n",
    "        )\n",
    "\n",
    "clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric = 'RMSE', \n",
    "        verbose=100, early_stopping_rounds= 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection & Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = data2[:, 1:]\n",
    "target = y_train\n",
    "target_log = np.log1p(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "smf = SelectFromModel(clf, threshold='2.0*mean')\n",
    "smf.fit(ftr, target)\n",
    "X_new = smf.transform(ftr)\n",
    "X_te_new = smf.transform(data_te2[:, 1:])\n",
    "feature_idx = smf.get_support()\n",
    "# feature_name = ftr.columns[feature_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차 검증별 정확도: [-67.1745 -68.5738 -69.7623 -65.5635 -66.4682]\n",
      "평균 검증 정확도: -67.5085\n",
      "RMSE: 8.216354804099296\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf, X_new, target, scoring='neg_mean_squared_error', cv=5)\n",
    "print('교차 검증별 정확도:', np.round(scores, 4))\n",
    "print('평균 검증 정확도:', np.round(np.mean(scores), 4))\n",
    "print('RMSE:', np.sqrt(-np.mean(scores)))\n",
    "# mean -\n",
    "# 1.0mean - 8.21746057149578\n",
    "# 2.0mean - 8.216354804099296\n",
    "# 3.0mean - 8.207348627039885\n",
    "# 3.5mean -       \n",
    "# 4.0mean - \n",
    "# 4.5mean - \n",
    "# 5.0mean - \n",
    "# 6.0mean - \n",
    "# 6.5mean - \n",
    "# 7.0mean - 8.220517039736022\n",
    "# 8.0mean - \n",
    "# 9.0mean - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21587, 972), (14380, 972))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape, X_te_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(X_new).to_csv('jin_ha_sel_feature.csv', index = False )\n",
    "# pd.DataFrame(X_te_new).to_csv('jin_ha_sel_feature_te.csv', index = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameter Tuning_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_new = np.array(data)\n",
    "# X_te_new = np.array(data_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_new\n",
    "y_train = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv', encoding='cp949').age\n",
    "target = y_train\n",
    "target_log = np.log1p(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21587,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15110, 972), (6477, 972))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(ftr, target, test_size=0.3, stratify = target,  random_state=1000)\n",
    "train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.063286\n",
      "0:\tlearn: 10.2909770\ttotal: 288ms\tremaining: 4m 48s\n",
      "1:\tlearn: 10.1481013\ttotal: 398ms\tremaining: 3m 18s\n",
      "2:\tlearn: 10.0221514\ttotal: 507ms\tremaining: 2m 48s\n",
      "3:\tlearn: 9.9075040\ttotal: 625ms\tremaining: 2m 35s\n",
      "4:\tlearn: 9.8144400\ttotal: 741ms\tremaining: 2m 27s\n",
      "5:\tlearn: 9.7153531\ttotal: 852ms\tremaining: 2m 21s\n",
      "6:\tlearn: 9.6161469\ttotal: 961ms\tremaining: 2m 16s\n",
      "7:\tlearn: 9.5354290\ttotal: 1.08s\tremaining: 2m 14s\n",
      "8:\tlearn: 9.4567265\ttotal: 1.2s\tremaining: 2m 11s\n",
      "9:\tlearn: 9.3902092\ttotal: 1.32s\tremaining: 2m 10s\n",
      "10:\tlearn: 9.3256660\ttotal: 1.44s\tremaining: 2m 9s\n",
      "11:\tlearn: 9.2625956\ttotal: 1.56s\tremaining: 2m 8s\n",
      "12:\tlearn: 9.2099063\ttotal: 1.68s\tremaining: 2m 7s\n",
      "13:\tlearn: 9.1543947\ttotal: 1.79s\tremaining: 2m 6s\n",
      "14:\tlearn: 9.1118236\ttotal: 1.91s\tremaining: 2m 5s\n",
      "15:\tlearn: 9.0664531\ttotal: 2.04s\tremaining: 2m 5s\n",
      "16:\tlearn: 9.0175304\ttotal: 2.16s\tremaining: 2m 5s\n",
      "17:\tlearn: 8.9791046\ttotal: 2.29s\tremaining: 2m 4s\n",
      "18:\tlearn: 8.9413547\ttotal: 2.41s\tremaining: 2m 4s\n",
      "19:\tlearn: 8.9007010\ttotal: 2.56s\tremaining: 2m 5s\n",
      "20:\tlearn: 8.8679608\ttotal: 2.71s\tremaining: 2m 6s\n",
      "21:\tlearn: 8.8351147\ttotal: 2.84s\tremaining: 2m 6s\n",
      "22:\tlearn: 8.8080469\ttotal: 2.97s\tremaining: 2m 5s\n",
      "23:\tlearn: 8.7789652\ttotal: 3.09s\tremaining: 2m 5s\n",
      "24:\tlearn: 8.7544658\ttotal: 3.22s\tremaining: 2m 5s\n",
      "25:\tlearn: 8.7278317\ttotal: 3.33s\tremaining: 2m 4s\n",
      "26:\tlearn: 8.7005343\ttotal: 3.46s\tremaining: 2m 4s\n",
      "27:\tlearn: 8.6746028\ttotal: 3.58s\tremaining: 2m 4s\n",
      "28:\tlearn: 8.6522721\ttotal: 3.7s\tremaining: 2m 4s\n",
      "29:\tlearn: 8.6291115\ttotal: 3.83s\tremaining: 2m 3s\n",
      "30:\tlearn: 8.6088296\ttotal: 3.96s\tremaining: 2m 3s\n",
      "31:\tlearn: 8.5916413\ttotal: 4.08s\tremaining: 2m 3s\n",
      "32:\tlearn: 8.5720428\ttotal: 4.2s\tremaining: 2m 2s\n",
      "33:\tlearn: 8.5559049\ttotal: 4.32s\tremaining: 2m 2s\n",
      "34:\tlearn: 8.5373902\ttotal: 4.43s\tremaining: 2m 2s\n",
      "35:\tlearn: 8.5188086\ttotal: 4.55s\tremaining: 2m 1s\n",
      "36:\tlearn: 8.5032032\ttotal: 4.66s\tremaining: 2m 1s\n",
      "37:\tlearn: 8.4907114\ttotal: 4.78s\tremaining: 2m\n",
      "38:\tlearn: 8.4764615\ttotal: 4.9s\tremaining: 2m\n",
      "39:\tlearn: 8.4603192\ttotal: 5.01s\tremaining: 2m\n",
      "40:\tlearn: 8.4445443\ttotal: 5.13s\tremaining: 1m 59s\n",
      "41:\tlearn: 8.4313694\ttotal: 5.24s\tremaining: 1m 59s\n",
      "42:\tlearn: 8.4191342\ttotal: 5.37s\tremaining: 1m 59s\n",
      "43:\tlearn: 8.4075381\ttotal: 5.48s\tremaining: 1m 59s\n",
      "44:\tlearn: 8.3926559\ttotal: 5.6s\tremaining: 1m 58s\n",
      "45:\tlearn: 8.3793473\ttotal: 5.71s\tremaining: 1m 58s\n",
      "46:\tlearn: 8.3682524\ttotal: 5.82s\tremaining: 1m 58s\n",
      "47:\tlearn: 8.3562293\ttotal: 5.94s\tremaining: 1m 57s\n",
      "48:\tlearn: 8.3473726\ttotal: 6.05s\tremaining: 1m 57s\n",
      "49:\tlearn: 8.3369049\ttotal: 6.17s\tremaining: 1m 57s\n",
      "50:\tlearn: 8.3237379\ttotal: 6.29s\tremaining: 1m 56s\n",
      "51:\tlearn: 8.3127983\ttotal: 6.4s\tremaining: 1m 56s\n",
      "52:\tlearn: 8.3031800\ttotal: 6.5s\tremaining: 1m 56s\n",
      "53:\tlearn: 8.2941569\ttotal: 6.62s\tremaining: 1m 55s\n",
      "54:\tlearn: 8.2847957\ttotal: 6.73s\tremaining: 1m 55s\n",
      "55:\tlearn: 8.2763662\ttotal: 6.84s\tremaining: 1m 55s\n",
      "56:\tlearn: 8.2679345\ttotal: 6.95s\tremaining: 1m 54s\n",
      "57:\tlearn: 8.2603507\ttotal: 7.06s\tremaining: 1m 54s\n",
      "58:\tlearn: 8.2535027\ttotal: 7.17s\tremaining: 1m 54s\n",
      "59:\tlearn: 8.2460484\ttotal: 7.28s\tremaining: 1m 54s\n",
      "60:\tlearn: 8.2370503\ttotal: 7.39s\tremaining: 1m 53s\n",
      "61:\tlearn: 8.2299581\ttotal: 7.51s\tremaining: 1m 53s\n",
      "62:\tlearn: 8.2226519\ttotal: 7.62s\tremaining: 1m 53s\n",
      "63:\tlearn: 8.2149498\ttotal: 7.73s\tremaining: 1m 53s\n",
      "64:\tlearn: 8.2071765\ttotal: 7.84s\tremaining: 1m 52s\n",
      "65:\tlearn: 8.2007139\ttotal: 7.95s\tremaining: 1m 52s\n",
      "66:\tlearn: 8.1938440\ttotal: 8.06s\tremaining: 1m 52s\n",
      "67:\tlearn: 8.1882796\ttotal: 8.17s\tremaining: 1m 52s\n",
      "68:\tlearn: 8.1816248\ttotal: 8.28s\tremaining: 1m 51s\n",
      "69:\tlearn: 8.1738210\ttotal: 8.39s\tremaining: 1m 51s\n",
      "70:\tlearn: 8.1674905\ttotal: 8.5s\tremaining: 1m 51s\n",
      "71:\tlearn: 8.1614517\ttotal: 8.61s\tremaining: 1m 51s\n",
      "72:\tlearn: 8.1547457\ttotal: 8.72s\tremaining: 1m 50s\n",
      "73:\tlearn: 8.1498381\ttotal: 8.84s\tremaining: 1m 50s\n",
      "74:\tlearn: 8.1434525\ttotal: 8.96s\tremaining: 1m 50s\n",
      "75:\tlearn: 8.1384289\ttotal: 9.07s\tremaining: 1m 50s\n",
      "76:\tlearn: 8.1345781\ttotal: 9.19s\tremaining: 1m 50s\n",
      "77:\tlearn: 8.1282433\ttotal: 9.32s\tremaining: 1m 50s\n",
      "78:\tlearn: 8.1240629\ttotal: 9.44s\tremaining: 1m 50s\n",
      "79:\tlearn: 8.1179007\ttotal: 9.57s\tremaining: 1m 50s\n",
      "80:\tlearn: 8.1134047\ttotal: 9.7s\tremaining: 1m 50s\n",
      "81:\tlearn: 8.1086609\ttotal: 9.82s\tremaining: 1m 49s\n",
      "82:\tlearn: 8.1008445\ttotal: 9.94s\tremaining: 1m 49s\n",
      "83:\tlearn: 8.0965194\ttotal: 10.1s\tremaining: 1m 49s\n",
      "84:\tlearn: 8.0910153\ttotal: 10.2s\tremaining: 1m 49s\n",
      "85:\tlearn: 8.0863632\ttotal: 10.3s\tremaining: 1m 49s\n",
      "86:\tlearn: 8.0798627\ttotal: 10.4s\tremaining: 1m 49s\n",
      "87:\tlearn: 8.0757931\ttotal: 10.6s\tremaining: 1m 49s\n",
      "88:\tlearn: 8.0700732\ttotal: 10.7s\tremaining: 1m 49s\n",
      "89:\tlearn: 8.0653098\ttotal: 10.8s\tremaining: 1m 49s\n",
      "90:\tlearn: 8.0604703\ttotal: 11s\tremaining: 1m 49s\n",
      "91:\tlearn: 8.0544251\ttotal: 11.1s\tremaining: 1m 49s\n",
      "92:\tlearn: 8.0488461\ttotal: 11.2s\tremaining: 1m 49s\n",
      "93:\tlearn: 8.0448706\ttotal: 11.3s\tremaining: 1m 49s\n",
      "94:\tlearn: 8.0403771\ttotal: 11.5s\tremaining: 1m 49s\n",
      "95:\tlearn: 8.0356424\ttotal: 11.6s\tremaining: 1m 49s\n",
      "96:\tlearn: 8.0305196\ttotal: 11.7s\tremaining: 1m 49s\n",
      "97:\tlearn: 8.0253070\ttotal: 11.9s\tremaining: 1m 49s\n",
      "98:\tlearn: 8.0211522\ttotal: 12s\tremaining: 1m 49s\n",
      "99:\tlearn: 8.0154572\ttotal: 12.1s\tremaining: 1m 49s\n",
      "100:\tlearn: 8.0100206\ttotal: 12.3s\tremaining: 1m 49s\n",
      "101:\tlearn: 8.0058890\ttotal: 12.4s\tremaining: 1m 48s\n",
      "102:\tlearn: 8.0005933\ttotal: 12.5s\tremaining: 1m 48s\n",
      "103:\tlearn: 7.9959884\ttotal: 12.6s\tremaining: 1m 48s\n",
      "104:\tlearn: 7.9922313\ttotal: 12.8s\tremaining: 1m 48s\n",
      "105:\tlearn: 7.9870007\ttotal: 12.9s\tremaining: 1m 48s\n",
      "106:\tlearn: 7.9822416\ttotal: 13s\tremaining: 1m 48s\n",
      "107:\tlearn: 7.9780091\ttotal: 13.2s\tremaining: 1m 48s\n",
      "108:\tlearn: 7.9731105\ttotal: 13.3s\tremaining: 1m 48s\n",
      "109:\tlearn: 7.9675404\ttotal: 13.4s\tremaining: 1m 48s\n",
      "110:\tlearn: 7.9633769\ttotal: 13.5s\tremaining: 1m 48s\n",
      "111:\tlearn: 7.9578686\ttotal: 13.6s\tremaining: 1m 48s\n",
      "112:\tlearn: 7.9550653\ttotal: 13.7s\tremaining: 1m 47s\n",
      "113:\tlearn: 7.9495086\ttotal: 13.9s\tremaining: 1m 47s\n",
      "114:\tlearn: 7.9454312\ttotal: 14s\tremaining: 1m 47s\n",
      "115:\tlearn: 7.9416520\ttotal: 14.1s\tremaining: 1m 47s\n",
      "116:\tlearn: 7.9382176\ttotal: 14.2s\tremaining: 1m 47s\n",
      "117:\tlearn: 7.9329825\ttotal: 14.3s\tremaining: 1m 46s\n",
      "118:\tlearn: 7.9295452\ttotal: 14.4s\tremaining: 1m 46s\n",
      "119:\tlearn: 7.9258884\ttotal: 14.5s\tremaining: 1m 46s\n",
      "120:\tlearn: 7.9231395\ttotal: 14.7s\tremaining: 1m 46s\n",
      "121:\tlearn: 7.9173000\ttotal: 14.8s\tremaining: 1m 46s\n",
      "122:\tlearn: 7.9132541\ttotal: 14.9s\tremaining: 1m 46s\n",
      "123:\tlearn: 7.9090919\ttotal: 15s\tremaining: 1m 45s\n",
      "124:\tlearn: 7.9055405\ttotal: 15.1s\tremaining: 1m 45s\n",
      "125:\tlearn: 7.9025282\ttotal: 15.2s\tremaining: 1m 45s\n",
      "126:\tlearn: 7.8974772\ttotal: 15.3s\tremaining: 1m 45s\n",
      "127:\tlearn: 7.8933003\ttotal: 15.5s\tremaining: 1m 45s\n",
      "128:\tlearn: 7.8879314\ttotal: 15.6s\tremaining: 1m 45s\n",
      "129:\tlearn: 7.8848503\ttotal: 15.7s\tremaining: 1m 45s\n",
      "130:\tlearn: 7.8806519\ttotal: 15.8s\tremaining: 1m 45s\n",
      "131:\tlearn: 7.8764898\ttotal: 16s\tremaining: 1m 44s\n",
      "132:\tlearn: 7.8710933\ttotal: 16.1s\tremaining: 1m 44s\n",
      "133:\tlearn: 7.8669456\ttotal: 16.2s\tremaining: 1m 44s\n",
      "134:\tlearn: 7.8613375\ttotal: 16.3s\tremaining: 1m 44s\n",
      "135:\tlearn: 7.8581180\ttotal: 16.4s\tremaining: 1m 44s\n",
      "136:\tlearn: 7.8542595\ttotal: 16.5s\tremaining: 1m 44s\n",
      "137:\tlearn: 7.8492528\ttotal: 16.7s\tremaining: 1m 44s\n",
      "138:\tlearn: 7.8470138\ttotal: 16.8s\tremaining: 1m 43s\n",
      "139:\tlearn: 7.8428149\ttotal: 16.9s\tremaining: 1m 43s\n",
      "140:\tlearn: 7.8371466\ttotal: 17s\tremaining: 1m 43s\n",
      "141:\tlearn: 7.8324240\ttotal: 17.1s\tremaining: 1m 43s\n",
      "142:\tlearn: 7.8272734\ttotal: 17.3s\tremaining: 1m 43s\n",
      "143:\tlearn: 7.8238854\ttotal: 17.4s\tremaining: 1m 43s\n",
      "144:\tlearn: 7.8213758\ttotal: 17.5s\tremaining: 1m 43s\n",
      "145:\tlearn: 7.8172357\ttotal: 17.7s\tremaining: 1m 43s\n",
      "146:\tlearn: 7.8126557\ttotal: 17.8s\tremaining: 1m 43s\n",
      "147:\tlearn: 7.8087020\ttotal: 17.9s\tremaining: 1m 43s\n",
      "148:\tlearn: 7.8044345\ttotal: 18.1s\tremaining: 1m 43s\n",
      "149:\tlearn: 7.8015260\ttotal: 18.2s\tremaining: 1m 43s\n",
      "150:\tlearn: 7.7974293\ttotal: 18.3s\tremaining: 1m 42s\n",
      "151:\tlearn: 7.7941310\ttotal: 18.4s\tremaining: 1m 42s\n",
      "152:\tlearn: 7.7908908\ttotal: 18.6s\tremaining: 1m 42s\n",
      "153:\tlearn: 7.7889597\ttotal: 18.7s\tremaining: 1m 42s\n",
      "154:\tlearn: 7.7864910\ttotal: 18.8s\tremaining: 1m 42s\n",
      "155:\tlearn: 7.7842385\ttotal: 19s\tremaining: 1m 42s\n",
      "156:\tlearn: 7.7807765\ttotal: 19.1s\tremaining: 1m 42s\n",
      "157:\tlearn: 7.7774145\ttotal: 19.2s\tremaining: 1m 42s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158:\tlearn: 7.7729158\ttotal: 19.4s\tremaining: 1m 42s\n",
      "159:\tlearn: 7.7688841\ttotal: 19.5s\tremaining: 1m 42s\n",
      "160:\tlearn: 7.7652161\ttotal: 19.6s\tremaining: 1m 42s\n",
      "161:\tlearn: 7.7603348\ttotal: 19.8s\tremaining: 1m 42s\n",
      "162:\tlearn: 7.7554240\ttotal: 19.9s\tremaining: 1m 42s\n",
      "163:\tlearn: 7.7510576\ttotal: 20s\tremaining: 1m 42s\n",
      "164:\tlearn: 7.7471521\ttotal: 20.1s\tremaining: 1m 41s\n",
      "165:\tlearn: 7.7439962\ttotal: 20.3s\tremaining: 1m 41s\n",
      "166:\tlearn: 7.7394181\ttotal: 20.4s\tremaining: 1m 41s\n",
      "167:\tlearn: 7.7360600\ttotal: 20.5s\tremaining: 1m 41s\n",
      "168:\tlearn: 7.7324076\ttotal: 20.7s\tremaining: 1m 41s\n",
      "169:\tlearn: 7.7297620\ttotal: 20.8s\tremaining: 1m 41s\n",
      "170:\tlearn: 7.7271490\ttotal: 20.9s\tremaining: 1m 41s\n",
      "171:\tlearn: 7.7220814\ttotal: 21s\tremaining: 1m 41s\n",
      "172:\tlearn: 7.7172570\ttotal: 21.1s\tremaining: 1m 41s\n",
      "173:\tlearn: 7.7135523\ttotal: 21.3s\tremaining: 1m 40s\n",
      "174:\tlearn: 7.7101029\ttotal: 21.4s\tremaining: 1m 40s\n",
      "175:\tlearn: 7.7064795\ttotal: 21.5s\tremaining: 1m 40s\n",
      "176:\tlearn: 7.7019700\ttotal: 21.6s\tremaining: 1m 40s\n",
      "177:\tlearn: 7.6963452\ttotal: 21.7s\tremaining: 1m 40s\n",
      "178:\tlearn: 7.6916126\ttotal: 21.9s\tremaining: 1m 40s\n",
      "179:\tlearn: 7.6871062\ttotal: 22s\tremaining: 1m 40s\n",
      "180:\tlearn: 7.6844611\ttotal: 22.1s\tremaining: 1m 39s\n",
      "181:\tlearn: 7.6811115\ttotal: 22.2s\tremaining: 1m 39s\n",
      "182:\tlearn: 7.6770804\ttotal: 22.3s\tremaining: 1m 39s\n",
      "183:\tlearn: 7.6736432\ttotal: 22.4s\tremaining: 1m 39s\n",
      "184:\tlearn: 7.6696215\ttotal: 22.6s\tremaining: 1m 39s\n",
      "185:\tlearn: 7.6667187\ttotal: 22.7s\tremaining: 1m 39s\n",
      "186:\tlearn: 7.6616903\ttotal: 22.8s\tremaining: 1m 39s\n",
      "187:\tlearn: 7.6566221\ttotal: 23s\tremaining: 1m 39s\n",
      "188:\tlearn: 7.6504040\ttotal: 23.1s\tremaining: 1m 39s\n",
      "189:\tlearn: 7.6459363\ttotal: 23.2s\tremaining: 1m 38s\n",
      "190:\tlearn: 7.6415056\ttotal: 23.3s\tremaining: 1m 38s\n",
      "191:\tlearn: 7.6359729\ttotal: 23.4s\tremaining: 1m 38s\n",
      "192:\tlearn: 7.6313664\ttotal: 23.5s\tremaining: 1m 38s\n",
      "193:\tlearn: 7.6265911\ttotal: 23.7s\tremaining: 1m 38s\n",
      "194:\tlearn: 7.6228915\ttotal: 23.8s\tremaining: 1m 38s\n",
      "195:\tlearn: 7.6188722\ttotal: 23.9s\tremaining: 1m 38s\n",
      "196:\tlearn: 7.6142993\ttotal: 24s\tremaining: 1m 37s\n",
      "197:\tlearn: 7.6083346\ttotal: 24.1s\tremaining: 1m 37s\n",
      "198:\tlearn: 7.6041009\ttotal: 24.3s\tremaining: 1m 37s\n",
      "199:\tlearn: 7.6011890\ttotal: 24.4s\tremaining: 1m 37s\n",
      "200:\tlearn: 7.5961948\ttotal: 24.5s\tremaining: 1m 37s\n",
      "201:\tlearn: 7.5919675\ttotal: 24.6s\tremaining: 1m 37s\n",
      "202:\tlearn: 7.5871435\ttotal: 24.7s\tremaining: 1m 37s\n",
      "203:\tlearn: 7.5820521\ttotal: 24.8s\tremaining: 1m 36s\n",
      "204:\tlearn: 7.5777813\ttotal: 25s\tremaining: 1m 36s\n",
      "205:\tlearn: 7.5742125\ttotal: 25.1s\tremaining: 1m 36s\n",
      "206:\tlearn: 7.5686203\ttotal: 25.2s\tremaining: 1m 36s\n",
      "207:\tlearn: 7.5641166\ttotal: 25.3s\tremaining: 1m 36s\n",
      "208:\tlearn: 7.5593612\ttotal: 25.5s\tremaining: 1m 36s\n",
      "209:\tlearn: 7.5542852\ttotal: 25.6s\tremaining: 1m 36s\n",
      "210:\tlearn: 7.5511593\ttotal: 25.7s\tremaining: 1m 36s\n",
      "211:\tlearn: 7.5453757\ttotal: 25.8s\tremaining: 1m 36s\n",
      "212:\tlearn: 7.5420374\ttotal: 26s\tremaining: 1m 35s\n",
      "213:\tlearn: 7.5378686\ttotal: 26.1s\tremaining: 1m 35s\n",
      "214:\tlearn: 7.5347867\ttotal: 26.2s\tremaining: 1m 35s\n",
      "215:\tlearn: 7.5299726\ttotal: 26.4s\tremaining: 1m 35s\n",
      "216:\tlearn: 7.5256645\ttotal: 26.5s\tremaining: 1m 35s\n",
      "217:\tlearn: 7.5222745\ttotal: 26.7s\tremaining: 1m 35s\n",
      "218:\tlearn: 7.5186286\ttotal: 26.8s\tremaining: 1m 35s\n",
      "219:\tlearn: 7.5136079\ttotal: 26.9s\tremaining: 1m 35s\n",
      "220:\tlearn: 7.5100612\ttotal: 27s\tremaining: 1m 35s\n",
      "221:\tlearn: 7.5062786\ttotal: 27.2s\tremaining: 1m 35s\n",
      "222:\tlearn: 7.5027543\ttotal: 27.3s\tremaining: 1m 35s\n",
      "223:\tlearn: 7.4990335\ttotal: 27.4s\tremaining: 1m 35s\n",
      "224:\tlearn: 7.4945671\ttotal: 27.6s\tremaining: 1m 34s\n",
      "225:\tlearn: 7.4908571\ttotal: 27.7s\tremaining: 1m 34s\n",
      "226:\tlearn: 7.4876284\ttotal: 27.8s\tremaining: 1m 34s\n",
      "227:\tlearn: 7.4838302\ttotal: 28s\tremaining: 1m 34s\n",
      "228:\tlearn: 7.4784002\ttotal: 28.1s\tremaining: 1m 34s\n",
      "229:\tlearn: 7.4752767\ttotal: 28.2s\tremaining: 1m 34s\n",
      "230:\tlearn: 7.4701665\ttotal: 28.4s\tremaining: 1m 34s\n",
      "231:\tlearn: 7.4657887\ttotal: 28.5s\tremaining: 1m 34s\n",
      "232:\tlearn: 7.4641884\ttotal: 28.6s\tremaining: 1m 34s\n",
      "233:\tlearn: 7.4590805\ttotal: 28.7s\tremaining: 1m 34s\n",
      "234:\tlearn: 7.4555414\ttotal: 28.9s\tremaining: 1m 33s\n",
      "235:\tlearn: 7.4525105\ttotal: 29s\tremaining: 1m 33s\n",
      "236:\tlearn: 7.4487606\ttotal: 29.1s\tremaining: 1m 33s\n",
      "237:\tlearn: 7.4447417\ttotal: 29.2s\tremaining: 1m 33s\n",
      "238:\tlearn: 7.4405846\ttotal: 29.3s\tremaining: 1m 33s\n",
      "239:\tlearn: 7.4363010\ttotal: 29.5s\tremaining: 1m 33s\n",
      "240:\tlearn: 7.4328534\ttotal: 29.6s\tremaining: 1m 33s\n",
      "241:\tlearn: 7.4285012\ttotal: 29.7s\tremaining: 1m 32s\n",
      "242:\tlearn: 7.4252356\ttotal: 29.8s\tremaining: 1m 32s\n",
      "243:\tlearn: 7.4217169\ttotal: 29.9s\tremaining: 1m 32s\n",
      "244:\tlearn: 7.4180251\ttotal: 30s\tremaining: 1m 32s\n",
      "245:\tlearn: 7.4145862\ttotal: 30.1s\tremaining: 1m 32s\n",
      "246:\tlearn: 7.4108070\ttotal: 30.2s\tremaining: 1m 32s\n",
      "247:\tlearn: 7.4075027\ttotal: 30.3s\tremaining: 1m 31s\n",
      "248:\tlearn: 7.4034232\ttotal: 30.4s\tremaining: 1m 31s\n",
      "249:\tlearn: 7.4004862\ttotal: 30.6s\tremaining: 1m 31s\n",
      "250:\tlearn: 7.3965925\ttotal: 30.7s\tremaining: 1m 31s\n",
      "251:\tlearn: 7.3922138\ttotal: 30.8s\tremaining: 1m 31s\n",
      "252:\tlearn: 7.3887403\ttotal: 30.9s\tremaining: 1m 31s\n",
      "253:\tlearn: 7.3861320\ttotal: 31s\tremaining: 1m 31s\n",
      "254:\tlearn: 7.3822839\ttotal: 31.1s\tremaining: 1m 30s\n",
      "255:\tlearn: 7.3784023\ttotal: 31.2s\tremaining: 1m 30s\n",
      "256:\tlearn: 7.3751632\ttotal: 31.3s\tremaining: 1m 30s\n",
      "257:\tlearn: 7.3706300\ttotal: 31.4s\tremaining: 1m 30s\n",
      "258:\tlearn: 7.3660707\ttotal: 31.5s\tremaining: 1m 30s\n",
      "259:\tlearn: 7.3617972\ttotal: 31.6s\tremaining: 1m 30s\n",
      "260:\tlearn: 7.3581623\ttotal: 31.8s\tremaining: 1m 29s\n",
      "261:\tlearn: 7.3551802\ttotal: 31.9s\tremaining: 1m 29s\n",
      "262:\tlearn: 7.3520479\ttotal: 32s\tremaining: 1m 29s\n",
      "263:\tlearn: 7.3485194\ttotal: 32.1s\tremaining: 1m 29s\n",
      "264:\tlearn: 7.3451643\ttotal: 32.2s\tremaining: 1m 29s\n",
      "265:\tlearn: 7.3412569\ttotal: 32.3s\tremaining: 1m 29s\n",
      "266:\tlearn: 7.3381851\ttotal: 32.4s\tremaining: 1m 29s\n",
      "267:\tlearn: 7.3341431\ttotal: 32.5s\tremaining: 1m 28s\n",
      "268:\tlearn: 7.3293511\ttotal: 32.7s\tremaining: 1m 28s\n",
      "269:\tlearn: 7.3255515\ttotal: 32.8s\tremaining: 1m 28s\n",
      "270:\tlearn: 7.3249675\ttotal: 32.9s\tremaining: 1m 28s\n",
      "271:\tlearn: 7.3212106\ttotal: 33s\tremaining: 1m 28s\n",
      "272:\tlearn: 7.3176109\ttotal: 33.1s\tremaining: 1m 28s\n",
      "273:\tlearn: 7.3132917\ttotal: 33.2s\tremaining: 1m 27s\n",
      "274:\tlearn: 7.3095973\ttotal: 33.3s\tremaining: 1m 27s\n",
      "275:\tlearn: 7.3053560\ttotal: 33.4s\tremaining: 1m 27s\n",
      "276:\tlearn: 7.3020018\ttotal: 33.5s\tremaining: 1m 27s\n",
      "277:\tlearn: 7.2989460\ttotal: 33.7s\tremaining: 1m 27s\n",
      "278:\tlearn: 7.2946781\ttotal: 33.8s\tremaining: 1m 27s\n",
      "279:\tlearn: 7.2916373\ttotal: 33.9s\tremaining: 1m 27s\n",
      "280:\tlearn: 7.2863092\ttotal: 34s\tremaining: 1m 26s\n",
      "281:\tlearn: 7.2829009\ttotal: 34.1s\tremaining: 1m 26s\n",
      "282:\tlearn: 7.2785093\ttotal: 34.2s\tremaining: 1m 26s\n",
      "283:\tlearn: 7.2742782\ttotal: 34.3s\tremaining: 1m 26s\n",
      "284:\tlearn: 7.2707311\ttotal: 34.5s\tremaining: 1m 26s\n",
      "285:\tlearn: 7.2672752\ttotal: 34.6s\tremaining: 1m 26s\n",
      "286:\tlearn: 7.2635317\ttotal: 34.7s\tremaining: 1m 26s\n",
      "287:\tlearn: 7.2589432\ttotal: 34.8s\tremaining: 1m 26s\n",
      "288:\tlearn: 7.2544705\ttotal: 34.9s\tremaining: 1m 25s\n",
      "289:\tlearn: 7.2496414\ttotal: 35.1s\tremaining: 1m 25s\n",
      "290:\tlearn: 7.2461109\ttotal: 35.2s\tremaining: 1m 25s\n",
      "291:\tlearn: 7.2430009\ttotal: 35.3s\tremaining: 1m 25s\n",
      "292:\tlearn: 7.2397671\ttotal: 35.5s\tremaining: 1m 25s\n",
      "293:\tlearn: 7.2366539\ttotal: 35.6s\tremaining: 1m 25s\n",
      "294:\tlearn: 7.2345556\ttotal: 35.7s\tremaining: 1m 25s\n",
      "295:\tlearn: 7.2299108\ttotal: 35.9s\tremaining: 1m 25s\n",
      "296:\tlearn: 7.2266305\ttotal: 36s\tremaining: 1m 25s\n",
      "297:\tlearn: 7.2222497\ttotal: 36.1s\tremaining: 1m 25s\n",
      "298:\tlearn: 7.2186889\ttotal: 36.3s\tremaining: 1m 25s\n",
      "299:\tlearn: 7.2142762\ttotal: 36.4s\tremaining: 1m 24s\n",
      "300:\tlearn: 7.2104479\ttotal: 36.5s\tremaining: 1m 24s\n",
      "301:\tlearn: 7.2070719\ttotal: 36.7s\tremaining: 1m 24s\n",
      "302:\tlearn: 7.2044540\ttotal: 36.8s\tremaining: 1m 24s\n",
      "303:\tlearn: 7.1997986\ttotal: 36.9s\tremaining: 1m 24s\n",
      "304:\tlearn: 7.1955791\ttotal: 37.1s\tremaining: 1m 24s\n",
      "305:\tlearn: 7.1923516\ttotal: 37.2s\tremaining: 1m 24s\n",
      "306:\tlearn: 7.1881859\ttotal: 37.3s\tremaining: 1m 24s\n",
      "307:\tlearn: 7.1851824\ttotal: 37.4s\tremaining: 1m 24s\n",
      "308:\tlearn: 7.1819598\ttotal: 37.6s\tremaining: 1m 23s\n",
      "309:\tlearn: 7.1786149\ttotal: 37.7s\tremaining: 1m 23s\n",
      "310:\tlearn: 7.1748508\ttotal: 37.8s\tremaining: 1m 23s\n",
      "311:\tlearn: 7.1713207\ttotal: 37.9s\tremaining: 1m 23s\n",
      "312:\tlearn: 7.1682333\ttotal: 38s\tremaining: 1m 23s\n",
      "313:\tlearn: 7.1649544\ttotal: 38.2s\tremaining: 1m 23s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314:\tlearn: 7.1645222\ttotal: 38.3s\tremaining: 1m 23s\n",
      "315:\tlearn: 7.1608781\ttotal: 38.4s\tremaining: 1m 23s\n",
      "316:\tlearn: 7.1562851\ttotal: 38.5s\tremaining: 1m 23s\n",
      "317:\tlearn: 7.1529613\ttotal: 38.7s\tremaining: 1m 22s\n",
      "318:\tlearn: 7.1498862\ttotal: 38.8s\tremaining: 1m 22s\n",
      "319:\tlearn: 7.1464989\ttotal: 38.9s\tremaining: 1m 22s\n",
      "320:\tlearn: 7.1437533\ttotal: 39s\tremaining: 1m 22s\n",
      "321:\tlearn: 7.1402518\ttotal: 39.1s\tremaining: 1m 22s\n",
      "322:\tlearn: 7.1373559\ttotal: 39.3s\tremaining: 1m 22s\n",
      "323:\tlearn: 7.1333928\ttotal: 39.4s\tremaining: 1m 22s\n",
      "324:\tlearn: 7.1304405\ttotal: 39.5s\tremaining: 1m 22s\n",
      "325:\tlearn: 7.1268398\ttotal: 39.7s\tremaining: 1m 21s\n",
      "326:\tlearn: 7.1236088\ttotal: 39.8s\tremaining: 1m 21s\n",
      "327:\tlearn: 7.1232240\ttotal: 39.9s\tremaining: 1m 21s\n",
      "328:\tlearn: 7.1196598\ttotal: 40s\tremaining: 1m 21s\n",
      "329:\tlearn: 7.1193108\ttotal: 40.1s\tremaining: 1m 21s\n",
      "330:\tlearn: 7.1162454\ttotal: 40.2s\tremaining: 1m 21s\n",
      "331:\tlearn: 7.1128066\ttotal: 40.4s\tremaining: 1m 21s\n",
      "332:\tlearn: 7.1095259\ttotal: 40.5s\tremaining: 1m 21s\n",
      "333:\tlearn: 7.1065050\ttotal: 40.6s\tremaining: 1m 20s\n",
      "334:\tlearn: 7.1023551\ttotal: 40.7s\tremaining: 1m 20s\n",
      "335:\tlearn: 7.0989558\ttotal: 40.9s\tremaining: 1m 20s\n",
      "336:\tlearn: 7.0951747\ttotal: 41s\tremaining: 1m 20s\n",
      "337:\tlearn: 7.0910935\ttotal: 41.1s\tremaining: 1m 20s\n",
      "338:\tlearn: 7.0875220\ttotal: 41.2s\tremaining: 1m 20s\n",
      "339:\tlearn: 7.0833822\ttotal: 41.3s\tremaining: 1m 20s\n",
      "340:\tlearn: 7.0796655\ttotal: 41.5s\tremaining: 1m 20s\n",
      "341:\tlearn: 7.0777465\ttotal: 41.6s\tremaining: 1m 20s\n",
      "342:\tlearn: 7.0734933\ttotal: 41.7s\tremaining: 1m 19s\n",
      "343:\tlearn: 7.0732315\ttotal: 41.8s\tremaining: 1m 19s\n",
      "344:\tlearn: 7.0695529\ttotal: 42s\tremaining: 1m 19s\n",
      "345:\tlearn: 7.0663533\ttotal: 42.1s\tremaining: 1m 19s\n",
      "346:\tlearn: 7.0657796\ttotal: 42.2s\tremaining: 1m 19s\n",
      "347:\tlearn: 7.0614290\ttotal: 42.4s\tremaining: 1m 19s\n",
      "348:\tlearn: 7.0579681\ttotal: 42.5s\tremaining: 1m 19s\n",
      "349:\tlearn: 7.0540137\ttotal: 42.6s\tremaining: 1m 19s\n",
      "350:\tlearn: 7.0517402\ttotal: 42.8s\tremaining: 1m 19s\n",
      "351:\tlearn: 7.0473107\ttotal: 42.9s\tremaining: 1m 18s\n",
      "352:\tlearn: 7.0446580\ttotal: 43s\tremaining: 1m 18s\n",
      "353:\tlearn: 7.0414366\ttotal: 43.2s\tremaining: 1m 18s\n",
      "354:\tlearn: 7.0375571\ttotal: 43.3s\tremaining: 1m 18s\n",
      "355:\tlearn: 7.0338963\ttotal: 43.4s\tremaining: 1m 18s\n",
      "356:\tlearn: 7.0302618\ttotal: 43.6s\tremaining: 1m 18s\n",
      "357:\tlearn: 7.0276383\ttotal: 43.7s\tremaining: 1m 18s\n",
      "358:\tlearn: 7.0240361\ttotal: 43.9s\tremaining: 1m 18s\n",
      "359:\tlearn: 7.0194523\ttotal: 44s\tremaining: 1m 18s\n",
      "360:\tlearn: 7.0168922\ttotal: 44.1s\tremaining: 1m 18s\n",
      "361:\tlearn: 7.0143665\ttotal: 44.3s\tremaining: 1m 18s\n",
      "362:\tlearn: 7.0113686\ttotal: 44.4s\tremaining: 1m 17s\n",
      "363:\tlearn: 7.0086461\ttotal: 44.5s\tremaining: 1m 17s\n",
      "364:\tlearn: 7.0061495\ttotal: 44.7s\tremaining: 1m 17s\n",
      "365:\tlearn: 7.0030743\ttotal: 44.8s\tremaining: 1m 17s\n",
      "366:\tlearn: 6.9999923\ttotal: 44.9s\tremaining: 1m 17s\n",
      "367:\tlearn: 6.9965182\ttotal: 45.1s\tremaining: 1m 17s\n",
      "368:\tlearn: 6.9937497\ttotal: 45.2s\tremaining: 1m 17s\n",
      "369:\tlearn: 6.9899518\ttotal: 45.4s\tremaining: 1m 17s\n",
      "370:\tlearn: 6.9868317\ttotal: 45.6s\tremaining: 1m 17s\n",
      "371:\tlearn: 6.9821664\ttotal: 45.7s\tremaining: 1m 17s\n",
      "372:\tlearn: 6.9793014\ttotal: 45.8s\tremaining: 1m 17s\n",
      "373:\tlearn: 6.9762873\ttotal: 46s\tremaining: 1m 16s\n",
      "374:\tlearn: 6.9728981\ttotal: 46.1s\tremaining: 1m 16s\n",
      "375:\tlearn: 6.9696079\ttotal: 46.2s\tremaining: 1m 16s\n",
      "376:\tlearn: 6.9666068\ttotal: 46.3s\tremaining: 1m 16s\n",
      "377:\tlearn: 6.9631753\ttotal: 46.4s\tremaining: 1m 16s\n",
      "378:\tlearn: 6.9587910\ttotal: 46.6s\tremaining: 1m 16s\n",
      "379:\tlearn: 6.9556059\ttotal: 46.7s\tremaining: 1m 16s\n",
      "380:\tlearn: 6.9522835\ttotal: 46.8s\tremaining: 1m 16s\n",
      "381:\tlearn: 6.9485616\ttotal: 46.9s\tremaining: 1m 15s\n",
      "382:\tlearn: 6.9453964\ttotal: 47s\tremaining: 1m 15s\n",
      "383:\tlearn: 6.9422980\ttotal: 47.2s\tremaining: 1m 15s\n",
      "384:\tlearn: 6.9375151\ttotal: 47.3s\tremaining: 1m 15s\n",
      "385:\tlearn: 6.9343307\ttotal: 47.4s\tremaining: 1m 15s\n",
      "386:\tlearn: 6.9317745\ttotal: 47.5s\tremaining: 1m 15s\n",
      "387:\tlearn: 6.9281461\ttotal: 47.6s\tremaining: 1m 15s\n",
      "388:\tlearn: 6.9247567\ttotal: 47.8s\tremaining: 1m 15s\n",
      "389:\tlearn: 6.9218652\ttotal: 47.9s\tremaining: 1m 14s\n",
      "390:\tlearn: 6.9181830\ttotal: 48s\tremaining: 1m 14s\n",
      "391:\tlearn: 6.9148865\ttotal: 48.1s\tremaining: 1m 14s\n",
      "392:\tlearn: 6.9110122\ttotal: 48.2s\tremaining: 1m 14s\n",
      "393:\tlearn: 6.9073630\ttotal: 48.4s\tremaining: 1m 14s\n",
      "394:\tlearn: 6.9045289\ttotal: 48.5s\tremaining: 1m 14s\n",
      "395:\tlearn: 6.9011361\ttotal: 48.6s\tremaining: 1m 14s\n",
      "396:\tlearn: 6.8978173\ttotal: 48.7s\tremaining: 1m 14s\n",
      "397:\tlearn: 6.8947034\ttotal: 48.9s\tremaining: 1m 13s\n",
      "398:\tlearn: 6.8942710\ttotal: 49s\tremaining: 1m 13s\n",
      "399:\tlearn: 6.8907091\ttotal: 49.1s\tremaining: 1m 13s\n",
      "400:\tlearn: 6.8881365\ttotal: 49.2s\tremaining: 1m 13s\n",
      "401:\tlearn: 6.8850608\ttotal: 49.4s\tremaining: 1m 13s\n",
      "402:\tlearn: 6.8825527\ttotal: 49.5s\tremaining: 1m 13s\n",
      "403:\tlearn: 6.8791847\ttotal: 49.6s\tremaining: 1m 13s\n",
      "404:\tlearn: 6.8757340\ttotal: 49.8s\tremaining: 1m 13s\n",
      "405:\tlearn: 6.8725358\ttotal: 49.9s\tremaining: 1m 13s\n",
      "406:\tlearn: 6.8689866\ttotal: 50s\tremaining: 1m 12s\n",
      "407:\tlearn: 6.8657173\ttotal: 50.2s\tremaining: 1m 12s\n",
      "408:\tlearn: 6.8628813\ttotal: 50.3s\tremaining: 1m 12s\n",
      "409:\tlearn: 6.8600453\ttotal: 50.5s\tremaining: 1m 12s\n",
      "410:\tlearn: 6.8598227\ttotal: 50.6s\tremaining: 1m 12s\n",
      "411:\tlearn: 6.8567511\ttotal: 50.8s\tremaining: 1m 12s\n",
      "412:\tlearn: 6.8543251\ttotal: 50.9s\tremaining: 1m 12s\n",
      "413:\tlearn: 6.8510942\ttotal: 51s\tremaining: 1m 12s\n",
      "414:\tlearn: 6.8491675\ttotal: 51.2s\tremaining: 1m 12s\n",
      "415:\tlearn: 6.8469394\ttotal: 51.3s\tremaining: 1m 11s\n",
      "416:\tlearn: 6.8438617\ttotal: 51.4s\tremaining: 1m 11s\n",
      "417:\tlearn: 6.8399664\ttotal: 51.6s\tremaining: 1m 11s\n",
      "418:\tlearn: 6.8370438\ttotal: 51.7s\tremaining: 1m 11s\n",
      "419:\tlearn: 6.8331970\ttotal: 51.8s\tremaining: 1m 11s\n",
      "420:\tlearn: 6.8309581\ttotal: 52s\tremaining: 1m 11s\n",
      "421:\tlearn: 6.8286434\ttotal: 52.1s\tremaining: 1m 11s\n",
      "422:\tlearn: 6.8259055\ttotal: 52.2s\tremaining: 1m 11s\n",
      "423:\tlearn: 6.8226718\ttotal: 52.4s\tremaining: 1m 11s\n",
      "424:\tlearn: 6.8192257\ttotal: 52.5s\tremaining: 1m 11s\n",
      "425:\tlearn: 6.8176067\ttotal: 52.6s\tremaining: 1m 10s\n",
      "426:\tlearn: 6.8144595\ttotal: 52.8s\tremaining: 1m 10s\n",
      "427:\tlearn: 6.8124135\ttotal: 52.9s\tremaining: 1m 10s\n",
      "428:\tlearn: 6.8084706\ttotal: 53.1s\tremaining: 1m 10s\n",
      "429:\tlearn: 6.8056156\ttotal: 53.3s\tremaining: 1m 10s\n",
      "430:\tlearn: 6.8020675\ttotal: 53.4s\tremaining: 1m 10s\n",
      "431:\tlearn: 6.7984158\ttotal: 53.5s\tremaining: 1m 10s\n",
      "432:\tlearn: 6.7938865\ttotal: 53.7s\tremaining: 1m 10s\n",
      "433:\tlearn: 6.7902623\ttotal: 53.8s\tremaining: 1m 10s\n",
      "434:\tlearn: 6.7877832\ttotal: 53.9s\tremaining: 1m 10s\n",
      "435:\tlearn: 6.7841621\ttotal: 54s\tremaining: 1m 9s\n",
      "436:\tlearn: 6.7809716\ttotal: 54.2s\tremaining: 1m 9s\n",
      "437:\tlearn: 6.7774874\ttotal: 54.3s\tremaining: 1m 9s\n",
      "438:\tlearn: 6.7749555\ttotal: 54.4s\tremaining: 1m 9s\n",
      "439:\tlearn: 6.7745582\ttotal: 54.5s\tremaining: 1m 9s\n",
      "440:\tlearn: 6.7714033\ttotal: 54.7s\tremaining: 1m 9s\n",
      "441:\tlearn: 6.7684059\ttotal: 54.8s\tremaining: 1m 9s\n",
      "442:\tlearn: 6.7652517\ttotal: 54.9s\tremaining: 1m 9s\n",
      "443:\tlearn: 6.7620191\ttotal: 55s\tremaining: 1m 8s\n",
      "444:\tlearn: 6.7590174\ttotal: 55.1s\tremaining: 1m 8s\n",
      "445:\tlearn: 6.7560858\ttotal: 55.3s\tremaining: 1m 8s\n",
      "446:\tlearn: 6.7529281\ttotal: 55.4s\tremaining: 1m 8s\n",
      "447:\tlearn: 6.7497468\ttotal: 55.5s\tremaining: 1m 8s\n",
      "448:\tlearn: 6.7459312\ttotal: 55.6s\tremaining: 1m 8s\n",
      "449:\tlearn: 6.7426299\ttotal: 55.8s\tremaining: 1m 8s\n",
      "450:\tlearn: 6.7382303\ttotal: 55.9s\tremaining: 1m 8s\n",
      "451:\tlearn: 6.7358618\ttotal: 56s\tremaining: 1m 7s\n",
      "452:\tlearn: 6.7329939\ttotal: 56.2s\tremaining: 1m 7s\n",
      "453:\tlearn: 6.7294541\ttotal: 56.3s\tremaining: 1m 7s\n",
      "454:\tlearn: 6.7271295\ttotal: 56.4s\tremaining: 1m 7s\n",
      "455:\tlearn: 6.7237301\ttotal: 56.5s\tremaining: 1m 7s\n",
      "456:\tlearn: 6.7192207\ttotal: 56.6s\tremaining: 1m 7s\n",
      "457:\tlearn: 6.7174015\ttotal: 56.8s\tremaining: 1m 7s\n",
      "458:\tlearn: 6.7147573\ttotal: 56.9s\tremaining: 1m 7s\n",
      "459:\tlearn: 6.7124972\ttotal: 57s\tremaining: 1m 6s\n",
      "460:\tlearn: 6.7096720\ttotal: 57.1s\tremaining: 1m 6s\n",
      "461:\tlearn: 6.7072520\ttotal: 57.2s\tremaining: 1m 6s\n",
      "462:\tlearn: 6.7050300\ttotal: 57.4s\tremaining: 1m 6s\n",
      "463:\tlearn: 6.7014735\ttotal: 57.5s\tremaining: 1m 6s\n",
      "464:\tlearn: 6.7010512\ttotal: 57.6s\tremaining: 1m 6s\n",
      "465:\tlearn: 6.6975767\ttotal: 57.7s\tremaining: 1m 6s\n",
      "466:\tlearn: 6.6957517\ttotal: 57.8s\tremaining: 1m 5s\n",
      "467:\tlearn: 6.6935646\ttotal: 57.9s\tremaining: 1m 5s\n",
      "468:\tlearn: 6.6902398\ttotal: 58s\tremaining: 1m 5s\n",
      "469:\tlearn: 6.6867849\ttotal: 58.2s\tremaining: 1m 5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470:\tlearn: 6.6846079\ttotal: 58.3s\tremaining: 1m 5s\n",
      "471:\tlearn: 6.6814090\ttotal: 58.4s\tremaining: 1m 5s\n",
      "472:\tlearn: 6.6785185\ttotal: 58.5s\tremaining: 1m 5s\n",
      "473:\tlearn: 6.6756706\ttotal: 58.7s\tremaining: 1m 5s\n",
      "474:\tlearn: 6.6723097\ttotal: 58.8s\tremaining: 1m 4s\n",
      "475:\tlearn: 6.6704296\ttotal: 58.9s\tremaining: 1m 4s\n",
      "476:\tlearn: 6.6679033\ttotal: 59s\tremaining: 1m 4s\n",
      "477:\tlearn: 6.6641493\ttotal: 59.2s\tremaining: 1m 4s\n",
      "478:\tlearn: 6.6607228\ttotal: 59.3s\tremaining: 1m 4s\n",
      "479:\tlearn: 6.6585967\ttotal: 59.4s\tremaining: 1m 4s\n",
      "480:\tlearn: 6.6557399\ttotal: 59.6s\tremaining: 1m 4s\n",
      "481:\tlearn: 6.6532047\ttotal: 59.7s\tremaining: 1m 4s\n",
      "482:\tlearn: 6.6495809\ttotal: 59.8s\tremaining: 1m 4s\n",
      "483:\tlearn: 6.6468179\ttotal: 60s\tremaining: 1m 3s\n",
      "484:\tlearn: 6.6442837\ttotal: 1m\tremaining: 1m 3s\n",
      "485:\tlearn: 6.6410888\ttotal: 1m\tremaining: 1m 3s\n",
      "486:\tlearn: 6.6386595\ttotal: 1m\tremaining: 1m 3s\n",
      "487:\tlearn: 6.6351921\ttotal: 1m\tremaining: 1m 3s\n",
      "488:\tlearn: 6.6324010\ttotal: 1m\tremaining: 1m 3s\n",
      "489:\tlearn: 6.6289673\ttotal: 1m\tremaining: 1m 3s\n",
      "490:\tlearn: 6.6249459\ttotal: 1m\tremaining: 1m 3s\n",
      "491:\tlearn: 6.6217701\ttotal: 1m\tremaining: 1m 2s\n",
      "492:\tlearn: 6.6180148\ttotal: 1m 1s\tremaining: 1m 2s\n",
      "493:\tlearn: 6.6151652\ttotal: 1m 1s\tremaining: 1m 2s\n",
      "494:\tlearn: 6.6135542\ttotal: 1m 1s\tremaining: 1m 2s\n",
      "495:\tlearn: 6.6108780\ttotal: 1m 1s\tremaining: 1m 2s\n",
      "496:\tlearn: 6.6079189\ttotal: 1m 1s\tremaining: 1m 2s\n",
      "497:\tlearn: 6.6044616\ttotal: 1m 1s\tremaining: 1m 2s\n",
      "498:\tlearn: 6.6012578\ttotal: 1m 1s\tremaining: 1m 2s\n",
      "499:\tlearn: 6.5971738\ttotal: 1m 1s\tremaining: 1m 1s\n",
      "500:\tlearn: 6.5951924\ttotal: 1m 2s\tremaining: 1m 1s\n",
      "501:\tlearn: 6.5920880\ttotal: 1m 2s\tremaining: 1m 1s\n",
      "502:\tlearn: 6.5895349\ttotal: 1m 2s\tremaining: 1m 1s\n",
      "503:\tlearn: 6.5870231\ttotal: 1m 2s\tremaining: 1m 1s\n",
      "504:\tlearn: 6.5843021\ttotal: 1m 2s\tremaining: 1m 1s\n",
      "505:\tlearn: 6.5828565\ttotal: 1m 2s\tremaining: 1m 1s\n",
      "506:\tlearn: 6.5795488\ttotal: 1m 2s\tremaining: 1m 1s\n",
      "507:\tlearn: 6.5762586\ttotal: 1m 2s\tremaining: 1m\n",
      "508:\tlearn: 6.5740370\ttotal: 1m 3s\tremaining: 1m\n",
      "509:\tlearn: 6.5722405\ttotal: 1m 3s\tremaining: 1m\n",
      "510:\tlearn: 6.5686229\ttotal: 1m 3s\tremaining: 1m\n",
      "511:\tlearn: 6.5647381\ttotal: 1m 3s\tremaining: 1m\n",
      "512:\tlearn: 6.5622611\ttotal: 1m 3s\tremaining: 1m\n",
      "513:\tlearn: 6.5597838\ttotal: 1m 3s\tremaining: 1m\n",
      "514:\tlearn: 6.5576661\ttotal: 1m 3s\tremaining: 1m\n",
      "515:\tlearn: 6.5549912\ttotal: 1m 3s\tremaining: 59.9s\n",
      "516:\tlearn: 6.5520735\ttotal: 1m 3s\tremaining: 59.7s\n",
      "517:\tlearn: 6.5503076\ttotal: 1m 4s\tremaining: 59.6s\n",
      "518:\tlearn: 6.5471530\ttotal: 1m 4s\tremaining: 59.5s\n",
      "519:\tlearn: 6.5442584\ttotal: 1m 4s\tremaining: 59.3s\n",
      "520:\tlearn: 6.5410944\ttotal: 1m 4s\tremaining: 59.2s\n",
      "521:\tlearn: 6.5383384\ttotal: 1m 4s\tremaining: 59s\n",
      "522:\tlearn: 6.5379521\ttotal: 1m 4s\tremaining: 58.9s\n",
      "523:\tlearn: 6.5338056\ttotal: 1m 4s\tremaining: 58.8s\n",
      "524:\tlearn: 6.5300388\ttotal: 1m 4s\tremaining: 58.6s\n",
      "525:\tlearn: 6.5278569\ttotal: 1m 4s\tremaining: 58.5s\n",
      "526:\tlearn: 6.5252580\ttotal: 1m 5s\tremaining: 58.4s\n",
      "527:\tlearn: 6.5212981\ttotal: 1m 5s\tremaining: 58.2s\n",
      "528:\tlearn: 6.5185778\ttotal: 1m 5s\tremaining: 58.1s\n",
      "529:\tlearn: 6.5139208\ttotal: 1m 5s\tremaining: 58s\n",
      "530:\tlearn: 6.5112802\ttotal: 1m 5s\tremaining: 57.8s\n",
      "531:\tlearn: 6.5076442\ttotal: 1m 5s\tremaining: 57.7s\n",
      "532:\tlearn: 6.5049481\ttotal: 1m 5s\tremaining: 57.6s\n",
      "533:\tlearn: 6.5017359\ttotal: 1m 5s\tremaining: 57.4s\n",
      "534:\tlearn: 6.4994294\ttotal: 1m 5s\tremaining: 57.3s\n",
      "535:\tlearn: 6.4969596\ttotal: 1m 6s\tremaining: 57.2s\n",
      "536:\tlearn: 6.4930861\ttotal: 1m 6s\tremaining: 57s\n",
      "537:\tlearn: 6.4901639\ttotal: 1m 6s\tremaining: 56.9s\n",
      "538:\tlearn: 6.4874572\ttotal: 1m 6s\tremaining: 56.8s\n",
      "539:\tlearn: 6.4851159\ttotal: 1m 6s\tremaining: 56.6s\n",
      "540:\tlearn: 6.4823308\ttotal: 1m 6s\tremaining: 56.5s\n",
      "541:\tlearn: 6.4793154\ttotal: 1m 6s\tremaining: 56.4s\n",
      "542:\tlearn: 6.4761253\ttotal: 1m 6s\tremaining: 56.3s\n",
      "543:\tlearn: 6.4759330\ttotal: 1m 6s\tremaining: 56.1s\n",
      "544:\tlearn: 6.4725513\ttotal: 1m 7s\tremaining: 56s\n",
      "545:\tlearn: 6.4703364\ttotal: 1m 7s\tremaining: 55.8s\n",
      "546:\tlearn: 6.4674595\ttotal: 1m 7s\tremaining: 55.7s\n",
      "547:\tlearn: 6.4642459\ttotal: 1m 7s\tremaining: 55.6s\n",
      "548:\tlearn: 6.4617746\ttotal: 1m 7s\tremaining: 55.5s\n",
      "549:\tlearn: 6.4585523\ttotal: 1m 7s\tremaining: 55.3s\n",
      "550:\tlearn: 6.4551763\ttotal: 1m 7s\tremaining: 55.2s\n",
      "551:\tlearn: 6.4522852\ttotal: 1m 7s\tremaining: 55.1s\n",
      "552:\tlearn: 6.4487003\ttotal: 1m 8s\tremaining: 55s\n",
      "553:\tlearn: 6.4461530\ttotal: 1m 8s\tremaining: 54.8s\n",
      "554:\tlearn: 6.4437936\ttotal: 1m 8s\tremaining: 54.7s\n",
      "555:\tlearn: 6.4404072\ttotal: 1m 8s\tremaining: 54.6s\n",
      "556:\tlearn: 6.4378305\ttotal: 1m 8s\tremaining: 54.5s\n",
      "557:\tlearn: 6.4353947\ttotal: 1m 8s\tremaining: 54.4s\n",
      "558:\tlearn: 6.4352117\ttotal: 1m 8s\tremaining: 54.2s\n",
      "559:\tlearn: 6.4320700\ttotal: 1m 8s\tremaining: 54.1s\n",
      "560:\tlearn: 6.4285853\ttotal: 1m 8s\tremaining: 54s\n",
      "561:\tlearn: 6.4258152\ttotal: 1m 9s\tremaining: 53.9s\n",
      "562:\tlearn: 6.4223118\ttotal: 1m 9s\tremaining: 53.7s\n",
      "563:\tlearn: 6.4199797\ttotal: 1m 9s\tremaining: 53.6s\n",
      "564:\tlearn: 6.4181424\ttotal: 1m 9s\tremaining: 53.5s\n",
      "565:\tlearn: 6.4151022\ttotal: 1m 9s\tremaining: 53.4s\n",
      "566:\tlearn: 6.4129374\ttotal: 1m 9s\tremaining: 53.2s\n",
      "567:\tlearn: 6.4100396\ttotal: 1m 9s\tremaining: 53.1s\n",
      "568:\tlearn: 6.4079947\ttotal: 1m 9s\tremaining: 53s\n",
      "569:\tlearn: 6.4053209\ttotal: 1m 10s\tremaining: 52.9s\n",
      "570:\tlearn: 6.4027606\ttotal: 1m 10s\tremaining: 52.7s\n",
      "571:\tlearn: 6.3990935\ttotal: 1m 10s\tremaining: 52.6s\n",
      "572:\tlearn: 6.3958191\ttotal: 1m 10s\tremaining: 52.5s\n",
      "573:\tlearn: 6.3929959\ttotal: 1m 10s\tremaining: 52.4s\n",
      "574:\tlearn: 6.3900377\ttotal: 1m 10s\tremaining: 52.2s\n",
      "575:\tlearn: 6.3877929\ttotal: 1m 10s\tremaining: 52.1s\n",
      "576:\tlearn: 6.3847233\ttotal: 1m 10s\tremaining: 52s\n",
      "577:\tlearn: 6.3819385\ttotal: 1m 10s\tremaining: 51.8s\n",
      "578:\tlearn: 6.3783618\ttotal: 1m 11s\tremaining: 51.7s\n",
      "579:\tlearn: 6.3751902\ttotal: 1m 11s\tremaining: 51.6s\n",
      "580:\tlearn: 6.3725295\ttotal: 1m 11s\tremaining: 51.4s\n",
      "581:\tlearn: 6.3691453\ttotal: 1m 11s\tremaining: 51.3s\n",
      "582:\tlearn: 6.3667112\ttotal: 1m 11s\tremaining: 51.2s\n",
      "583:\tlearn: 6.3632400\ttotal: 1m 11s\tremaining: 51s\n",
      "584:\tlearn: 6.3616249\ttotal: 1m 11s\tremaining: 50.9s\n",
      "585:\tlearn: 6.3585085\ttotal: 1m 11s\tremaining: 50.8s\n",
      "586:\tlearn: 6.3563075\ttotal: 1m 11s\tremaining: 50.6s\n",
      "587:\tlearn: 6.3534243\ttotal: 1m 12s\tremaining: 50.5s\n",
      "588:\tlearn: 6.3508360\ttotal: 1m 12s\tremaining: 50.4s\n",
      "589:\tlearn: 6.3464367\ttotal: 1m 12s\tremaining: 50.3s\n",
      "590:\tlearn: 6.3433359\ttotal: 1m 12s\tremaining: 50.1s\n",
      "591:\tlearn: 6.3407656\ttotal: 1m 12s\tremaining: 50s\n",
      "592:\tlearn: 6.3377665\ttotal: 1m 12s\tremaining: 49.9s\n",
      "593:\tlearn: 6.3349869\ttotal: 1m 12s\tremaining: 49.7s\n",
      "594:\tlearn: 6.3314983\ttotal: 1m 12s\tremaining: 49.6s\n",
      "595:\tlearn: 6.3290392\ttotal: 1m 12s\tremaining: 49.5s\n",
      "596:\tlearn: 6.3257233\ttotal: 1m 13s\tremaining: 49.3s\n",
      "597:\tlearn: 6.3232181\ttotal: 1m 13s\tremaining: 49.2s\n",
      "598:\tlearn: 6.3185092\ttotal: 1m 13s\tremaining: 49.1s\n",
      "599:\tlearn: 6.3158212\ttotal: 1m 13s\tremaining: 49s\n",
      "600:\tlearn: 6.3131742\ttotal: 1m 13s\tremaining: 48.8s\n",
      "601:\tlearn: 6.3097711\ttotal: 1m 13s\tremaining: 48.7s\n",
      "602:\tlearn: 6.3068491\ttotal: 1m 13s\tremaining: 48.6s\n",
      "603:\tlearn: 6.3039608\ttotal: 1m 13s\tremaining: 48.4s\n",
      "604:\tlearn: 6.3011485\ttotal: 1m 13s\tremaining: 48.3s\n",
      "605:\tlearn: 6.2987091\ttotal: 1m 14s\tremaining: 48.2s\n",
      "606:\tlearn: 6.2953336\ttotal: 1m 14s\tremaining: 48s\n",
      "607:\tlearn: 6.2919982\ttotal: 1m 14s\tremaining: 47.9s\n",
      "608:\tlearn: 6.2897442\ttotal: 1m 14s\tremaining: 47.8s\n",
      "609:\tlearn: 6.2860451\ttotal: 1m 14s\tremaining: 47.7s\n",
      "610:\tlearn: 6.2822892\ttotal: 1m 14s\tremaining: 47.5s\n",
      "611:\tlearn: 6.2797136\ttotal: 1m 14s\tremaining: 47.4s\n",
      "612:\tlearn: 6.2776014\ttotal: 1m 14s\tremaining: 47.3s\n",
      "613:\tlearn: 6.2757282\ttotal: 1m 15s\tremaining: 47.2s\n",
      "614:\tlearn: 6.2741433\ttotal: 1m 15s\tremaining: 47s\n",
      "615:\tlearn: 6.2713202\ttotal: 1m 15s\tremaining: 46.9s\n",
      "616:\tlearn: 6.2676629\ttotal: 1m 15s\tremaining: 46.8s\n",
      "617:\tlearn: 6.2649737\ttotal: 1m 15s\tremaining: 46.7s\n",
      "618:\tlearn: 6.2621194\ttotal: 1m 15s\tremaining: 46.5s\n",
      "619:\tlearn: 6.2590332\ttotal: 1m 15s\tremaining: 46.4s\n",
      "620:\tlearn: 6.2550670\ttotal: 1m 15s\tremaining: 46.3s\n",
      "621:\tlearn: 6.2534132\ttotal: 1m 15s\tremaining: 46.2s\n",
      "622:\tlearn: 6.2515144\ttotal: 1m 16s\tremaining: 46.1s\n",
      "623:\tlearn: 6.2491829\ttotal: 1m 16s\tremaining: 45.9s\n",
      "624:\tlearn: 6.2463016\ttotal: 1m 16s\tremaining: 45.8s\n",
      "625:\tlearn: 6.2442002\ttotal: 1m 16s\tremaining: 45.7s\n",
      "626:\tlearn: 6.2416473\ttotal: 1m 16s\tremaining: 45.6s\n",
      "627:\tlearn: 6.2391655\ttotal: 1m 16s\tremaining: 45.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628:\tlearn: 6.2361925\ttotal: 1m 16s\tremaining: 45.3s\n",
      "629:\tlearn: 6.2332698\ttotal: 1m 16s\tremaining: 45.2s\n",
      "630:\tlearn: 6.2303337\ttotal: 1m 17s\tremaining: 45.1s\n",
      "631:\tlearn: 6.2265431\ttotal: 1m 17s\tremaining: 45s\n",
      "632:\tlearn: 6.2243424\ttotal: 1m 17s\tremaining: 44.8s\n",
      "633:\tlearn: 6.2217792\ttotal: 1m 17s\tremaining: 44.7s\n",
      "634:\tlearn: 6.2189229\ttotal: 1m 17s\tremaining: 44.6s\n",
      "635:\tlearn: 6.2160858\ttotal: 1m 17s\tremaining: 44.5s\n",
      "636:\tlearn: 6.2124884\ttotal: 1m 17s\tremaining: 44.4s\n",
      "637:\tlearn: 6.2082296\ttotal: 1m 17s\tremaining: 44.3s\n",
      "638:\tlearn: 6.2054409\ttotal: 1m 18s\tremaining: 44.2s\n",
      "639:\tlearn: 6.2033462\ttotal: 1m 18s\tremaining: 44s\n",
      "640:\tlearn: 6.2003467\ttotal: 1m 18s\tremaining: 43.9s\n",
      "641:\tlearn: 6.2001782\ttotal: 1m 18s\tremaining: 43.8s\n",
      "642:\tlearn: 6.1976888\ttotal: 1m 18s\tremaining: 43.6s\n",
      "643:\tlearn: 6.1950842\ttotal: 1m 18s\tremaining: 43.5s\n",
      "644:\tlearn: 6.1924449\ttotal: 1m 18s\tremaining: 43.4s\n",
      "645:\tlearn: 6.1896087\ttotal: 1m 18s\tremaining: 43.3s\n",
      "646:\tlearn: 6.1867202\ttotal: 1m 19s\tremaining: 43.2s\n",
      "647:\tlearn: 6.1833623\ttotal: 1m 19s\tremaining: 43s\n",
      "648:\tlearn: 6.1798879\ttotal: 1m 19s\tremaining: 42.9s\n",
      "649:\tlearn: 6.1760180\ttotal: 1m 19s\tremaining: 42.8s\n",
      "650:\tlearn: 6.1734672\ttotal: 1m 19s\tremaining: 42.7s\n",
      "651:\tlearn: 6.1719380\ttotal: 1m 19s\tremaining: 42.6s\n",
      "652:\tlearn: 6.1695723\ttotal: 1m 19s\tremaining: 42.4s\n",
      "653:\tlearn: 6.1671595\ttotal: 1m 19s\tremaining: 42.3s\n",
      "654:\tlearn: 6.1651449\ttotal: 1m 20s\tremaining: 42.2s\n",
      "655:\tlearn: 6.1612017\ttotal: 1m 20s\tremaining: 42.1s\n",
      "656:\tlearn: 6.1584370\ttotal: 1m 20s\tremaining: 41.9s\n",
      "657:\tlearn: 6.1557987\ttotal: 1m 20s\tremaining: 41.8s\n",
      "658:\tlearn: 6.1525783\ttotal: 1m 20s\tremaining: 41.7s\n",
      "659:\tlearn: 6.1481132\ttotal: 1m 20s\tremaining: 41.5s\n",
      "660:\tlearn: 6.1452786\ttotal: 1m 20s\tremaining: 41.4s\n",
      "661:\tlearn: 6.1428116\ttotal: 1m 20s\tremaining: 41.3s\n",
      "662:\tlearn: 6.1400172\ttotal: 1m 20s\tremaining: 41.2s\n",
      "663:\tlearn: 6.1373513\ttotal: 1m 21s\tremaining: 41s\n",
      "664:\tlearn: 6.1355315\ttotal: 1m 21s\tremaining: 40.9s\n",
      "665:\tlearn: 6.1339489\ttotal: 1m 21s\tremaining: 40.8s\n",
      "666:\tlearn: 6.1303912\ttotal: 1m 21s\tremaining: 40.7s\n",
      "667:\tlearn: 6.1281438\ttotal: 1m 21s\tremaining: 40.6s\n",
      "668:\tlearn: 6.1258549\ttotal: 1m 21s\tremaining: 40.5s\n",
      "669:\tlearn: 6.1212016\ttotal: 1m 21s\tremaining: 40.3s\n",
      "670:\tlearn: 6.1189652\ttotal: 1m 22s\tremaining: 40.2s\n",
      "671:\tlearn: 6.1137841\ttotal: 1m 22s\tremaining: 40.1s\n",
      "672:\tlearn: 6.1136217\ttotal: 1m 22s\tremaining: 40s\n",
      "673:\tlearn: 6.1109355\ttotal: 1m 22s\tremaining: 39.9s\n",
      "674:\tlearn: 6.1076948\ttotal: 1m 22s\tremaining: 39.7s\n",
      "675:\tlearn: 6.1048353\ttotal: 1m 22s\tremaining: 39.6s\n",
      "676:\tlearn: 6.1012678\ttotal: 1m 22s\tremaining: 39.5s\n",
      "677:\tlearn: 6.0987113\ttotal: 1m 22s\tremaining: 39.4s\n",
      "678:\tlearn: 6.0951875\ttotal: 1m 23s\tremaining: 39.3s\n",
      "679:\tlearn: 6.0918383\ttotal: 1m 23s\tremaining: 39.1s\n",
      "680:\tlearn: 6.0898068\ttotal: 1m 23s\tremaining: 39s\n",
      "681:\tlearn: 6.0869164\ttotal: 1m 23s\tremaining: 38.9s\n",
      "682:\tlearn: 6.0848547\ttotal: 1m 23s\tremaining: 38.8s\n",
      "683:\tlearn: 6.0846945\ttotal: 1m 23s\tremaining: 38.6s\n",
      "684:\tlearn: 6.0819603\ttotal: 1m 23s\tremaining: 38.5s\n",
      "685:\tlearn: 6.0792340\ttotal: 1m 23s\tremaining: 38.4s\n",
      "686:\tlearn: 6.0767122\ttotal: 1m 24s\tremaining: 38.3s\n",
      "687:\tlearn: 6.0736417\ttotal: 1m 24s\tremaining: 38.2s\n",
      "688:\tlearn: 6.0707792\ttotal: 1m 24s\tremaining: 38s\n",
      "689:\tlearn: 6.0671217\ttotal: 1m 24s\tremaining: 37.9s\n",
      "690:\tlearn: 6.0641642\ttotal: 1m 24s\tremaining: 37.8s\n",
      "691:\tlearn: 6.0605995\ttotal: 1m 24s\tremaining: 37.7s\n",
      "692:\tlearn: 6.0575850\ttotal: 1m 24s\tremaining: 37.6s\n",
      "693:\tlearn: 6.0540768\ttotal: 1m 24s\tremaining: 37.5s\n",
      "694:\tlearn: 6.0524620\ttotal: 1m 25s\tremaining: 37.3s\n",
      "695:\tlearn: 6.0496028\ttotal: 1m 25s\tremaining: 37.2s\n",
      "696:\tlearn: 6.0463397\ttotal: 1m 25s\tremaining: 37.1s\n",
      "697:\tlearn: 6.0431904\ttotal: 1m 25s\tremaining: 37s\n",
      "698:\tlearn: 6.0409896\ttotal: 1m 25s\tremaining: 36.9s\n",
      "699:\tlearn: 6.0377029\ttotal: 1m 25s\tremaining: 36.8s\n",
      "700:\tlearn: 6.0344843\ttotal: 1m 25s\tremaining: 36.7s\n",
      "701:\tlearn: 6.0324527\ttotal: 1m 26s\tremaining: 36.6s\n",
      "702:\tlearn: 6.0308688\ttotal: 1m 26s\tremaining: 36.4s\n",
      "703:\tlearn: 6.0307119\ttotal: 1m 26s\tremaining: 36.3s\n",
      "704:\tlearn: 6.0280602\ttotal: 1m 26s\tremaining: 36.2s\n",
      "705:\tlearn: 6.0257425\ttotal: 1m 26s\tremaining: 36.1s\n",
      "706:\tlearn: 6.0238676\ttotal: 1m 26s\tremaining: 36s\n",
      "707:\tlearn: 6.0201007\ttotal: 1m 26s\tremaining: 35.8s\n",
      "708:\tlearn: 6.0180908\ttotal: 1m 27s\tremaining: 35.7s\n",
      "709:\tlearn: 6.0149983\ttotal: 1m 27s\tremaining: 35.6s\n",
      "710:\tlearn: 6.0114131\ttotal: 1m 27s\tremaining: 35.5s\n",
      "711:\tlearn: 6.0091534\ttotal: 1m 27s\tremaining: 35.4s\n",
      "712:\tlearn: 6.0071215\ttotal: 1m 27s\tremaining: 35.3s\n",
      "713:\tlearn: 6.0049458\ttotal: 1m 27s\tremaining: 35.1s\n",
      "714:\tlearn: 6.0025522\ttotal: 1m 27s\tremaining: 35s\n",
      "715:\tlearn: 5.9999497\ttotal: 1m 27s\tremaining: 34.9s\n",
      "716:\tlearn: 5.9966015\ttotal: 1m 28s\tremaining: 34.8s\n",
      "717:\tlearn: 5.9940415\ttotal: 1m 28s\tremaining: 34.6s\n",
      "718:\tlearn: 5.9904274\ttotal: 1m 28s\tremaining: 34.5s\n",
      "719:\tlearn: 5.9890441\ttotal: 1m 28s\tremaining: 34.4s\n",
      "720:\tlearn: 5.9861286\ttotal: 1m 28s\tremaining: 34.3s\n",
      "721:\tlearn: 5.9832476\ttotal: 1m 28s\tremaining: 34.1s\n",
      "722:\tlearn: 5.9808132\ttotal: 1m 28s\tremaining: 34s\n",
      "723:\tlearn: 5.9777047\ttotal: 1m 28s\tremaining: 33.9s\n",
      "724:\tlearn: 5.9775543\ttotal: 1m 29s\tremaining: 33.8s\n",
      "725:\tlearn: 5.9756042\ttotal: 1m 29s\tremaining: 33.6s\n",
      "726:\tlearn: 5.9738381\ttotal: 1m 29s\tremaining: 33.5s\n",
      "727:\tlearn: 5.9715683\ttotal: 1m 29s\tremaining: 33.4s\n",
      "728:\tlearn: 5.9686796\ttotal: 1m 29s\tremaining: 33.3s\n",
      "729:\tlearn: 5.9680068\ttotal: 1m 29s\tremaining: 33.2s\n",
      "730:\tlearn: 5.9653001\ttotal: 1m 29s\tremaining: 33s\n",
      "731:\tlearn: 5.9651593\ttotal: 1m 29s\tremaining: 32.9s\n",
      "732:\tlearn: 5.9626224\ttotal: 1m 29s\tremaining: 32.8s\n",
      "733:\tlearn: 5.9601694\ttotal: 1m 30s\tremaining: 32.7s\n",
      "734:\tlearn: 5.9578117\ttotal: 1m 30s\tremaining: 32.5s\n",
      "735:\tlearn: 5.9549750\ttotal: 1m 30s\tremaining: 32.4s\n",
      "736:\tlearn: 5.9530258\ttotal: 1m 30s\tremaining: 32.3s\n",
      "737:\tlearn: 5.9498898\ttotal: 1m 30s\tremaining: 32.2s\n",
      "738:\tlearn: 5.9465988\ttotal: 1m 30s\tremaining: 32s\n",
      "739:\tlearn: 5.9441147\ttotal: 1m 30s\tremaining: 31.9s\n",
      "740:\tlearn: 5.9415681\ttotal: 1m 31s\tremaining: 31.8s\n",
      "741:\tlearn: 5.9387284\ttotal: 1m 31s\tremaining: 31.7s\n",
      "742:\tlearn: 5.9360093\ttotal: 1m 31s\tremaining: 31.6s\n",
      "743:\tlearn: 5.9324534\ttotal: 1m 31s\tremaining: 31.4s\n",
      "744:\tlearn: 5.9293082\ttotal: 1m 31s\tremaining: 31.3s\n",
      "745:\tlearn: 5.9255887\ttotal: 1m 31s\tremaining: 31.2s\n",
      "746:\tlearn: 5.9233105\ttotal: 1m 31s\tremaining: 31.1s\n",
      "747:\tlearn: 5.9228457\ttotal: 1m 31s\tremaining: 31s\n",
      "748:\tlearn: 5.9205758\ttotal: 1m 32s\tremaining: 30.8s\n",
      "749:\tlearn: 5.9182576\ttotal: 1m 32s\tremaining: 30.7s\n",
      "750:\tlearn: 5.9157178\ttotal: 1m 32s\tremaining: 30.6s\n",
      "751:\tlearn: 5.9133762\ttotal: 1m 32s\tremaining: 30.5s\n",
      "752:\tlearn: 5.9117431\ttotal: 1m 32s\tremaining: 30.4s\n",
      "753:\tlearn: 5.9095328\ttotal: 1m 32s\tremaining: 30.3s\n",
      "754:\tlearn: 5.9074793\ttotal: 1m 32s\tremaining: 30.1s\n",
      "755:\tlearn: 5.9037479\ttotal: 1m 32s\tremaining: 30s\n",
      "756:\tlearn: 5.9009482\ttotal: 1m 33s\tremaining: 29.9s\n",
      "757:\tlearn: 5.8987567\ttotal: 1m 33s\tremaining: 29.8s\n",
      "758:\tlearn: 5.8962825\ttotal: 1m 33s\tremaining: 29.7s\n",
      "759:\tlearn: 5.8930388\ttotal: 1m 33s\tremaining: 29.5s\n",
      "760:\tlearn: 5.8900158\ttotal: 1m 33s\tremaining: 29.4s\n",
      "761:\tlearn: 5.8875257\ttotal: 1m 33s\tremaining: 29.3s\n",
      "762:\tlearn: 5.8847223\ttotal: 1m 33s\tremaining: 29.2s\n",
      "763:\tlearn: 5.8831456\ttotal: 1m 34s\tremaining: 29s\n",
      "764:\tlearn: 5.8793564\ttotal: 1m 34s\tremaining: 28.9s\n",
      "765:\tlearn: 5.8763093\ttotal: 1m 34s\tremaining: 28.8s\n",
      "766:\tlearn: 5.8738275\ttotal: 1m 34s\tremaining: 28.7s\n",
      "767:\tlearn: 5.8715117\ttotal: 1m 34s\tremaining: 28.6s\n",
      "768:\tlearn: 5.8686990\ttotal: 1m 34s\tremaining: 28.4s\n",
      "769:\tlearn: 5.8659971\ttotal: 1m 34s\tremaining: 28.3s\n",
      "770:\tlearn: 5.8624401\ttotal: 1m 34s\tremaining: 28.2s\n",
      "771:\tlearn: 5.8601271\ttotal: 1m 35s\tremaining: 28.1s\n",
      "772:\tlearn: 5.8584645\ttotal: 1m 35s\tremaining: 27.9s\n",
      "773:\tlearn: 5.8566686\ttotal: 1m 35s\tremaining: 27.8s\n",
      "774:\tlearn: 5.8542774\ttotal: 1m 35s\tremaining: 27.7s\n",
      "775:\tlearn: 5.8509643\ttotal: 1m 35s\tremaining: 27.6s\n",
      "776:\tlearn: 5.8476571\ttotal: 1m 35s\tremaining: 27.5s\n",
      "777:\tlearn: 5.8446460\ttotal: 1m 35s\tremaining: 27.3s\n",
      "778:\tlearn: 5.8417562\ttotal: 1m 35s\tremaining: 27.2s\n",
      "779:\tlearn: 5.8387012\ttotal: 1m 36s\tremaining: 27.1s\n",
      "780:\tlearn: 5.8383219\ttotal: 1m 36s\tremaining: 27s\n",
      "781:\tlearn: 5.8357083\ttotal: 1m 36s\tremaining: 26.8s\n",
      "782:\tlearn: 5.8327505\ttotal: 1m 36s\tremaining: 26.7s\n",
      "783:\tlearn: 5.8305499\ttotal: 1m 36s\tremaining: 26.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784:\tlearn: 5.8275165\ttotal: 1m 36s\tremaining: 26.5s\n",
      "785:\tlearn: 5.8255437\ttotal: 1m 36s\tremaining: 26.3s\n",
      "786:\tlearn: 5.8216685\ttotal: 1m 36s\tremaining: 26.2s\n",
      "787:\tlearn: 5.8187036\ttotal: 1m 37s\tremaining: 26.1s\n",
      "788:\tlearn: 5.8170066\ttotal: 1m 37s\tremaining: 26s\n",
      "789:\tlearn: 5.8148339\ttotal: 1m 37s\tremaining: 25.8s\n",
      "790:\tlearn: 5.8125110\ttotal: 1m 37s\tremaining: 25.7s\n",
      "791:\tlearn: 5.8099953\ttotal: 1m 37s\tremaining: 25.6s\n",
      "792:\tlearn: 5.8075349\ttotal: 1m 37s\tremaining: 25.5s\n",
      "793:\tlearn: 5.8043893\ttotal: 1m 37s\tremaining: 25.4s\n",
      "794:\tlearn: 5.8019313\ttotal: 1m 37s\tremaining: 25.2s\n",
      "795:\tlearn: 5.7990089\ttotal: 1m 38s\tremaining: 25.1s\n",
      "796:\tlearn: 5.7964075\ttotal: 1m 38s\tremaining: 25s\n",
      "797:\tlearn: 5.7940441\ttotal: 1m 38s\tremaining: 24.9s\n",
      "798:\tlearn: 5.7909128\ttotal: 1m 38s\tremaining: 24.8s\n",
      "799:\tlearn: 5.7884386\ttotal: 1m 38s\tremaining: 24.7s\n",
      "800:\tlearn: 5.7865149\ttotal: 1m 38s\tremaining: 24.5s\n",
      "801:\tlearn: 5.7836114\ttotal: 1m 38s\tremaining: 24.4s\n",
      "802:\tlearn: 5.7797150\ttotal: 1m 39s\tremaining: 24.3s\n",
      "803:\tlearn: 5.7772246\ttotal: 1m 39s\tremaining: 24.2s\n",
      "804:\tlearn: 5.7747717\ttotal: 1m 39s\tremaining: 24s\n",
      "805:\tlearn: 5.7714776\ttotal: 1m 39s\tremaining: 23.9s\n",
      "806:\tlearn: 5.7688835\ttotal: 1m 39s\tremaining: 23.8s\n",
      "807:\tlearn: 5.7656413\ttotal: 1m 39s\tremaining: 23.7s\n",
      "808:\tlearn: 5.7625692\ttotal: 1m 39s\tremaining: 23.6s\n",
      "809:\tlearn: 5.7607593\ttotal: 1m 39s\tremaining: 23.4s\n",
      "810:\tlearn: 5.7575378\ttotal: 1m 40s\tremaining: 23.3s\n",
      "811:\tlearn: 5.7551370\ttotal: 1m 40s\tremaining: 23.2s\n",
      "812:\tlearn: 5.7547543\ttotal: 1m 40s\tremaining: 23.1s\n",
      "813:\tlearn: 5.7529500\ttotal: 1m 40s\tremaining: 22.9s\n",
      "814:\tlearn: 5.7505321\ttotal: 1m 40s\tremaining: 22.8s\n",
      "815:\tlearn: 5.7478024\ttotal: 1m 40s\tremaining: 22.7s\n",
      "816:\tlearn: 5.7457682\ttotal: 1m 40s\tremaining: 22.6s\n",
      "817:\tlearn: 5.7427500\ttotal: 1m 40s\tremaining: 22.5s\n",
      "818:\tlearn: 5.7403755\ttotal: 1m 41s\tremaining: 22.3s\n",
      "819:\tlearn: 5.7384385\ttotal: 1m 41s\tremaining: 22.2s\n",
      "820:\tlearn: 5.7358266\ttotal: 1m 41s\tremaining: 22.1s\n",
      "821:\tlearn: 5.7327895\ttotal: 1m 41s\tremaining: 22s\n",
      "822:\tlearn: 5.7302738\ttotal: 1m 41s\tremaining: 21.8s\n",
      "823:\tlearn: 5.7266483\ttotal: 1m 41s\tremaining: 21.7s\n",
      "824:\tlearn: 5.7243139\ttotal: 1m 41s\tremaining: 21.6s\n",
      "825:\tlearn: 5.7213726\ttotal: 1m 41s\tremaining: 21.5s\n",
      "826:\tlearn: 5.7181752\ttotal: 1m 42s\tremaining: 21.4s\n",
      "827:\tlearn: 5.7178252\ttotal: 1m 42s\tremaining: 21.2s\n",
      "828:\tlearn: 5.7157551\ttotal: 1m 42s\tremaining: 21.1s\n",
      "829:\tlearn: 5.7128993\ttotal: 1m 42s\tremaining: 21s\n",
      "830:\tlearn: 5.7107805\ttotal: 1m 42s\tremaining: 20.9s\n",
      "831:\tlearn: 5.7087658\ttotal: 1m 42s\tremaining: 20.8s\n",
      "832:\tlearn: 5.7056813\ttotal: 1m 42s\tremaining: 20.6s\n",
      "833:\tlearn: 5.7028106\ttotal: 1m 43s\tremaining: 20.5s\n",
      "834:\tlearn: 5.7002702\ttotal: 1m 43s\tremaining: 20.4s\n",
      "835:\tlearn: 5.6975465\ttotal: 1m 43s\tremaining: 20.3s\n",
      "836:\tlearn: 5.6942782\ttotal: 1m 43s\tremaining: 20.1s\n",
      "837:\tlearn: 5.6919376\ttotal: 1m 43s\tremaining: 20s\n",
      "838:\tlearn: 5.6889970\ttotal: 1m 43s\tremaining: 19.9s\n",
      "839:\tlearn: 5.6871676\ttotal: 1m 43s\tremaining: 19.8s\n",
      "840:\tlearn: 5.6845542\ttotal: 1m 43s\tremaining: 19.6s\n",
      "841:\tlearn: 5.6811464\ttotal: 1m 43s\tremaining: 19.5s\n",
      "842:\tlearn: 5.6782901\ttotal: 1m 44s\tremaining: 19.4s\n",
      "843:\tlearn: 5.6760601\ttotal: 1m 44s\tremaining: 19.3s\n",
      "844:\tlearn: 5.6739519\ttotal: 1m 44s\tremaining: 19.1s\n",
      "845:\tlearn: 5.6718442\ttotal: 1m 44s\tremaining: 19s\n",
      "846:\tlearn: 5.6690886\ttotal: 1m 44s\tremaining: 18.9s\n",
      "847:\tlearn: 5.6670428\ttotal: 1m 44s\tremaining: 18.8s\n",
      "848:\tlearn: 5.6647532\ttotal: 1m 44s\tremaining: 18.6s\n",
      "849:\tlearn: 5.6633426\ttotal: 1m 44s\tremaining: 18.5s\n",
      "850:\tlearn: 5.6605006\ttotal: 1m 45s\tremaining: 18.4s\n",
      "851:\tlearn: 5.6580759\ttotal: 1m 45s\tremaining: 18.3s\n",
      "852:\tlearn: 5.6562137\ttotal: 1m 45s\tremaining: 18.1s\n",
      "853:\tlearn: 5.6538587\ttotal: 1m 45s\tremaining: 18s\n",
      "854:\tlearn: 5.6514488\ttotal: 1m 45s\tremaining: 17.9s\n",
      "855:\tlearn: 5.6513133\ttotal: 1m 45s\tremaining: 17.8s\n",
      "856:\tlearn: 5.6482589\ttotal: 1m 45s\tremaining: 17.6s\n",
      "857:\tlearn: 5.6458644\ttotal: 1m 45s\tremaining: 17.5s\n",
      "858:\tlearn: 5.6457315\ttotal: 1m 46s\tremaining: 17.4s\n",
      "859:\tlearn: 5.6430634\ttotal: 1m 46s\tremaining: 17.3s\n",
      "860:\tlearn: 5.6402567\ttotal: 1m 46s\tremaining: 17.2s\n",
      "861:\tlearn: 5.6379042\ttotal: 1m 46s\tremaining: 17s\n",
      "862:\tlearn: 5.6347755\ttotal: 1m 46s\tremaining: 16.9s\n",
      "863:\tlearn: 5.6318608\ttotal: 1m 46s\tremaining: 16.8s\n",
      "864:\tlearn: 5.6294817\ttotal: 1m 46s\tremaining: 16.7s\n",
      "865:\tlearn: 5.6276806\ttotal: 1m 46s\tremaining: 16.5s\n",
      "866:\tlearn: 5.6258086\ttotal: 1m 47s\tremaining: 16.4s\n",
      "867:\tlearn: 5.6238211\ttotal: 1m 47s\tremaining: 16.3s\n",
      "868:\tlearn: 5.6226864\ttotal: 1m 47s\tremaining: 16.2s\n",
      "869:\tlearn: 5.6208749\ttotal: 1m 47s\tremaining: 16s\n",
      "870:\tlearn: 5.6174780\ttotal: 1m 47s\tremaining: 15.9s\n",
      "871:\tlearn: 5.6138434\ttotal: 1m 47s\tremaining: 15.8s\n",
      "872:\tlearn: 5.6121053\ttotal: 1m 47s\tremaining: 15.7s\n",
      "873:\tlearn: 5.6097432\ttotal: 1m 47s\tremaining: 15.6s\n",
      "874:\tlearn: 5.6072565\ttotal: 1m 48s\tremaining: 15.4s\n",
      "875:\tlearn: 5.6052722\ttotal: 1m 48s\tremaining: 15.3s\n",
      "876:\tlearn: 5.6030214\ttotal: 1m 48s\tremaining: 15.2s\n",
      "877:\tlearn: 5.6007772\ttotal: 1m 48s\tremaining: 15.1s\n",
      "878:\tlearn: 5.5982213\ttotal: 1m 48s\tremaining: 15s\n",
      "879:\tlearn: 5.5954168\ttotal: 1m 48s\tremaining: 14.8s\n",
      "880:\tlearn: 5.5932708\ttotal: 1m 49s\tremaining: 14.7s\n",
      "881:\tlearn: 5.5919781\ttotal: 1m 49s\tremaining: 14.6s\n",
      "882:\tlearn: 5.5897790\ttotal: 1m 49s\tremaining: 14.5s\n",
      "883:\tlearn: 5.5865818\ttotal: 1m 49s\tremaining: 14.4s\n",
      "884:\tlearn: 5.5836627\ttotal: 1m 49s\tremaining: 14.2s\n",
      "885:\tlearn: 5.5816704\ttotal: 1m 49s\tremaining: 14.1s\n",
      "886:\tlearn: 5.5790213\ttotal: 1m 49s\tremaining: 14s\n",
      "887:\tlearn: 5.5772548\ttotal: 1m 49s\tremaining: 13.9s\n",
      "888:\tlearn: 5.5760505\ttotal: 1m 50s\tremaining: 13.7s\n",
      "889:\tlearn: 5.5734640\ttotal: 1m 50s\tremaining: 13.6s\n",
      "890:\tlearn: 5.5703246\ttotal: 1m 50s\tremaining: 13.5s\n",
      "891:\tlearn: 5.5678643\ttotal: 1m 50s\tremaining: 13.4s\n",
      "892:\tlearn: 5.5640188\ttotal: 1m 50s\tremaining: 13.3s\n",
      "893:\tlearn: 5.5612657\ttotal: 1m 50s\tremaining: 13.1s\n",
      "894:\tlearn: 5.5589981\ttotal: 1m 50s\tremaining: 13s\n",
      "895:\tlearn: 5.5571180\ttotal: 1m 51s\tremaining: 12.9s\n",
      "896:\tlearn: 5.5544596\ttotal: 1m 51s\tremaining: 12.8s\n",
      "897:\tlearn: 5.5516484\ttotal: 1m 51s\tremaining: 12.6s\n",
      "898:\tlearn: 5.5487857\ttotal: 1m 51s\tremaining: 12.5s\n",
      "899:\tlearn: 5.5458831\ttotal: 1m 51s\tremaining: 12.4s\n",
      "900:\tlearn: 5.5429230\ttotal: 1m 51s\tremaining: 12.3s\n",
      "901:\tlearn: 5.5405163\ttotal: 1m 51s\tremaining: 12.1s\n",
      "902:\tlearn: 5.5381389\ttotal: 1m 51s\tremaining: 12s\n",
      "903:\tlearn: 5.5358037\ttotal: 1m 52s\tremaining: 11.9s\n",
      "904:\tlearn: 5.5333417\ttotal: 1m 52s\tremaining: 11.8s\n",
      "905:\tlearn: 5.5332168\ttotal: 1m 52s\tremaining: 11.7s\n",
      "906:\tlearn: 5.5296805\ttotal: 1m 52s\tremaining: 11.5s\n",
      "907:\tlearn: 5.5275338\ttotal: 1m 52s\tremaining: 11.4s\n",
      "908:\tlearn: 5.5255583\ttotal: 1m 52s\tremaining: 11.3s\n",
      "909:\tlearn: 5.5240375\ttotal: 1m 52s\tremaining: 11.2s\n",
      "910:\tlearn: 5.5217934\ttotal: 1m 52s\tremaining: 11s\n",
      "911:\tlearn: 5.5179911\ttotal: 1m 53s\tremaining: 10.9s\n",
      "912:\tlearn: 5.5163435\ttotal: 1m 53s\tremaining: 10.8s\n",
      "913:\tlearn: 5.5130571\ttotal: 1m 53s\tremaining: 10.7s\n",
      "914:\tlearn: 5.5117407\ttotal: 1m 53s\tremaining: 10.5s\n",
      "915:\tlearn: 5.5091010\ttotal: 1m 53s\tremaining: 10.4s\n",
      "916:\tlearn: 5.5075449\ttotal: 1m 53s\tremaining: 10.3s\n",
      "917:\tlearn: 5.5045319\ttotal: 1m 53s\tremaining: 10.2s\n",
      "918:\tlearn: 5.5019111\ttotal: 1m 53s\tremaining: 10s\n",
      "919:\tlearn: 5.4984689\ttotal: 1m 54s\tremaining: 9.92s\n",
      "920:\tlearn: 5.4963993\ttotal: 1m 54s\tremaining: 9.79s\n",
      "921:\tlearn: 5.4937486\ttotal: 1m 54s\tremaining: 9.67s\n",
      "922:\tlearn: 5.4920349\ttotal: 1m 54s\tremaining: 9.54s\n",
      "923:\tlearn: 5.4897789\ttotal: 1m 54s\tremaining: 9.42s\n",
      "924:\tlearn: 5.4870271\ttotal: 1m 54s\tremaining: 9.29s\n",
      "925:\tlearn: 5.4843166\ttotal: 1m 54s\tremaining: 9.17s\n",
      "926:\tlearn: 5.4807386\ttotal: 1m 54s\tremaining: 9.05s\n",
      "927:\tlearn: 5.4785242\ttotal: 1m 54s\tremaining: 8.92s\n",
      "928:\tlearn: 5.4765304\ttotal: 1m 55s\tremaining: 8.8s\n",
      "929:\tlearn: 5.4739619\ttotal: 1m 55s\tremaining: 8.67s\n",
      "930:\tlearn: 5.4716709\ttotal: 1m 55s\tremaining: 8.55s\n",
      "931:\tlearn: 5.4689331\ttotal: 1m 55s\tremaining: 8.43s\n",
      "932:\tlearn: 5.4670932\ttotal: 1m 55s\tremaining: 8.3s\n",
      "933:\tlearn: 5.4645057\ttotal: 1m 55s\tremaining: 8.18s\n",
      "934:\tlearn: 5.4623383\ttotal: 1m 55s\tremaining: 8.05s\n",
      "935:\tlearn: 5.4601191\ttotal: 1m 55s\tremaining: 7.93s\n",
      "936:\tlearn: 5.4575779\ttotal: 1m 56s\tremaining: 7.8s\n",
      "937:\tlearn: 5.4557444\ttotal: 1m 56s\tremaining: 7.68s\n",
      "938:\tlearn: 5.4532891\ttotal: 1m 56s\tremaining: 7.56s\n",
      "939:\tlearn: 5.4507342\ttotal: 1m 56s\tremaining: 7.43s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "940:\tlearn: 5.4479920\ttotal: 1m 56s\tremaining: 7.31s\n",
      "941:\tlearn: 5.4454896\ttotal: 1m 56s\tremaining: 7.19s\n",
      "942:\tlearn: 5.4434202\ttotal: 1m 56s\tremaining: 7.07s\n",
      "943:\tlearn: 5.4408817\ttotal: 1m 57s\tremaining: 6.94s\n",
      "944:\tlearn: 5.4388237\ttotal: 1m 57s\tremaining: 6.82s\n",
      "945:\tlearn: 5.4368575\ttotal: 1m 57s\tremaining: 6.69s\n",
      "946:\tlearn: 5.4339289\ttotal: 1m 57s\tremaining: 6.57s\n",
      "947:\tlearn: 5.4319771\ttotal: 1m 57s\tremaining: 6.44s\n",
      "948:\tlearn: 5.4296886\ttotal: 1m 57s\tremaining: 6.32s\n",
      "949:\tlearn: 5.4278116\ttotal: 1m 57s\tremaining: 6.2s\n",
      "950:\tlearn: 5.4248842\ttotal: 1m 57s\tremaining: 6.07s\n",
      "951:\tlearn: 5.4214456\ttotal: 1m 58s\tremaining: 5.95s\n",
      "952:\tlearn: 5.4189505\ttotal: 1m 58s\tremaining: 5.83s\n",
      "953:\tlearn: 5.4166579\ttotal: 1m 58s\tremaining: 5.7s\n",
      "954:\tlearn: 5.4141506\ttotal: 1m 58s\tremaining: 5.58s\n",
      "955:\tlearn: 5.4122774\ttotal: 1m 58s\tremaining: 5.45s\n",
      "956:\tlearn: 5.4100281\ttotal: 1m 58s\tremaining: 5.33s\n",
      "957:\tlearn: 5.4075416\ttotal: 1m 58s\tremaining: 5.21s\n",
      "958:\tlearn: 5.4048706\ttotal: 1m 58s\tremaining: 5.08s\n",
      "959:\tlearn: 5.4026613\ttotal: 1m 59s\tremaining: 4.96s\n",
      "960:\tlearn: 5.4012721\ttotal: 1m 59s\tremaining: 4.84s\n",
      "961:\tlearn: 5.3978305\ttotal: 1m 59s\tremaining: 4.71s\n",
      "962:\tlearn: 5.3961816\ttotal: 1m 59s\tremaining: 4.59s\n",
      "963:\tlearn: 5.3949089\ttotal: 1m 59s\tremaining: 4.47s\n",
      "964:\tlearn: 5.3914780\ttotal: 1m 59s\tremaining: 4.34s\n",
      "965:\tlearn: 5.3893546\ttotal: 1m 59s\tremaining: 4.22s\n",
      "966:\tlearn: 5.3864684\ttotal: 1m 59s\tremaining: 4.09s\n",
      "967:\tlearn: 5.3842135\ttotal: 2m\tremaining: 3.97s\n",
      "968:\tlearn: 5.3823171\ttotal: 2m\tremaining: 3.84s\n",
      "969:\tlearn: 5.3796786\ttotal: 2m\tremaining: 3.72s\n",
      "970:\tlearn: 5.3782947\ttotal: 2m\tremaining: 3.6s\n",
      "971:\tlearn: 5.3761812\ttotal: 2m\tremaining: 3.47s\n",
      "972:\tlearn: 5.3744619\ttotal: 2m\tremaining: 3.35s\n",
      "973:\tlearn: 5.3715364\ttotal: 2m\tremaining: 3.22s\n",
      "974:\tlearn: 5.3682639\ttotal: 2m\tremaining: 3.1s\n",
      "975:\tlearn: 5.3659656\ttotal: 2m\tremaining: 2.97s\n",
      "976:\tlearn: 5.3630113\ttotal: 2m 1s\tremaining: 2.85s\n",
      "977:\tlearn: 5.3604358\ttotal: 2m 1s\tremaining: 2.73s\n",
      "978:\tlearn: 5.3574248\ttotal: 2m 1s\tremaining: 2.6s\n",
      "979:\tlearn: 5.3552174\ttotal: 2m 1s\tremaining: 2.48s\n",
      "980:\tlearn: 5.3526968\ttotal: 2m 1s\tremaining: 2.35s\n",
      "981:\tlearn: 5.3501241\ttotal: 2m 1s\tremaining: 2.23s\n",
      "982:\tlearn: 5.3490532\ttotal: 2m 1s\tremaining: 2.1s\n",
      "983:\tlearn: 5.3463351\ttotal: 2m 1s\tremaining: 1.98s\n",
      "984:\tlearn: 5.3438574\ttotal: 2m 1s\tremaining: 1.86s\n",
      "985:\tlearn: 5.3412219\ttotal: 2m 2s\tremaining: 1.73s\n",
      "986:\tlearn: 5.3382087\ttotal: 2m 2s\tremaining: 1.61s\n",
      "987:\tlearn: 5.3359606\ttotal: 2m 2s\tremaining: 1.49s\n",
      "988:\tlearn: 5.3333432\ttotal: 2m 2s\tremaining: 1.36s\n",
      "989:\tlearn: 5.3298354\ttotal: 2m 2s\tremaining: 1.24s\n",
      "990:\tlearn: 5.3279218\ttotal: 2m 2s\tremaining: 1.11s\n",
      "991:\tlearn: 5.3253223\ttotal: 2m 2s\tremaining: 990ms\n",
      "992:\tlearn: 5.3226351\ttotal: 2m 2s\tremaining: 867ms\n",
      "993:\tlearn: 5.3210653\ttotal: 2m 3s\tremaining: 743ms\n",
      "994:\tlearn: 5.3191779\ttotal: 2m 3s\tremaining: 619ms\n",
      "995:\tlearn: 5.3167033\ttotal: 2m 3s\tremaining: 495ms\n",
      "996:\tlearn: 5.3146512\ttotal: 2m 3s\tremaining: 371ms\n",
      "997:\tlearn: 5.3120599\ttotal: 2m 3s\tremaining: 248ms\n",
      "998:\tlearn: 5.3098607\ttotal: 2m 3s\tremaining: 124ms\n",
      "999:\tlearn: 5.3073338\ttotal: 2m 3s\tremaining: 0us\n",
      "LGBMRegressor 로그 변환된 RMSE: 8.3\n",
      "CatBoostRegressor 로그 변환된 RMSE: 8.213\n",
      "XGBRegressor 로그 변환된 RMSE: 8.711\n",
      "Ridge 로그 변환된 RMSE: 8.399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.300104403661345, 8.213310543868028, 8.711355359905992, 8.399157864033706]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rmse(model):\n",
    "    pred = model.predict(valid_x)\n",
    "    mse = mean_squared_error(valid_y , pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print('{0} 로그 변환된 RMSE: {1}'.format(model.__class__.__name__,np.round(rmse, 3)))\n",
    "    return rmse\n",
    "\n",
    "def get_rmses(models):\n",
    "    rmses = [ ]\n",
    "    for model in models:\n",
    "        rmse = get_rmse(model)\n",
    "        rmses.append(rmse)\n",
    "    return rmses\n",
    "\n",
    "\n",
    "\n",
    "lgbm = LGBMRegressor(random_state = 1000  )\n",
    "lgbm = lgbm.fit(train_x , train_y)\n",
    "\n",
    "cat = CatBoostRegressor(random_state=1000 )\n",
    "cat = cat.fit(train_x , train_y)\n",
    "\n",
    "xgb = XGBRegressor(random_state = 1000 )\n",
    "xgb.fit(train_x , train_y )\n",
    "\n",
    "reg_ridge = Ridge(random_state = 1000)\n",
    "reg_ridge.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "models = [lgbm, cat, xgb, reg_ridge ]\n",
    "get_rmses(models)\n",
    "\n",
    "\n",
    "# LGBMRegressor 로그 변환된 RMSE: 8.3\n",
    "# CatBoostRegressor 로그 변환된 RMSE: 8.213\n",
    "# XGBRegressor 로그 변환된 RMSE: 8.711\n",
    "# Ridge 로그 변환된 RMSE: 8.399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_params = {\n",
    "    'max_depth':(8, 16),\n",
    "    'num_leaves':(24, 64),\n",
    "    'min_child_samples':(10, 200),\n",
    "    'min_child_weight':(1, 50),\n",
    "    'subsample':(0.5, 1),\n",
    "    'colsample_bytree':(0.5, 1),\n",
    "    'max_bin':(10, 500),\n",
    "    'reg_lambda':(0.001, 10),\n",
    "    'reg_alpha':(0.01, 50)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_rmse_eval(max_depth, num_leaves, min_child_samples, min_child_weight, subsample, \n",
    "                colsample_bytree, max_bin, reg_lambda, reg_alpha):\n",
    "    \n",
    "    params = {\n",
    "        \"n_estimators\":2000, \n",
    "        \"learning_rate\":0.02,\n",
    "        'max_depth':int(round(max_depth)),\n",
    "        'num_leaves':int(round(num_leaves)),\n",
    "        'min_child_samples': int(round(min_child_samples)),\n",
    "        'min_child_weight': int(round(min_child_weight)),\n",
    "        'subsample':max(min(subsample, 1), 0),\n",
    "        'colsample_bytree':max(min(colsample_bytree, 1), 0),\n",
    "        'reg_lambda': max(reg_lambda,0),\n",
    "        'reg_alpha': max(reg_alpha, 0)\n",
    "    }\n",
    "    \n",
    "    lgb_model = LGBMRegressor(**params)\n",
    "    lgb_model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'RMSE', verbose= 100, \n",
    "                early_stopping_rounds= 100)\n",
    "    valid_pred = lgb_model.predict(valid_x)\n",
    "    RMSE = np.sqrt(mean_squared_error(valid_y, valid_pred))\n",
    "    \n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |  max_bin  | max_depth | min_ch... | min_ch... | num_le... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.99494\ttraining's l2: 63.919\tvalid_1's rmse: 8.49855\tvalid_1's l2: 72.2253\n",
      "[200]\ttraining's rmse: 7.27637\ttraining's l2: 52.9456\tvalid_1's rmse: 8.25376\tvalid_1's l2: 68.1246\n",
      "[300]\ttraining's rmse: 6.80184\ttraining's l2: 46.2651\tvalid_1's rmse: 8.19875\tvalid_1's l2: 67.2196\n",
      "[400]\ttraining's rmse: 6.42346\ttraining's l2: 41.2608\tvalid_1's rmse: 8.18056\tvalid_1's l2: 66.9216\n",
      "[500]\ttraining's rmse: 6.08938\ttraining's l2: 37.0805\tvalid_1's rmse: 8.16847\tvalid_1's l2: 66.7239\n",
      "[600]\ttraining's rmse: 5.78737\ttraining's l2: 33.4937\tvalid_1's rmse: 8.16555\tvalid_1's l2: 66.6763\n",
      "[700]\ttraining's rmse: 5.51054\ttraining's l2: 30.3661\tvalid_1's rmse: 8.16032\tvalid_1's l2: 66.5908\n",
      "[800]\ttraining's rmse: 5.25738\ttraining's l2: 27.6401\tvalid_1's rmse: 8.16252\tvalid_1's l2: 66.6267\n",
      "Early stopping, best iteration is:\n",
      "[742]\ttraining's rmse: 5.40242\ttraining's l2: 29.1861\tvalid_1's rmse: 8.15958\tvalid_1's l2: 66.5788\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 8.16    \u001b[0m | \u001b[0m 0.8268  \u001b[0m | \u001b[0m 66.35   \u001b[0m | \u001b[0m 15.6    \u001b[0m | \u001b[0m 101.6   \u001b[0m | \u001b[0m 43.75   \u001b[0m | \u001b[0m 32.49   \u001b[0m | \u001b[0m 2.045   \u001b[0m | \u001b[0m 3.973   \u001b[0m | \u001b[0m 0.6166  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.64745\ttraining's l2: 58.4835\tvalid_1's rmse: 8.46329\tvalid_1's l2: 71.6274\n",
      "[200]\ttraining's rmse: 6.70804\ttraining's l2: 44.9978\tvalid_1's rmse: 8.24638\tvalid_1's l2: 68.0028\n",
      "[300]\ttraining's rmse: 6.0701\ttraining's l2: 36.8461\tvalid_1's rmse: 8.19326\tvalid_1's l2: 67.1295\n",
      "[400]\ttraining's rmse: 5.55613\ttraining's l2: 30.8706\tvalid_1's rmse: 8.17828\tvalid_1's l2: 66.8842\n",
      "[500]\ttraining's rmse: 5.10813\ttraining's l2: 26.093\tvalid_1's rmse: 8.17416\tvalid_1's l2: 66.8169\n",
      "[600]\ttraining's rmse: 4.70969\ttraining's l2: 22.1811\tvalid_1's rmse: 8.17086\tvalid_1's l2: 66.7629\n",
      "[700]\ttraining's rmse: 4.35489\ttraining's l2: 18.965\tvalid_1's rmse: 8.17251\tvalid_1's l2: 66.7898\n",
      "Early stopping, best iteration is:\n",
      "[616]\ttraining's rmse: 4.65101\ttraining's l2: 21.6319\tvalid_1's rmse: 8.17005\tvalid_1's l2: 66.7496\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 8.17    \u001b[0m | \u001b[95m 0.9209  \u001b[0m | \u001b[95m 111.5   \u001b[0m | \u001b[95m 13.94   \u001b[0m | \u001b[95m 84.51   \u001b[0m | \u001b[95m 9.931   \u001b[0m | \u001b[95m 53.74   \u001b[0m | \u001b[95m 3.488   \u001b[0m | \u001b[95m 8.853   \u001b[0m | \u001b[95m 0.9763  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.84473\ttraining's l2: 61.5398\tvalid_1's rmse: 8.47171\tvalid_1's l2: 71.7699\n",
      "[200]\ttraining's rmse: 7.11024\ttraining's l2: 50.5555\tvalid_1's rmse: 8.25608\tvalid_1's l2: 68.1629\n",
      "[300]\ttraining's rmse: 6.71787\ttraining's l2: 45.1298\tvalid_1's rmse: 8.20513\tvalid_1's l2: 67.3242\n",
      "[400]\ttraining's rmse: 6.43553\ttraining's l2: 41.416\tvalid_1's rmse: 8.19074\tvalid_1's l2: 67.0882\n",
      "[500]\ttraining's rmse: 6.19178\ttraining's l2: 38.3382\tvalid_1's rmse: 8.17941\tvalid_1's l2: 66.9028\n",
      "[600]\ttraining's rmse: 5.95081\ttraining's l2: 35.4121\tvalid_1's rmse: 8.17211\tvalid_1's l2: 66.7834\n",
      "Early stopping, best iteration is:\n",
      "[593]\ttraining's rmse: 5.96954\ttraining's l2: 35.6354\tvalid_1's rmse: 8.17156\tvalid_1's l2: 66.7744\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 8.172   \u001b[0m | \u001b[95m 0.9656  \u001b[0m | \u001b[95m 213.6   \u001b[0m | \u001b[95m 8.232   \u001b[0m | \u001b[95m 196.6   \u001b[0m | \u001b[95m 17.64   \u001b[0m | \u001b[95m 52.27   \u001b[0m | \u001b[95m 18.1    \u001b[0m | \u001b[95m 0.352   \u001b[0m | \u001b[95m 0.9275  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 8.17197\ttraining's l2: 66.781\tvalid_1's rmse: 8.54598\tvalid_1's l2: 73.0337\n",
      "[200]\ttraining's rmse: 7.55809\ttraining's l2: 57.1247\tvalid_1's rmse: 8.28237\tvalid_1's l2: 68.5976\n",
      "[300]\ttraining's rmse: 7.18009\ttraining's l2: 51.5537\tvalid_1's rmse: 8.2203\tvalid_1's l2: 67.5734\n",
      "[400]\ttraining's rmse: 6.87793\ttraining's l2: 47.3059\tvalid_1's rmse: 8.19459\tvalid_1's l2: 67.1513\n",
      "[500]\ttraining's rmse: 6.61187\ttraining's l2: 43.7169\tvalid_1's rmse: 8.17783\tvalid_1's l2: 66.8769\n",
      "[600]\ttraining's rmse: 6.37102\ttraining's l2: 40.5899\tvalid_1's rmse: 8.17124\tvalid_1's l2: 66.7692\n",
      "[700]\ttraining's rmse: 6.14624\ttraining's l2: 37.7762\tvalid_1's rmse: 8.1722\tvalid_1's l2: 66.7849\n",
      "Early stopping, best iteration is:\n",
      "[643]\ttraining's rmse: 6.27316\ttraining's l2: 39.3525\tvalid_1's rmse: 8.16914\tvalid_1's l2: 66.7349\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 8.169   \u001b[0m | \u001b[0m 0.8286  \u001b[0m | \u001b[0m 385.2   \u001b[0m | \u001b[0m 12.43   \u001b[0m | \u001b[0m 178.2   \u001b[0m | \u001b[0m 45.31   \u001b[0m | \u001b[0m 24.42   \u001b[0m | \u001b[0m 3.737   \u001b[0m | \u001b[0m 2.447   \u001b[0m | \u001b[0m 0.5667  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 8.15475\ttraining's l2: 66.4999\tvalid_1's rmse: 8.54441\tvalid_1's l2: 73.0069\n",
      "[200]\ttraining's rmse: 7.51661\ttraining's l2: 56.4994\tvalid_1's rmse: 8.28671\tvalid_1's l2: 68.6696\n",
      "[300]\ttraining's rmse: 7.11056\ttraining's l2: 50.56\tvalid_1's rmse: 8.21939\tvalid_1's l2: 67.5584\n",
      "[400]\ttraining's rmse: 6.78394\ttraining's l2: 46.0218\tvalid_1's rmse: 8.19223\tvalid_1's l2: 67.1126\n",
      "[500]\ttraining's rmse: 6.50044\ttraining's l2: 42.2557\tvalid_1's rmse: 8.18178\tvalid_1's l2: 66.9416\n",
      "[600]\ttraining's rmse: 6.24562\ttraining's l2: 39.0078\tvalid_1's rmse: 8.18044\tvalid_1's l2: 66.9196\n",
      "Early stopping, best iteration is:\n",
      "[551]\ttraining's rmse: 6.3691\ttraining's l2: 40.5654\tvalid_1's rmse: 8.17904\tvalid_1's l2: 66.8967\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 8.179   \u001b[0m | \u001b[95m 0.849   \u001b[0m | \u001b[95m 205.1   \u001b[0m | \u001b[95m 15.06   \u001b[0m | \u001b[95m 44.39   \u001b[0m | \u001b[95m 22.19   \u001b[0m | \u001b[95m 24.73   \u001b[0m | \u001b[95m 34.57   \u001b[0m | \u001b[95m 4.697   \u001b[0m | \u001b[95m 0.5641  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.81707\ttraining's l2: 61.1066\tvalid_1's rmse: 8.47751\tvalid_1's l2: 71.8681\n",
      "[200]\ttraining's rmse: 7.02954\ttraining's l2: 49.4144\tvalid_1's rmse: 8.24181\tvalid_1's l2: 67.9274\n",
      "[300]\ttraining's rmse: 6.55403\ttraining's l2: 42.9553\tvalid_1's rmse: 8.18753\tvalid_1's l2: 67.0356\n",
      "[400]\ttraining's rmse: 6.21737\ttraining's l2: 38.6557\tvalid_1's rmse: 8.16957\tvalid_1's l2: 66.7418\n",
      "[500]\ttraining's rmse: 5.91361\ttraining's l2: 34.9708\tvalid_1's rmse: 8.16092\tvalid_1's l2: 66.6005\n",
      "[600]\ttraining's rmse: 5.64782\ttraining's l2: 31.8979\tvalid_1's rmse: 8.15745\tvalid_1's l2: 66.5439\n",
      "[700]\ttraining's rmse: 5.39698\ttraining's l2: 29.1274\tvalid_1's rmse: 8.15726\tvalid_1's l2: 66.5409\n",
      "Early stopping, best iteration is:\n",
      "[670]\ttraining's rmse: 5.46913\ttraining's l2: 29.9114\tvalid_1's rmse: 8.15627\tvalid_1's l2: 66.5248\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 8.156   \u001b[0m | \u001b[0m 0.5042  \u001b[0m | \u001b[0m 210.6   \u001b[0m | \u001b[0m 10.12   \u001b[0m | \u001b[0m 197.0   \u001b[0m | \u001b[0m 22.58   \u001b[0m | \u001b[0m 56.8    \u001b[0m | \u001b[0m 14.7    \u001b[0m | \u001b[0m 1.03    \u001b[0m | \u001b[0m 0.6526  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.84639\ttraining's l2: 61.5658\tvalid_1's rmse: 8.46274\tvalid_1's l2: 71.618\n",
      "[200]\ttraining's rmse: 7.05344\ttraining's l2: 49.751\tvalid_1's rmse: 8.23194\tvalid_1's l2: 67.7648\n",
      "[300]\ttraining's rmse: 6.54991\ttraining's l2: 42.9014\tvalid_1's rmse: 8.17967\tvalid_1's l2: 66.907\n",
      "[400]\ttraining's rmse: 6.16545\ttraining's l2: 38.0128\tvalid_1's rmse: 8.16144\tvalid_1's l2: 66.6091\n",
      "[500]\ttraining's rmse: 5.81546\ttraining's l2: 33.8195\tvalid_1's rmse: 8.15527\tvalid_1's l2: 66.5085\n",
      "[600]\ttraining's rmse: 5.5054\ttraining's l2: 30.3094\tvalid_1's rmse: 8.15199\tvalid_1's l2: 66.4549\n",
      "Early stopping, best iteration is:\n",
      "[576]\ttraining's rmse: 5.57901\ttraining's l2: 31.1253\tvalid_1's rmse: 8.15021\tvalid_1's l2: 66.426\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 8.15    \u001b[0m | \u001b[0m 0.6842  \u001b[0m | \u001b[0m 25.21   \u001b[0m | \u001b[0m 10.86   \u001b[0m | \u001b[0m 168.4   \u001b[0m | \u001b[0m 24.29   \u001b[0m | \u001b[0m 44.84   \u001b[0m | \u001b[0m 7.394   \u001b[0m | \u001b[0m 2.342   \u001b[0m | \u001b[0m 0.736   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.93267\ttraining's l2: 62.9272\tvalid_1's rmse: 8.49974\tvalid_1's l2: 72.2456\n",
      "[200]\ttraining's rmse: 7.17145\ttraining's l2: 51.4297\tvalid_1's rmse: 8.25409\tvalid_1's l2: 68.1301\n",
      "[300]\ttraining's rmse: 6.65646\ttraining's l2: 44.3085\tvalid_1's rmse: 8.18901\tvalid_1's l2: 67.0598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttraining's rmse: 6.24164\ttraining's l2: 38.9581\tvalid_1's rmse: 8.16844\tvalid_1's l2: 66.7235\n",
      "[500]\ttraining's rmse: 5.88321\ttraining's l2: 34.6122\tvalid_1's rmse: 8.15995\tvalid_1's l2: 66.5847\n",
      "[600]\ttraining's rmse: 5.55839\ttraining's l2: 30.8957\tvalid_1's rmse: 8.15266\tvalid_1's l2: 66.4659\n",
      "[700]\ttraining's rmse: 5.26456\ttraining's l2: 27.7156\tvalid_1's rmse: 8.14984\tvalid_1's l2: 66.4199\n",
      "Early stopping, best iteration is:\n",
      "[651]\ttraining's rmse: 5.40497\ttraining's l2: 29.2137\tvalid_1's rmse: 8.14907\tvalid_1's l2: 66.4073\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 8.149   \u001b[0m | \u001b[0m 0.7675  \u001b[0m | \u001b[0m 132.0   \u001b[0m | \u001b[0m 14.3    \u001b[0m | \u001b[0m 40.39   \u001b[0m | \u001b[0m 44.76   \u001b[0m | \u001b[0m 35.85   \u001b[0m | \u001b[0m 33.16   \u001b[0m | \u001b[0m 4.179   \u001b[0m | \u001b[0m 0.9905  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.58396\ttraining's l2: 57.5165\tvalid_1's rmse: 8.45274\tvalid_1's l2: 71.4488\n",
      "[200]\ttraining's rmse: 6.65393\ttraining's l2: 44.2748\tvalid_1's rmse: 8.23469\tvalid_1's l2: 67.8102\n",
      "[300]\ttraining's rmse: 6.17768\ttraining's l2: 38.1637\tvalid_1's rmse: 8.19101\tvalid_1's l2: 67.0926\n",
      "[400]\ttraining's rmse: 5.84126\ttraining's l2: 34.1203\tvalid_1's rmse: 8.17427\tvalid_1's l2: 66.8187\n",
      "[500]\ttraining's rmse: 5.53158\ttraining's l2: 30.5984\tvalid_1's rmse: 8.1619\tvalid_1's l2: 66.6166\n",
      "[600]\ttraining's rmse: 5.20339\ttraining's l2: 27.0753\tvalid_1's rmse: 8.15849\tvalid_1's l2: 66.5609\n",
      "[700]\ttraining's rmse: 4.89526\ttraining's l2: 23.9635\tvalid_1's rmse: 8.15621\tvalid_1's l2: 66.5238\n",
      "[800]\ttraining's rmse: 4.60507\ttraining's l2: 21.2067\tvalid_1's rmse: 8.1583\tvalid_1's l2: 66.5579\n",
      "Early stopping, best iteration is:\n",
      "[753]\ttraining's rmse: 4.73547\ttraining's l2: 22.4247\tvalid_1's rmse: 8.15572\tvalid_1's l2: 66.5157\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 8.156   \u001b[0m | \u001b[0m 0.5909  \u001b[0m | \u001b[0m 151.4   \u001b[0m | \u001b[0m 8.126   \u001b[0m | \u001b[0m 67.71   \u001b[0m | \u001b[0m 12.18   \u001b[0m | \u001b[0m 57.97   \u001b[0m | \u001b[0m 5.113   \u001b[0m | \u001b[0m 2.963   \u001b[0m | \u001b[0m 0.5074  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 8.07866\ttraining's l2: 65.2647\tvalid_1's rmse: 8.5358\tvalid_1's l2: 72.8599\n",
      "[200]\ttraining's rmse: 7.40839\ttraining's l2: 54.8842\tvalid_1's rmse: 8.27666\tvalid_1's l2: 68.503\n",
      "[300]\ttraining's rmse: 6.98044\ttraining's l2: 48.7265\tvalid_1's rmse: 8.21327\tvalid_1's l2: 67.4578\n",
      "[400]\ttraining's rmse: 6.63421\ttraining's l2: 44.0127\tvalid_1's rmse: 8.18506\tvalid_1's l2: 66.9951\n",
      "[500]\ttraining's rmse: 6.32972\ttraining's l2: 40.0654\tvalid_1's rmse: 8.17086\tvalid_1's l2: 66.763\n",
      "[600]\ttraining's rmse: 6.05042\ttraining's l2: 36.6076\tvalid_1's rmse: 8.16467\tvalid_1's l2: 66.6618\n",
      "[700]\ttraining's rmse: 5.79523\ttraining's l2: 33.5847\tvalid_1's rmse: 8.16578\tvalid_1's l2: 66.68\n",
      "[800]\ttraining's rmse: 5.55694\ttraining's l2: 30.8796\tvalid_1's rmse: 8.16146\tvalid_1's l2: 66.6094\n",
      "[900]\ttraining's rmse: 5.33579\ttraining's l2: 28.4706\tvalid_1's rmse: 8.16354\tvalid_1's l2: 66.6434\n",
      "Early stopping, best iteration is:\n",
      "[813]\ttraining's rmse: 5.52685\ttraining's l2: 30.5461\tvalid_1's rmse: 8.16037\tvalid_1's l2: 66.5917\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 8.16    \u001b[0m | \u001b[0m 0.9254  \u001b[0m | \u001b[0m 416.7   \u001b[0m | \u001b[0m 10.77   \u001b[0m | \u001b[0m 126.9   \u001b[0m | \u001b[0m 26.11   \u001b[0m | \u001b[0m 29.61   \u001b[0m | \u001b[0m 48.7    \u001b[0m | \u001b[0m 7.307   \u001b[0m | \u001b[0m 0.5611  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 8.04916\ttraining's l2: 64.789\tvalid_1's rmse: 8.51845\tvalid_1's l2: 72.564\n",
      "[200]\ttraining's rmse: 7.35898\ttraining's l2: 54.1546\tvalid_1's rmse: 8.26668\tvalid_1's l2: 68.3379\n",
      "[300]\ttraining's rmse: 6.90546\ttraining's l2: 47.6854\tvalid_1's rmse: 8.19824\tvalid_1's l2: 67.2112\n",
      "[400]\ttraining's rmse: 6.53872\ttraining's l2: 42.7549\tvalid_1's rmse: 8.17038\tvalid_1's l2: 66.755\n",
      "[500]\ttraining's rmse: 6.21678\ttraining's l2: 38.6483\tvalid_1's rmse: 8.15585\tvalid_1's l2: 66.5179\n",
      "[600]\ttraining's rmse: 5.92575\ttraining's l2: 35.1145\tvalid_1's rmse: 8.14762\tvalid_1's l2: 66.3837\n",
      "[700]\ttraining's rmse: 5.65452\ttraining's l2: 31.9736\tvalid_1's rmse: 8.14587\tvalid_1's l2: 66.3552\n",
      "[800]\ttraining's rmse: 5.40976\ttraining's l2: 29.2655\tvalid_1's rmse: 8.14569\tvalid_1's l2: 66.3522\n",
      "Early stopping, best iteration is:\n",
      "[799]\ttraining's rmse: 5.41226\ttraining's l2: 29.2925\tvalid_1's rmse: 8.14563\tvalid_1's l2: 66.3513\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 8.146   \u001b[0m | \u001b[0m 0.5141  \u001b[0m | \u001b[0m 254.3   \u001b[0m | \u001b[0m 15.33   \u001b[0m | \u001b[0m 103.2   \u001b[0m | \u001b[0m 46.9    \u001b[0m | \u001b[0m 32.34   \u001b[0m | \u001b[0m 12.22   \u001b[0m | \u001b[0m 9.777   \u001b[0m | \u001b[0m 0.6989  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.99043\ttraining's l2: 63.847\tvalid_1's rmse: 8.51644\tvalid_1's l2: 72.5297\n",
      "[200]\ttraining's rmse: 7.27145\ttraining's l2: 52.8741\tvalid_1's rmse: 8.26523\tvalid_1's l2: 68.3141\n",
      "[300]\ttraining's rmse: 6.79364\ttraining's l2: 46.1535\tvalid_1's rmse: 8.20246\tvalid_1's l2: 67.2804\n",
      "[400]\ttraining's rmse: 6.40162\ttraining's l2: 40.9808\tvalid_1's rmse: 8.17729\tvalid_1's l2: 66.868\n",
      "[500]\ttraining's rmse: 6.0597\ttraining's l2: 36.7199\tvalid_1's rmse: 8.16979\tvalid_1's l2: 66.7456\n",
      "[600]\ttraining's rmse: 5.7514\ttraining's l2: 33.0786\tvalid_1's rmse: 8.1629\tvalid_1's l2: 66.6329\n",
      "[700]\ttraining's rmse: 5.46833\ttraining's l2: 29.9027\tvalid_1's rmse: 8.15768\tvalid_1's l2: 66.5478\n",
      "[800]\ttraining's rmse: 5.20792\ttraining's l2: 27.1224\tvalid_1's rmse: 8.15544\tvalid_1's l2: 66.5112\n",
      "Early stopping, best iteration is:\n",
      "[759]\ttraining's rmse: 5.31259\ttraining's l2: 28.2236\tvalid_1's rmse: 8.15355\tvalid_1's l2: 66.4803\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 8.154   \u001b[0m | \u001b[0m 0.5715  \u001b[0m | \u001b[0m 420.3   \u001b[0m | \u001b[0m 15.04   \u001b[0m | \u001b[0m 125.0   \u001b[0m | \u001b[0m 3.524   \u001b[0m | \u001b[0m 35.44   \u001b[0m | \u001b[0m 22.55   \u001b[0m | \u001b[0m 3.946   \u001b[0m | \u001b[0m 0.9637  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.88849\ttraining's l2: 62.2283\tvalid_1's rmse: 8.4988\tvalid_1's l2: 72.2296\n",
      "[200]\ttraining's rmse: 7.09626\ttraining's l2: 50.3569\tvalid_1's rmse: 8.26038\tvalid_1's l2: 68.2339\n",
      "[300]\ttraining's rmse: 6.55493\ttraining's l2: 42.9671\tvalid_1's rmse: 8.20256\tvalid_1's l2: 67.282\n",
      "[400]\ttraining's rmse: 6.11037\ttraining's l2: 37.3367\tvalid_1's rmse: 8.17738\tvalid_1's l2: 66.8696\n",
      "[500]\ttraining's rmse: 5.73143\ttraining's l2: 32.8493\tvalid_1's rmse: 8.17012\tvalid_1's l2: 66.7509\n",
      "[600]\ttraining's rmse: 5.39626\ttraining's l2: 29.1196\tvalid_1's rmse: 8.1635\tvalid_1's l2: 66.6427\n",
      "[700]\ttraining's rmse: 5.09392\ttraining's l2: 25.9481\tvalid_1's rmse: 8.1624\tvalid_1's l2: 66.6248\n",
      "Early stopping, best iteration is:\n",
      "[668]\ttraining's rmse: 5.18524\ttraining's l2: 26.8867\tvalid_1's rmse: 8.16175\tvalid_1's l2: 66.6141\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 8.162   \u001b[0m | \u001b[0m 0.5793  \u001b[0m | \u001b[0m 255.2   \u001b[0m | \u001b[0m 13.55   \u001b[0m | \u001b[0m 12.51   \u001b[0m | \u001b[0m 29.43   \u001b[0m | \u001b[0m 39.82   \u001b[0m | \u001b[0m 33.96   \u001b[0m | \u001b[0m 5.653   \u001b[0m | \u001b[0m 0.8581  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.77753\ttraining's l2: 60.4899\tvalid_1's rmse: 8.47609\tvalid_1's l2: 71.8442\n",
      "[200]\ttraining's rmse: 6.93129\ttraining's l2: 48.0428\tvalid_1's rmse: 8.23908\tvalid_1's l2: 67.8824\n",
      "[300]\ttraining's rmse: 6.35848\ttraining's l2: 40.4302\tvalid_1's rmse: 8.18213\tvalid_1's l2: 66.9472\n",
      "[400]\ttraining's rmse: 5.909\ttraining's l2: 34.9163\tvalid_1's rmse: 8.16737\tvalid_1's l2: 66.7059\n",
      "[500]\ttraining's rmse: 5.51279\ttraining's l2: 30.3909\tvalid_1's rmse: 8.15832\tvalid_1's l2: 66.5582\n",
      "[600]\ttraining's rmse: 5.15175\ttraining's l2: 26.5405\tvalid_1's rmse: 8.15669\tvalid_1's l2: 66.5316\n",
      "[700]\ttraining's rmse: 4.83104\ttraining's l2: 23.3389\tvalid_1's rmse: 8.1561\tvalid_1's l2: 66.5219\n",
      "[800]\ttraining's rmse: 4.53617\ttraining's l2: 20.5768\tvalid_1's rmse: 8.1522\tvalid_1's l2: 66.4584\n",
      "[900]\ttraining's rmse: 4.26792\ttraining's l2: 18.2151\tvalid_1's rmse: 8.15158\tvalid_1's l2: 66.4482\n",
      "Early stopping, best iteration is:\n",
      "[851]\ttraining's rmse: 4.3973\ttraining's l2: 19.3363\tvalid_1's rmse: 8.14992\tvalid_1's l2: 66.4211\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 8.15    \u001b[0m | \u001b[0m 0.6464  \u001b[0m | \u001b[0m 212.8   \u001b[0m | \u001b[0m 9.076   \u001b[0m | \u001b[0m 32.07   \u001b[0m | \u001b[0m 22.8    \u001b[0m | \u001b[0m 47.68   \u001b[0m | \u001b[0m 48.94   \u001b[0m | \u001b[0m 5.883   \u001b[0m | \u001b[0m 0.8768  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 7.70776\ttraining's l2: 59.4096\tvalid_1's rmse: 8.46812\tvalid_1's l2: 71.709\n",
      "[200]\ttraining's rmse: 6.82022\ttraining's l2: 46.5154\tvalid_1's rmse: 8.23814\tvalid_1's l2: 67.8669\n",
      "[300]\ttraining's rmse: 6.20141\ttraining's l2: 38.4575\tvalid_1's rmse: 8.18512\tvalid_1's l2: 66.9963\n",
      "[400]\ttraining's rmse: 5.69963\ttraining's l2: 32.4858\tvalid_1's rmse: 8.16826\tvalid_1's l2: 66.7205\n",
      "[500]\ttraining's rmse: 5.25472\ttraining's l2: 27.612\tvalid_1's rmse: 8.15877\tvalid_1's l2: 66.5655\n",
      "[600]\ttraining's rmse: 4.86589\ttraining's l2: 23.6768\tvalid_1's rmse: 8.15287\tvalid_1's l2: 66.4694\n",
      "[700]\ttraining's rmse: 4.5193\ttraining's l2: 20.4241\tvalid_1's rmse: 8.15326\tvalid_1's l2: 66.4756\n",
      "[800]\ttraining's rmse: 4.20697\ttraining's l2: 17.6986\tvalid_1's rmse: 8.1509\tvalid_1's l2: 66.4372\n",
      "Early stopping, best iteration is:\n",
      "[785]\ttraining's rmse: 4.25227\ttraining's l2: 18.0818\tvalid_1's rmse: 8.14929\tvalid_1's l2: 66.4109\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 8.149   \u001b[0m | \u001b[0m 0.6659  \u001b[0m | \u001b[0m 318.9   \u001b[0m | \u001b[0m 14.7    \u001b[0m | \u001b[0m 89.29   \u001b[0m | \u001b[0m 7.679   \u001b[0m | \u001b[0m 54.4    \u001b[0m | \u001b[0m 41.98   \u001b[0m | \u001b[0m 4.397   \u001b[0m | \u001b[0m 0.6464  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.58151\ttraining's l2: 57.4793\tvalid_1's rmse: 8.46286\tvalid_1's l2: 71.62\n",
      "[200]\ttraining's rmse: 6.62036\ttraining's l2: 43.8291\tvalid_1's rmse: 8.2387\tvalid_1's l2: 67.8761\n",
      "[300]\ttraining's rmse: 5.9687\ttraining's l2: 35.6253\tvalid_1's rmse: 8.18886\tvalid_1's l2: 67.0575\n",
      "[400]\ttraining's rmse: 5.45257\ttraining's l2: 29.7305\tvalid_1's rmse: 8.17427\tvalid_1's l2: 66.8187\n",
      "[500]\ttraining's rmse: 4.99984\ttraining's l2: 24.9984\tvalid_1's rmse: 8.1689\tvalid_1's l2: 66.7309\n",
      "[600]\ttraining's rmse: 4.59937\ttraining's l2: 21.1542\tvalid_1's rmse: 8.1653\tvalid_1's l2: 66.6721\n",
      "Early stopping, best iteration is:\n",
      "[597]\ttraining's rmse: 4.61118\ttraining's l2: 21.263\tvalid_1's rmse: 8.16491\tvalid_1's l2: 66.6658\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 8.165   \u001b[0m | \u001b[0m 0.8912  \u001b[0m | \u001b[0m 226.8   \u001b[0m | \u001b[0m 9.587   \u001b[0m | \u001b[0m 45.26   \u001b[0m | \u001b[0m 38.36   \u001b[0m | \u001b[0m 58.79   \u001b[0m | \u001b[0m 48.62   \u001b[0m | \u001b[0m 2.709   \u001b[0m | \u001b[0m 0.9832  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 8.01725\ttraining's l2: 64.2764\tvalid_1's rmse: 8.5251\tvalid_1's l2: 72.6773\n",
      "[200]\ttraining's rmse: 7.30519\ttraining's l2: 53.3658\tvalid_1's rmse: 8.28018\tvalid_1's l2: 68.5614\n",
      "[300]\ttraining's rmse: 6.84097\ttraining's l2: 46.7988\tvalid_1's rmse: 8.21032\tvalid_1's l2: 67.4094\n",
      "[400]\ttraining's rmse: 6.4748\ttraining's l2: 41.9231\tvalid_1's rmse: 8.19257\tvalid_1's l2: 67.1182\n",
      "[500]\ttraining's rmse: 6.15146\ttraining's l2: 37.8405\tvalid_1's rmse: 8.18627\tvalid_1's l2: 67.015\n",
      "[600]\ttraining's rmse: 5.85333\ttraining's l2: 34.2615\tvalid_1's rmse: 8.18291\tvalid_1's l2: 66.96\n",
      "[700]\ttraining's rmse: 5.58176\ttraining's l2: 31.1561\tvalid_1's rmse: 8.18078\tvalid_1's l2: 66.9252\n",
      "[800]\ttraining's rmse: 5.32557\ttraining's l2: 28.3617\tvalid_1's rmse: 8.17982\tvalid_1's l2: 66.9095\n",
      "[900]\ttraining's rmse: 5.09315\ttraining's l2: 25.9401\tvalid_1's rmse: 8.17784\tvalid_1's l2: 66.8771\n",
      "[1000]\ttraining's rmse: 4.87812\ttraining's l2: 23.7961\tvalid_1's rmse: 8.1767\tvalid_1's l2: 66.8584\n",
      "Early stopping, best iteration is:\n",
      "[931]\ttraining's rmse: 5.02441\ttraining's l2: 25.2447\tvalid_1's rmse: 8.17547\tvalid_1's l2: 66.8383\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 8.175   \u001b[0m | \u001b[0m 0.9319  \u001b[0m | \u001b[0m 265.1   \u001b[0m | \u001b[0m 8.635   \u001b[0m | \u001b[0m 55.92   \u001b[0m | \u001b[0m 4.254   \u001b[0m | \u001b[0m 32.04   \u001b[0m | \u001b[0m 37.83   \u001b[0m | \u001b[0m 6.108   \u001b[0m | \u001b[0m 0.9044  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.73256\ttraining's l2: 59.7925\tvalid_1's rmse: 8.47922\tvalid_1's l2: 71.8971\n",
      "[200]\ttraining's rmse: 6.84298\ttraining's l2: 46.8264\tvalid_1's rmse: 8.25666\tvalid_1's l2: 68.1725\n",
      "[300]\ttraining's rmse: 6.23736\ttraining's l2: 38.9047\tvalid_1's rmse: 8.20059\tvalid_1's l2: 67.2497\n",
      "[400]\ttraining's rmse: 5.76208\ttraining's l2: 33.2015\tvalid_1's rmse: 8.18074\tvalid_1's l2: 66.9246\n",
      "[500]\ttraining's rmse: 5.33453\ttraining's l2: 28.4572\tvalid_1's rmse: 8.17307\tvalid_1's l2: 66.7991\n",
      "[600]\ttraining's rmse: 4.96264\ttraining's l2: 24.6278\tvalid_1's rmse: 8.16994\tvalid_1's l2: 66.7479\n",
      "[700]\ttraining's rmse: 4.62864\ttraining's l2: 21.4243\tvalid_1's rmse: 8.16906\tvalid_1's l2: 66.7335\n",
      "Early stopping, best iteration is:\n",
      "[649]\ttraining's rmse: 4.79642\ttraining's l2: 23.0057\tvalid_1's rmse: 8.16684\tvalid_1's l2: 66.6972\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 8.167   \u001b[0m | \u001b[0m 0.8401  \u001b[0m | \u001b[0m 218.0   \u001b[0m | \u001b[0m 10.83   \u001b[0m | \u001b[0m 18.53   \u001b[0m | \u001b[0m 44.08   \u001b[0m | \u001b[0m 48.28   \u001b[0m | \u001b[0m 31.39   \u001b[0m | \u001b[0m 5.161   \u001b[0m | \u001b[0m 0.8321  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.59677\ttraining's l2: 57.7109\tvalid_1's rmse: 8.46455\tvalid_1's l2: 71.6486\n",
      "[200]\ttraining's rmse: 6.6295\ttraining's l2: 43.9503\tvalid_1's rmse: 8.24802\tvalid_1's l2: 68.0298\n",
      "[300]\ttraining's rmse: 5.97561\ttraining's l2: 35.7079\tvalid_1's rmse: 8.19425\tvalid_1's l2: 67.1458\n",
      "[400]\ttraining's rmse: 5.49267\ttraining's l2: 30.1694\tvalid_1's rmse: 8.17973\tvalid_1's l2: 66.908\n",
      "[500]\ttraining's rmse: 5.04293\ttraining's l2: 25.4311\tvalid_1's rmse: 8.17111\tvalid_1's l2: 66.7671\n",
      "[600]\ttraining's rmse: 4.63402\ttraining's l2: 21.4741\tvalid_1's rmse: 8.1728\tvalid_1's l2: 66.7946\n",
      "[700]\ttraining's rmse: 4.27682\ttraining's l2: 18.2912\tvalid_1's rmse: 8.17099\tvalid_1's l2: 66.765\n",
      "Early stopping, best iteration is:\n",
      "[697]\ttraining's rmse: 4.28734\ttraining's l2: 18.3813\tvalid_1's rmse: 8.17013\tvalid_1's l2: 66.751\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 8.17    \u001b[0m | \u001b[0m 0.7178  \u001b[0m | \u001b[0m 174.7   \u001b[0m | \u001b[0m 10.29   \u001b[0m | \u001b[0m 36.12   \u001b[0m | \u001b[0m 11.2    \u001b[0m | \u001b[0m 59.42   \u001b[0m | \u001b[0m 23.53   \u001b[0m | \u001b[0m 9.78    \u001b[0m | \u001b[0m 0.5628  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 8.11986\ttraining's l2: 65.9322\tvalid_1's rmse: 8.53333\tvalid_1's l2: 72.8177\n",
      "[200]\ttraining's rmse: 7.47394\ttraining's l2: 55.8597\tvalid_1's rmse: 8.27822\tvalid_1's l2: 68.529\n",
      "[300]\ttraining's rmse: 7.0688\ttraining's l2: 49.9679\tvalid_1's rmse: 8.2139\tvalid_1's l2: 67.4681\n",
      "[400]\ttraining's rmse: 6.73724\ttraining's l2: 45.3904\tvalid_1's rmse: 8.18888\tvalid_1's l2: 67.0577\n",
      "[500]\ttraining's rmse: 6.44142\ttraining's l2: 41.4919\tvalid_1's rmse: 8.17508\tvalid_1's l2: 66.832\n",
      "[600]\ttraining's rmse: 6.17097\ttraining's l2: 38.0809\tvalid_1's rmse: 8.16929\tvalid_1's l2: 66.7373\n",
      "[700]\ttraining's rmse: 5.92212\ttraining's l2: 35.0715\tvalid_1's rmse: 8.16948\tvalid_1's l2: 66.7403\n",
      "[800]\ttraining's rmse: 5.69137\ttraining's l2: 32.3916\tvalid_1's rmse: 8.16908\tvalid_1's l2: 66.7338\n",
      "Early stopping, best iteration is:\n",
      "[725]\ttraining's rmse: 5.86235\ttraining's l2: 34.3672\tvalid_1's rmse: 8.16704\tvalid_1's l2: 66.7005\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 8.167   \u001b[0m | \u001b[0m 0.8023  \u001b[0m | \u001b[0m 372.3   \u001b[0m | \u001b[0m 13.22   \u001b[0m | \u001b[0m 189.7   \u001b[0m | \u001b[0m 29.8    \u001b[0m | \u001b[0m 29.28   \u001b[0m | \u001b[0m 46.45   \u001b[0m | \u001b[0m 6.261   \u001b[0m | \u001b[0m 0.5583  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.82308\ttraining's l2: 61.2006\tvalid_1's rmse: 8.47452\tvalid_1's l2: 71.8175\n",
      "[200]\ttraining's rmse: 6.9981\ttraining's l2: 48.9735\tvalid_1's rmse: 8.23982\tvalid_1's l2: 67.8946\n",
      "[300]\ttraining's rmse: 6.44028\ttraining's l2: 41.4771\tvalid_1's rmse: 8.18319\tvalid_1's l2: 66.9646\n",
      "[400]\ttraining's rmse: 5.9947\ttraining's l2: 35.9364\tvalid_1's rmse: 8.16824\tvalid_1's l2: 66.7202\n",
      "[500]\ttraining's rmse: 5.60369\ttraining's l2: 31.4014\tvalid_1's rmse: 8.15759\tvalid_1's l2: 66.5462\n",
      "[600]\ttraining's rmse: 5.24976\ttraining's l2: 27.56\tvalid_1's rmse: 8.15277\tvalid_1's l2: 66.4677\n",
      "[700]\ttraining's rmse: 4.93296\ttraining's l2: 24.3341\tvalid_1's rmse: 8.14901\tvalid_1's l2: 66.4063\n",
      "Early stopping, best iteration is:\n",
      "[677]\ttraining's rmse: 5.00395\ttraining's l2: 25.0395\tvalid_1's rmse: 8.14751\tvalid_1's l2: 66.382\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 8.148   \u001b[0m | \u001b[0m 0.6121  \u001b[0m | \u001b[0m 56.01   \u001b[0m | \u001b[0m 13.32   \u001b[0m | \u001b[0m 86.02   \u001b[0m | \u001b[0m 2.535   \u001b[0m | \u001b[0m 43.43   \u001b[0m | \u001b[0m 12.8    \u001b[0m | \u001b[0m 3.448   \u001b[0m | \u001b[0m 0.5243  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's rmse: 7.79361\ttraining's l2: 60.7404\tvalid_1's rmse: 8.47343\tvalid_1's l2: 71.799\n",
      "[200]\ttraining's rmse: 6.95953\ttraining's l2: 48.435\tvalid_1's rmse: 8.23464\tvalid_1's l2: 67.8092\n",
      "[300]\ttraining's rmse: 6.38912\ttraining's l2: 40.8209\tvalid_1's rmse: 8.18224\tvalid_1's l2: 66.949\n",
      "[400]\ttraining's rmse: 5.9269\ttraining's l2: 35.1281\tvalid_1's rmse: 8.15833\tvalid_1's l2: 66.5583\n",
      "[500]\ttraining's rmse: 5.51888\ttraining's l2: 30.4581\tvalid_1's rmse: 8.14835\tvalid_1's l2: 66.3956\n",
      "[600]\ttraining's rmse: 5.15822\ttraining's l2: 26.6072\tvalid_1's rmse: 8.14151\tvalid_1's l2: 66.2841\n",
      "[700]\ttraining's rmse: 4.82979\ttraining's l2: 23.3268\tvalid_1's rmse: 8.13982\tvalid_1's l2: 66.2567\n",
      "Early stopping, best iteration is:\n",
      "[688]\ttraining's rmse: 4.86709\ttraining's l2: 23.6885\tvalid_1's rmse: 8.13894\tvalid_1's l2: 66.2424\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 8.139   \u001b[0m | \u001b[0m 0.6732  \u001b[0m | \u001b[0m 294.8   \u001b[0m | \u001b[0m 15.04   \u001b[0m | \u001b[0m 117.9   \u001b[0m | \u001b[0m 26.82   \u001b[0m | \u001b[0m 46.07   \u001b[0m | \u001b[0m 13.76   \u001b[0m | \u001b[0m 3.302   \u001b[0m | \u001b[0m 0.5363  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.89662\ttraining's l2: 62.3565\tvalid_1's rmse: 8.49662\tvalid_1's l2: 72.1925\n",
      "[200]\ttraining's rmse: 7.11129\ttraining's l2: 50.5705\tvalid_1's rmse: 8.25243\tvalid_1's l2: 68.1026\n",
      "[300]\ttraining's rmse: 6.5801\ttraining's l2: 43.2978\tvalid_1's rmse: 8.18959\tvalid_1's l2: 67.0693\n",
      "[400]\ttraining's rmse: 6.14832\ttraining's l2: 37.8019\tvalid_1's rmse: 8.17042\tvalid_1's l2: 66.7558\n",
      "[500]\ttraining's rmse: 5.76999\ttraining's l2: 33.2928\tvalid_1's rmse: 8.15815\tvalid_1's l2: 66.5555\n",
      "[600]\ttraining's rmse: 5.42915\ttraining's l2: 29.4757\tvalid_1's rmse: 8.15741\tvalid_1's l2: 66.5433\n",
      "[700]\ttraining's rmse: 5.12675\ttraining's l2: 26.2836\tvalid_1's rmse: 8.15589\tvalid_1's l2: 66.5185\n",
      "Early stopping, best iteration is:\n",
      "[633]\ttraining's rmse: 5.32663\ttraining's l2: 28.373\tvalid_1's rmse: 8.15468\tvalid_1's l2: 66.4989\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 8.155   \u001b[0m | \u001b[0m 0.819   \u001b[0m | \u001b[0m 33.68   \u001b[0m | \u001b[0m 14.31   \u001b[0m | \u001b[0m 81.77   \u001b[0m | \u001b[0m 30.16   \u001b[0m | \u001b[0m 39.27   \u001b[0m | \u001b[0m 19.79   \u001b[0m | \u001b[0m 7.493   \u001b[0m | \u001b[0m 0.6173  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.96773\ttraining's l2: 63.4847\tvalid_1's rmse: 8.51439\tvalid_1's l2: 72.4948\n",
      "[200]\ttraining's rmse: 7.2188\ttraining's l2: 52.111\tvalid_1's rmse: 8.26584\tvalid_1's l2: 68.3241\n",
      "[300]\ttraining's rmse: 6.7152\ttraining's l2: 45.0939\tvalid_1's rmse: 8.20731\tvalid_1's l2: 67.3599\n",
      "[400]\ttraining's rmse: 6.31564\ttraining's l2: 39.8873\tvalid_1's rmse: 8.18537\tvalid_1's l2: 67.0004\n",
      "[500]\ttraining's rmse: 5.96571\ttraining's l2: 35.5897\tvalid_1's rmse: 8.17933\tvalid_1's l2: 66.9015\n",
      "[600]\ttraining's rmse: 5.64918\ttraining's l2: 31.9132\tvalid_1's rmse: 8.17445\tvalid_1's l2: 66.8216\n",
      "[700]\ttraining's rmse: 5.3665\ttraining's l2: 28.7993\tvalid_1's rmse: 8.16902\tvalid_1's l2: 66.733\n",
      "[800]\ttraining's rmse: 5.10196\ttraining's l2: 26.03\tvalid_1's rmse: 8.16675\tvalid_1's l2: 66.6957\n",
      "[900]\ttraining's rmse: 4.86089\ttraining's l2: 23.6283\tvalid_1's rmse: 8.16529\tvalid_1's l2: 66.6719\n",
      "Early stopping, best iteration is:\n",
      "[887]\ttraining's rmse: 4.89079\ttraining's l2: 23.9198\tvalid_1's rmse: 8.16475\tvalid_1's l2: 66.6631\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 8.165   \u001b[0m | \u001b[0m 0.8386  \u001b[0m | \u001b[0m 251.6   \u001b[0m | \u001b[0m 11.27   \u001b[0m | \u001b[0m 12.03   \u001b[0m | \u001b[0m 26.55   \u001b[0m | \u001b[0m 35.24   \u001b[0m | \u001b[0m 33.82   \u001b[0m | \u001b[0m 8.208   \u001b[0m | \u001b[0m 0.7601  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.66754\ttraining's l2: 58.7911\tvalid_1's rmse: 8.46571\tvalid_1's l2: 71.6683\n",
      "[200]\ttraining's rmse: 6.74185\ttraining's l2: 45.4526\tvalid_1's rmse: 8.23926\tvalid_1's l2: 67.8855\n",
      "[300]\ttraining's rmse: 6.10745\ttraining's l2: 37.301\tvalid_1's rmse: 8.1889\tvalid_1's l2: 67.0582\n",
      "[400]\ttraining's rmse: 5.59118\ttraining's l2: 31.2612\tvalid_1's rmse: 8.16703\tvalid_1's l2: 66.7004\n",
      "[500]\ttraining's rmse: 5.1421\ttraining's l2: 26.4412\tvalid_1's rmse: 8.1599\tvalid_1's l2: 66.584\n",
      "[600]\ttraining's rmse: 4.74572\ttraining's l2: 22.5218\tvalid_1's rmse: 8.15415\tvalid_1's l2: 66.4902\n",
      "[700]\ttraining's rmse: 4.39194\ttraining's l2: 19.2891\tvalid_1's rmse: 8.15546\tvalid_1's l2: 66.5116\n",
      "Early stopping, best iteration is:\n",
      "[666]\ttraining's rmse: 4.50913\ttraining's l2: 20.3322\tvalid_1's rmse: 8.15278\tvalid_1's l2: 66.4679\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 8.153   \u001b[0m | \u001b[0m 0.8413  \u001b[0m | \u001b[0m 113.0   \u001b[0m | \u001b[0m 14.24   \u001b[0m | \u001b[0m 83.93   \u001b[0m | \u001b[0m 12.57   \u001b[0m | \u001b[0m 54.38   \u001b[0m | \u001b[0m 12.53   \u001b[0m | \u001b[0m 9.638   \u001b[0m | \u001b[0m 0.8934  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.50291\ttraining's l2: 56.2936\tvalid_1's rmse: 8.44479\tvalid_1's l2: 71.3145\n",
      "[200]\ttraining's rmse: 6.47797\ttraining's l2: 41.9641\tvalid_1's rmse: 8.22999\tvalid_1's l2: 67.7327\n",
      "[300]\ttraining's rmse: 5.77785\ttraining's l2: 33.3836\tvalid_1's rmse: 8.17838\tvalid_1's l2: 66.8859\n",
      "[400]\ttraining's rmse: 5.23113\ttraining's l2: 27.3648\tvalid_1's rmse: 8.16011\tvalid_1's l2: 66.5874\n",
      "[500]\ttraining's rmse: 4.74997\ttraining's l2: 22.5622\tvalid_1's rmse: 8.15255\tvalid_1's l2: 66.464\n",
      "[600]\ttraining's rmse: 4.32633\ttraining's l2: 18.7171\tvalid_1's rmse: 8.15234\tvalid_1's l2: 66.4606\n",
      "Early stopping, best iteration is:\n",
      "[520]\ttraining's rmse: 4.66443\ttraining's l2: 21.7569\tvalid_1's rmse: 8.1512\tvalid_1's l2: 66.442\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 8.151   \u001b[0m | \u001b[0m 0.7133  \u001b[0m | \u001b[0m 176.0   \u001b[0m | \u001b[0m 11.74   \u001b[0m | \u001b[0m 37.72   \u001b[0m | \u001b[0m 15.1    \u001b[0m | \u001b[0m 60.53   \u001b[0m | \u001b[0m 20.59   \u001b[0m | \u001b[0m 3.133   \u001b[0m | \u001b[0m 0.9567  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 8.15181\ttraining's l2: 66.452\tvalid_1's rmse: 8.55027\tvalid_1's l2: 73.1071\n",
      "[200]\ttraining's rmse: 7.513\ttraining's l2: 56.4451\tvalid_1's rmse: 8.2859\tvalid_1's l2: 68.6561\n",
      "[300]\ttraining's rmse: 7.10172\ttraining's l2: 50.4344\tvalid_1's rmse: 8.21501\tvalid_1's l2: 67.4865\n",
      "[400]\ttraining's rmse: 6.7734\ttraining's l2: 45.8789\tvalid_1's rmse: 8.18621\tvalid_1's l2: 67.014\n",
      "[500]\ttraining's rmse: 6.48692\ttraining's l2: 42.0801\tvalid_1's rmse: 8.17047\tvalid_1's l2: 66.7565\n",
      "[600]\ttraining's rmse: 6.22917\ttraining's l2: 38.8026\tvalid_1's rmse: 8.16183\tvalid_1's l2: 66.6155\n",
      "[700]\ttraining's rmse: 5.99368\ttraining's l2: 35.9242\tvalid_1's rmse: 8.15553\tvalid_1's l2: 66.5126\n",
      "[800]\ttraining's rmse: 5.77218\ttraining's l2: 33.3181\tvalid_1's rmse: 8.1509\tvalid_1's l2: 66.4372\n",
      "[900]\ttraining's rmse: 5.56337\ttraining's l2: 30.9511\tvalid_1's rmse: 8.15181\tvalid_1's l2: 66.452\n",
      "[1000]\ttraining's rmse: 5.36525\ttraining's l2: 28.7859\tvalid_1's rmse: 8.14947\tvalid_1's l2: 66.4138\n",
      "[1100]\ttraining's rmse: 5.18023\ttraining's l2: 26.8348\tvalid_1's rmse: 8.14891\tvalid_1's l2: 66.4048\n",
      "[1200]\ttraining's rmse: 5.00156\ttraining's l2: 25.0156\tvalid_1's rmse: 8.14883\tvalid_1's l2: 66.4034\n",
      "[1300]\ttraining's rmse: 4.83254\ttraining's l2: 23.3534\tvalid_1's rmse: 8.14625\tvalid_1's l2: 66.3614\n",
      "[1400]\ttraining's rmse: 4.67034\ttraining's l2: 21.8121\tvalid_1's rmse: 8.1468\tvalid_1's l2: 66.3703\n",
      "Early stopping, best iteration is:\n",
      "[1331]\ttraining's rmse: 4.78007\ttraining's l2: 22.8491\tvalid_1's rmse: 8.14465\tvalid_1's l2: 66.3354\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 8.145   \u001b[0m | \u001b[0m 0.5216  \u001b[0m | \u001b[0m 202.1   \u001b[0m | \u001b[0m 11.75   \u001b[0m | \u001b[0m 46.22   \u001b[0m | \u001b[0m 19.62   \u001b[0m | \u001b[0m 26.33   \u001b[0m | \u001b[0m 34.73   \u001b[0m | \u001b[0m 3.456   \u001b[0m | \u001b[0m 0.9112  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.87203\ttraining's l2: 61.9689\tvalid_1's rmse: 8.4799\tvalid_1's l2: 71.9088\n",
      "[200]\ttraining's rmse: 7.07851\ttraining's l2: 50.1053\tvalid_1's rmse: 8.23405\tvalid_1's l2: 67.7996\n",
      "[300]\ttraining's rmse: 6.54347\ttraining's l2: 42.8169\tvalid_1's rmse: 8.17671\tvalid_1's l2: 66.8586\n",
      "[400]\ttraining's rmse: 6.10942\ttraining's l2: 37.325\tvalid_1's rmse: 8.15418\tvalid_1's l2: 66.4907\n",
      "[500]\ttraining's rmse: 5.72182\ttraining's l2: 32.7393\tvalid_1's rmse: 8.14147\tvalid_1's l2: 66.2836\n",
      "[600]\ttraining's rmse: 5.37931\ttraining's l2: 28.937\tvalid_1's rmse: 8.1391\tvalid_1's l2: 66.245\n",
      "[700]\ttraining's rmse: 5.05975\ttraining's l2: 25.6011\tvalid_1's rmse: 8.13766\tvalid_1's l2: 66.2215\n",
      "[800]\ttraining's rmse: 4.77036\ttraining's l2: 22.7564\tvalid_1's rmse: 8.14257\tvalid_1's l2: 66.3014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[703]\ttraining's rmse: 5.05027\ttraining's l2: 25.5052\tvalid_1's rmse: 8.13667\tvalid_1's l2: 66.2054\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 8.137   \u001b[0m | \u001b[0m 0.5543  \u001b[0m | \u001b[0m 266.6   \u001b[0m | \u001b[0m 12.65   \u001b[0m | \u001b[0m 147.4   \u001b[0m | \u001b[0m 13.2    \u001b[0m | \u001b[0m 45.91   \u001b[0m | \u001b[0m 23.86   \u001b[0m | \u001b[0m 8.469   \u001b[0m | \u001b[0m 0.9726  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.84724\ttraining's l2: 61.5792\tvalid_1's rmse: 8.48472\tvalid_1's l2: 71.9905\n",
      "[200]\ttraining's rmse: 7.03253\ttraining's l2: 49.4565\tvalid_1's rmse: 8.24705\tvalid_1's l2: 68.0138\n",
      "[300]\ttraining's rmse: 6.49798\ttraining's l2: 42.2238\tvalid_1's rmse: 8.19342\tvalid_1's l2: 67.1322\n",
      "[400]\ttraining's rmse: 6.09192\ttraining's l2: 37.1115\tvalid_1's rmse: 8.17342\tvalid_1's l2: 66.8048\n",
      "[500]\ttraining's rmse: 5.73866\ttraining's l2: 32.9322\tvalid_1's rmse: 8.16334\tvalid_1's l2: 66.6401\n",
      "[600]\ttraining's rmse: 5.40838\ttraining's l2: 29.2506\tvalid_1's rmse: 8.15833\tvalid_1's l2: 66.5584\n",
      "[700]\ttraining's rmse: 5.10283\ttraining's l2: 26.0388\tvalid_1's rmse: 8.15621\tvalid_1's l2: 66.5238\n",
      "[800]\ttraining's rmse: 4.81959\ttraining's l2: 23.2284\tvalid_1's rmse: 8.15523\tvalid_1's l2: 66.5078\n",
      "[900]\ttraining's rmse: 4.55522\ttraining's l2: 20.75\tvalid_1's rmse: 8.15499\tvalid_1's l2: 66.5039\n",
      "Early stopping, best iteration is:\n",
      "[847]\ttraining's rmse: 4.69252\ttraining's l2: 22.0197\tvalid_1's rmse: 8.15429\tvalid_1's l2: 66.4924\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 8.154   \u001b[0m | \u001b[0m 0.6567  \u001b[0m | \u001b[0m 433.3   \u001b[0m | \u001b[0m 9.49    \u001b[0m | \u001b[0m 18.65   \u001b[0m | \u001b[0m 34.59   \u001b[0m | \u001b[0m 40.31   \u001b[0m | \u001b[0m 9.121   \u001b[0m | \u001b[0m 5.538   \u001b[0m | \u001b[0m 0.5815  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 8.08881\ttraining's l2: 65.4288\tvalid_1's rmse: 8.53022\tvalid_1's l2: 72.7647\n",
      "[200]\ttraining's rmse: 7.42466\ttraining's l2: 55.1255\tvalid_1's rmse: 8.26855\tvalid_1's l2: 68.369\n",
      "[300]\ttraining's rmse: 7.00054\ttraining's l2: 49.0076\tvalid_1's rmse: 8.20152\tvalid_1's l2: 67.2649\n",
      "[400]\ttraining's rmse: 6.66303\ttraining's l2: 44.396\tvalid_1's rmse: 8.17701\tvalid_1's l2: 66.8634\n",
      "[500]\ttraining's rmse: 6.36386\ttraining's l2: 40.4987\tvalid_1's rmse: 8.1688\tvalid_1's l2: 66.7292\n",
      "[600]\ttraining's rmse: 6.09157\ttraining's l2: 37.1073\tvalid_1's rmse: 8.16192\tvalid_1's l2: 66.6169\n",
      "[700]\ttraining's rmse: 5.83971\ttraining's l2: 34.1022\tvalid_1's rmse: 8.15855\tvalid_1's l2: 66.562\n",
      "Early stopping, best iteration is:\n",
      "[672]\ttraining's rmse: 5.90951\ttraining's l2: 34.9223\tvalid_1's rmse: 8.15779\tvalid_1's l2: 66.5495\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 8.158   \u001b[0m | \u001b[0m 0.6299  \u001b[0m | \u001b[0m 460.8   \u001b[0m | \u001b[0m 9.51    \u001b[0m | \u001b[0m 107.5   \u001b[0m | \u001b[0m 27.24   \u001b[0m | \u001b[0m 29.0    \u001b[0m | \u001b[0m 36.78   \u001b[0m | \u001b[0m 0.7092  \u001b[0m | \u001b[0m 0.9752  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "lgbBO = BayesianOptimization(f=lgb_rmse_eval, pbounds=bayesian_params, random_state=1000)\n",
    "lgbBO.maximize(init_points=5, n_iter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 8.159582609052569,\n",
       "  'params': {'colsample_bytree': 0.8267947927323047,\n",
       "   'max_bin': 66.35340213095881,\n",
       "   'max_depth': 15.602262914792195,\n",
       "   'min_child_samples': 101.61636627131966,\n",
       "   'min_child_weight': 43.75125222391973,\n",
       "   'num_leaves': 32.4933072369088,\n",
       "   'reg_alpha': 2.045074142206765,\n",
       "   'reg_lambda': 3.9725474189957124,\n",
       "   'subsample': 0.61656609867419}},\n",
       " {'target': 8.17004539342175,\n",
       "  'params': {'colsample_bytree': 0.9208703621265308,\n",
       "   'max_bin': 111.47034874950597,\n",
       "   'max_depth': 13.93975626865927,\n",
       "   'min_child_samples': 84.50928428367985,\n",
       "   'min_child_weight': 9.930569477666833,\n",
       "   'num_leaves': 53.7415765836856,\n",
       "   'reg_alpha': 3.488408227688028,\n",
       "   'reg_lambda': 8.853486706603126,\n",
       "   'subsample': 0.9763221996107709}},\n",
       " {'target': 8.171562888283582,\n",
       "  'params': {'colsample_bytree': 0.9655717173387555,\n",
       "   'max_bin': 213.56116699200484,\n",
       "   'max_depth': 8.23185327509497,\n",
       "   'min_child_samples': 196.58522207365755,\n",
       "   'min_child_weight': 17.64224649812914,\n",
       "   'num_leaves': 52.26748775555215,\n",
       "   'reg_alpha': 18.100234609702014,\n",
       "   'reg_lambda': 0.35202387361566295,\n",
       "   'subsample': 0.9275291266202039}},\n",
       " {'target': 8.16914417451538,\n",
       "  'params': {'colsample_bytree': 0.8286267540702462,\n",
       "   'max_bin': 385.1846671311669,\n",
       "   'max_depth': 12.432697904035154,\n",
       "   'min_child_samples': 178.16765795601393,\n",
       "   'min_child_weight': 45.305683154470756,\n",
       "   'num_leaves': 24.416868022358166,\n",
       "   'reg_alpha': 3.737091239759974,\n",
       "   'reg_lambda': 2.447047468388093,\n",
       "   'subsample': 0.5666523762257424}},\n",
       " {'target': 8.17904165690679,\n",
       "  'params': {'colsample_bytree': 0.8489625502048597,\n",
       "   'max_bin': 205.1203929277082,\n",
       "   'max_depth': 15.064977531233142,\n",
       "   'min_child_samples': 44.39142695200357,\n",
       "   'min_child_weight': 22.19245941780866,\n",
       "   'num_leaves': 24.725728110186907,\n",
       "   'reg_alpha': 34.57497870582523,\n",
       "   'reg_lambda': 4.697436829706333,\n",
       "   'subsample': 0.5641110948268239}},\n",
       " {'target': 8.156273768283857,\n",
       "  'params': {'colsample_bytree': 0.5041596724537438,\n",
       "   'max_bin': 210.5637682816542,\n",
       "   'max_depth': 10.116220031749162,\n",
       "   'min_child_samples': 197.04656187754293,\n",
       "   'min_child_weight': 22.582415168136066,\n",
       "   'num_leaves': 56.79669326504983,\n",
       "   'reg_alpha': 14.70114062974816,\n",
       "   'reg_lambda': 1.0303272898052314,\n",
       "   'subsample': 0.6526049808892315}},\n",
       " {'target': 8.150212234927787,\n",
       "  'params': {'colsample_bytree': 0.684242198017279,\n",
       "   'max_bin': 25.210162988456304,\n",
       "   'max_depth': 10.86227380741599,\n",
       "   'min_child_samples': 168.36821974494066,\n",
       "   'min_child_weight': 24.29033031493233,\n",
       "   'num_leaves': 44.837188494255855,\n",
       "   'reg_alpha': 7.394401788129303,\n",
       "   'reg_lambda': 2.3419398516773966,\n",
       "   'subsample': 0.7360379914074062}},\n",
       " {'target': 8.149067543075125,\n",
       "  'params': {'colsample_bytree': 0.7675419859658956,\n",
       "   'max_bin': 132.0199627366569,\n",
       "   'max_depth': 14.298915241781739,\n",
       "   'min_child_samples': 40.3924686618707,\n",
       "   'min_child_weight': 44.75925548935725,\n",
       "   'num_leaves': 35.85435623811703,\n",
       "   'reg_alpha': 33.161736698845125,\n",
       "   'reg_lambda': 4.179346930104301,\n",
       "   'subsample': 0.9904558712794339}},\n",
       " {'target': 8.155718457103376,\n",
       "  'params': {'colsample_bytree': 0.5909121889605802,\n",
       "   'max_bin': 151.4451283058841,\n",
       "   'max_depth': 8.12570510974261,\n",
       "   'min_child_samples': 67.70982537785662,\n",
       "   'min_child_weight': 12.179556446382666,\n",
       "   'num_leaves': 57.97492807929662,\n",
       "   'reg_alpha': 5.11327135472758,\n",
       "   'reg_lambda': 2.9634057731939567,\n",
       "   'subsample': 0.5073924938486567}},\n",
       " {'target': 8.160374675411536,\n",
       "  'params': {'colsample_bytree': 0.9254172005439811,\n",
       "   'max_bin': 416.7208648802382,\n",
       "   'max_depth': 10.76574953773526,\n",
       "   'min_child_samples': 126.91007769267303,\n",
       "   'min_child_weight': 26.11112160742515,\n",
       "   'num_leaves': 29.613949340778454,\n",
       "   'reg_alpha': 48.70271304115722,\n",
       "   'reg_lambda': 7.307286495379972,\n",
       "   'subsample': 0.5610568992324143}},\n",
       " {'target': 8.145628680763515,\n",
       "  'params': {'colsample_bytree': 0.5141043094390911,\n",
       "   'max_bin': 254.3407418874025,\n",
       "   'max_depth': 15.332770159742921,\n",
       "   'min_child_samples': 103.21393354412879,\n",
       "   'min_child_weight': 46.90043392010393,\n",
       "   'num_leaves': 32.33623155988221,\n",
       "   'reg_alpha': 12.21846097676822,\n",
       "   'reg_lambda': 9.777330191074164,\n",
       "   'subsample': 0.6988643735876281}},\n",
       " {'target': 8.153547272594029,\n",
       "  'params': {'colsample_bytree': 0.5714645329357839,\n",
       "   'max_bin': 420.3449035602966,\n",
       "   'max_depth': 15.043879875899249,\n",
       "   'min_child_samples': 124.99880880459827,\n",
       "   'min_child_weight': 3.5244762071364786,\n",
       "   'num_leaves': 35.44420386327446,\n",
       "   'reg_alpha': 22.54827686645599,\n",
       "   'reg_lambda': 3.945893499815971,\n",
       "   'subsample': 0.9636697925128256}},\n",
       " {'target': 8.161748633056996,\n",
       "  'params': {'colsample_bytree': 0.579276254301057,\n",
       "   'max_bin': 255.18517768650602,\n",
       "   'max_depth': 13.550001654403957,\n",
       "   'min_child_samples': 12.505066102402521,\n",
       "   'min_child_weight': 29.42773011515404,\n",
       "   'num_leaves': 39.820389083540235,\n",
       "   'reg_alpha': 33.960034049410424,\n",
       "   'reg_lambda': 5.65260345331489,\n",
       "   'subsample': 0.8581227529538189}},\n",
       " {'target': 8.149915172081283,\n",
       "  'params': {'colsample_bytree': 0.6464371516415328,\n",
       "   'max_bin': 212.7951882260217,\n",
       "   'max_depth': 9.075548813868783,\n",
       "   'min_child_samples': 32.06927980062517,\n",
       "   'min_child_weight': 22.79731494793803,\n",
       "   'num_leaves': 47.67936927865554,\n",
       "   'reg_alpha': 48.938112708376934,\n",
       "   'reg_lambda': 5.882906904286742,\n",
       "   'subsample': 0.8768332875895308}},\n",
       " {'target': 8.149286865270724,\n",
       "  'params': {'colsample_bytree': 0.66587818349067,\n",
       "   'max_bin': 318.85670293585764,\n",
       "   'max_depth': 14.696083656856331,\n",
       "   'min_child_samples': 89.28919948696489,\n",
       "   'min_child_weight': 7.6786440348103335,\n",
       "   'num_leaves': 54.40354378771569,\n",
       "   'reg_alpha': 41.984699177480934,\n",
       "   'reg_lambda': 4.397204203719935,\n",
       "   'subsample': 0.6464426489126807}},\n",
       " {'target': 8.16491111490235,\n",
       "  'params': {'colsample_bytree': 0.8911768943126078,\n",
       "   'max_bin': 226.82335694733166,\n",
       "   'max_depth': 9.58652048927745,\n",
       "   'min_child_samples': 45.26117358341472,\n",
       "   'min_child_weight': 38.35804601447657,\n",
       "   'num_leaves': 58.79479371344334,\n",
       "   'reg_alpha': 48.616770778588304,\n",
       "   'reg_lambda': 2.7092744689647326,\n",
       "   'subsample': 0.9831742043197643}},\n",
       " {'target': 8.175468831394106,\n",
       "  'params': {'colsample_bytree': 0.9318807646090632,\n",
       "   'max_bin': 265.05132830154224,\n",
       "   'max_depth': 8.634833232254739,\n",
       "   'min_child_samples': 55.92302516802116,\n",
       "   'min_child_weight': 4.254390351086849,\n",
       "   'num_leaves': 32.03886139159377,\n",
       "   'reg_alpha': 37.83364228095505,\n",
       "   'reg_lambda': 6.107782167557087,\n",
       "   'subsample': 0.9043824557585871}},\n",
       " {'target': 8.166836529387334,\n",
       "  'params': {'colsample_bytree': 0.8400676823446944,\n",
       "   'max_bin': 218.03126694416892,\n",
       "   'max_depth': 10.828374957564478,\n",
       "   'min_child_samples': 18.53296578103854,\n",
       "   'min_child_weight': 44.08053706636261,\n",
       "   'num_leaves': 48.2828624584605,\n",
       "   'reg_alpha': 31.386355289971423,\n",
       "   'reg_lambda': 5.16125396023365,\n",
       "   'subsample': 0.8320839011056689}},\n",
       " {'target': 8.170130246897218,\n",
       "  'params': {'colsample_bytree': 0.7178480127649094,\n",
       "   'max_bin': 174.6944105583218,\n",
       "   'max_depth': 10.291625851974237,\n",
       "   'min_child_samples': 36.11565074404746,\n",
       "   'min_child_weight': 11.2039887326501,\n",
       "   'num_leaves': 59.424618627709016,\n",
       "   'reg_alpha': 23.53333846758369,\n",
       "   'reg_lambda': 9.779720391939048,\n",
       "   'subsample': 0.562822235810886}},\n",
       " {'target': 8.167038841242782,\n",
       "  'params': {'colsample_bytree': 0.8023267797378124,\n",
       "   'max_bin': 372.2860100070965,\n",
       "   'max_depth': 13.219571070044436,\n",
       "   'min_child_samples': 189.73023488549583,\n",
       "   'min_child_weight': 29.798620436536304,\n",
       "   'num_leaves': 29.276672554717873,\n",
       "   'reg_alpha': 46.44571088697003,\n",
       "   'reg_lambda': 6.261069677877012,\n",
       "   'subsample': 0.5583173239443255}},\n",
       " {'target': 8.147513803212206,\n",
       "  'params': {'colsample_bytree': 0.6120817815183712,\n",
       "   'max_bin': 56.0127776812391,\n",
       "   'max_depth': 13.317409740581677,\n",
       "   'min_child_samples': 86.02488861817797,\n",
       "   'min_child_weight': 2.5349663365974218,\n",
       "   'num_leaves': 43.42729499707772,\n",
       "   'reg_alpha': 12.803004993547578,\n",
       "   'reg_lambda': 3.4478154743231246,\n",
       "   'subsample': 0.5242887645884554}},\n",
       " {'target': 8.13894101538311,\n",
       "  'params': {'colsample_bytree': 0.6732251624430192,\n",
       "   'max_bin': 294.7874607558146,\n",
       "   'max_depth': 15.03557802324086,\n",
       "   'min_child_samples': 117.94571860407643,\n",
       "   'min_child_weight': 26.818627877552583,\n",
       "   'num_leaves': 46.0687492784547,\n",
       "   'reg_alpha': 13.764074735933072,\n",
       "   'reg_lambda': 3.301926621834465,\n",
       "   'subsample': 0.536326218637151}},\n",
       " {'target': 8.154683374404343,\n",
       "  'params': {'colsample_bytree': 0.8190266681526392,\n",
       "   'max_bin': 33.68191484561372,\n",
       "   'max_depth': 14.314513862874048,\n",
       "   'min_child_samples': 81.77454047856418,\n",
       "   'min_child_weight': 30.156229899215923,\n",
       "   'num_leaves': 39.26811414744852,\n",
       "   'reg_alpha': 19.79003701633323,\n",
       "   'reg_lambda': 7.492847592955431,\n",
       "   'subsample': 0.6173422050108449}},\n",
       " {'target': 8.164748085806893,\n",
       "  'params': {'colsample_bytree': 0.8385856517896118,\n",
       "   'max_bin': 251.61892402891536,\n",
       "   'max_depth': 11.267276031992282,\n",
       "   'min_child_samples': 12.028025102003358,\n",
       "   'min_child_weight': 26.54668925131558,\n",
       "   'num_leaves': 35.242767697893,\n",
       "   'reg_alpha': 33.81741728271385,\n",
       "   'reg_lambda': 8.207769446273675,\n",
       "   'subsample': 0.7600707686142381}},\n",
       " {'target': 8.152783862594912,\n",
       "  'params': {'colsample_bytree': 0.8413340624812129,\n",
       "   'max_bin': 112.95704521595005,\n",
       "   'max_depth': 14.243514744878922,\n",
       "   'min_child_samples': 83.93291178417509,\n",
       "   'min_child_weight': 12.57141148775892,\n",
       "   'num_leaves': 54.37843338591637,\n",
       "   'reg_alpha': 12.531081036608146,\n",
       "   'reg_lambda': 9.638416163966015,\n",
       "   'subsample': 0.8933991595278337}},\n",
       " {'target': 8.151197862871083,\n",
       "  'params': {'colsample_bytree': 0.7133186235078599,\n",
       "   'max_bin': 176.04315769229578,\n",
       "   'max_depth': 11.737561818020156,\n",
       "   'min_child_samples': 37.720533616101974,\n",
       "   'min_child_weight': 15.104565535427446,\n",
       "   'num_leaves': 60.52547142700553,\n",
       "   'reg_alpha': 20.585488612917406,\n",
       "   'reg_lambda': 3.132610566739974,\n",
       "   'subsample': 0.9566551242084859}},\n",
       " {'target': 8.144653897217102,\n",
       "  'params': {'colsample_bytree': 0.5215941528662238,\n",
       "   'max_bin': 202.0957478598821,\n",
       "   'max_depth': 11.751186906225776,\n",
       "   'min_child_samples': 46.216784343811774,\n",
       "   'min_child_weight': 19.619519052349883,\n",
       "   'num_leaves': 26.331862800604647,\n",
       "   'reg_alpha': 34.72888116173019,\n",
       "   'reg_lambda': 3.455560380084788,\n",
       "   'subsample': 0.9111741395415324}},\n",
       " {'target': 8.136670067856032,\n",
       "  'params': {'colsample_bytree': 0.5543446579385094,\n",
       "   'max_bin': 266.60902033120465,\n",
       "   'max_depth': 12.646496395874314,\n",
       "   'min_child_samples': 147.43609706902322,\n",
       "   'min_child_weight': 13.197385976374632,\n",
       "   'num_leaves': 45.91016035048342,\n",
       "   'reg_alpha': 23.860024134242312,\n",
       "   'reg_lambda': 8.46904710281589,\n",
       "   'subsample': 0.9725842699472822}},\n",
       " {'target': 8.15429017011101,\n",
       "  'params': {'colsample_bytree': 0.6566524967268413,\n",
       "   'max_bin': 433.3057760989586,\n",
       "   'max_depth': 9.490104821932347,\n",
       "   'min_child_samples': 18.647514843077314,\n",
       "   'min_child_weight': 34.592646399786794,\n",
       "   'num_leaves': 40.311724206988984,\n",
       "   'reg_alpha': 9.1208781474659,\n",
       "   'reg_lambda': 5.538063770411299,\n",
       "   'subsample': 0.5814532301163122}},\n",
       " {'target': 8.157789482256456,\n",
       "  'params': {'colsample_bytree': 0.629860556156569,\n",
       "   'max_bin': 460.82332141706166,\n",
       "   'max_depth': 9.509502228957304,\n",
       "   'min_child_samples': 107.51085863131226,\n",
       "   'min_child_weight': 27.23714172016,\n",
       "   'num_leaves': 29.000040122066785,\n",
       "   'reg_alpha': 36.779227385182146,\n",
       "   'reg_lambda': 0.7092204670193314,\n",
       "   'subsample': 0.9752444932102324}}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbBO.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.159582609052569, 8.17004539342175, 8.171562888283582, 8.16914417451538, 8.17904165690679, 8.156273768283857, 8.150212234927787, 8.149067543075125, 8.155718457103376, 8.160374675411536, 8.145628680763515, 8.153547272594029, 8.161748633056996, 8.149915172081283, 8.149286865270724, 8.16491111490235, 8.175468831394106, 8.166836529387334, 8.170130246897218, 8.167038841242782, 8.147513803212206, 8.13894101538311, 8.154683374404343, 8.164748085806893, 8.152783862594912, 8.151197862871083, 8.144653897217102, 8.136670067856032, 8.15429017011101, 8.157789482256456]\n",
      "maximum target index: 27\n"
     ]
    }
   ],
   "source": [
    "# dictionary에 있는 target값을 모두 추출\n",
    "target_list = []\n",
    "for result in lgbBO.res:\n",
    "    target = result['target']\n",
    "    target_list.append(target)\n",
    "print(target_list)\n",
    "# 가장 큰 target 값을 가지는 순번(index)를 추출\n",
    "print('maximum target index:', np.argmin(np.array(target_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': 8.136670067856032, 'params': {'colsample_bytree': 0.5543446579385094, 'max_bin': 266.60902033120465, 'max_depth': 12.646496395874314, 'min_child_samples': 147.43609706902322, 'min_child_weight': 13.197385976374632, 'num_leaves': 45.91016035048342, 'reg_alpha': 23.860024134242312, 'reg_lambda': 8.46904710281589, 'subsample': 0.9725842699472822}}\n"
     ]
    }
   ],
   "source": [
    "# 가장 큰 target값을 가지는 index값을 기준으로 res에서 해당 parameter 추출. \n",
    "max_dict = lgbBO.res[np.argmin(np.array(target_list))]\n",
    "print(max_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = DataFrame(X_new)\n",
    "y_train = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv', encoding='cp949').age\n",
    "target = y_train\n",
    "target_log = np.log1p(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_apps_all_with_oof(ftr, target, nfolds=5):\n",
    "    ftr = ftr\n",
    "    target = target\n",
    "\n",
    "    # nfolds 개의 cross validatin fold set을 가지는 KFold 생성 \n",
    "    folds = KFold(n_splits=nfolds, shuffle=True, random_state=0)\n",
    "    \n",
    "    # Out of Folds로 학습된 모델의 validation set을 예측하여 결과 확률을 담을 array 생성.\n",
    "    # validation set가 n_split갯수만큼 있으므로 크기는 ftr_app의 크기가 되어야 함. \n",
    "    oof_preds = np.zeros((ftr.shape[0],))  \n",
    "    \n",
    "    # Ouf of Folds로 학습된 모델의 test dataset을 예측하여 결과 확률을 담을 array 생성. \n",
    "    test_preds = np.zeros(((X_te_new.shape[0],)))\n",
    "    \n",
    "    # n_estimators를 4000까지 확대. \n",
    "    clf = LGBMRegressor(\n",
    "                nthread=4,\n",
    "                n_estimators=4000,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=13,\n",
    "                num_leaves=37,\n",
    "                colsample_bytree=0.594,\n",
    "                subsample=0.713,\n",
    "                max_bin= 65,\n",
    "                reg_alpha=8.495,\n",
    "                reg_lambda=8.393,\n",
    "                min_child_weight=8,\n",
    "                min_child_samples=102,\n",
    "                silent=-1,\n",
    "                verbose=-1,\n",
    "                )\n",
    "\n",
    "    # nfolds 번 cross validation Iteration 반복하면서 OOF 방식으로 학습 및 테스트 데이터 예측\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(folds.split(ftr)):\n",
    "        print('##### iteration ', fold_idx, ' 시작')\n",
    "        # 학습용 데이터 세트의 인덱스와 검증용 데이터 세트의 인덱스 추출하여 이를 기반으로 학습/검증 데이터 추출\n",
    "        train_x = ftr.iloc[train_idx, :]\n",
    "        train_y = target.iloc[train_idx]\n",
    "        valid_x = ftr.iloc[valid_idx, :]\n",
    "        valid_y = target.iloc[valid_idx]\n",
    "        \n",
    "        # 추출된 학습/검증 데이터 세트로 모델 학습. early_stopping은 200으로 증가. \n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'RMSE', verbose= 200, \n",
    "                early_stopping_rounds= 200)\n",
    "        # 검증 데이터 세트로 예측된 확률 저장. 사용되지는 않음. \n",
    "        #oof_preds[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration_)       \n",
    "        # 학습된 모델로 테스트 데이터 세트에 예측 확률 계산. \n",
    "        # nfolds 번 반복 실행하므로 평균 확률을 구하기 위해 개별 수행시 마다 수행 횟수로 나눈 확률을 추후에 더해서 최종 평균 확률 계산. \n",
    "        test_preds += clf.predict(X_te_new, num_iteration=clf.best_iteration_)/folds.n_splits\n",
    "        \n",
    "        \n",
    "    return clf, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### iteration  0  시작\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 8.00239\ttraining's l2: 64.0382\tvalid_1's rmse: 8.56435\tvalid_1's l2: 73.3482\n",
      "[400]\ttraining's rmse: 7.31095\ttraining's l2: 53.4501\tvalid_1's rmse: 8.34053\tvalid_1's l2: 69.5644\n",
      "[600]\ttraining's rmse: 6.85789\ttraining's l2: 47.0306\tvalid_1's rmse: 8.27741\tvalid_1's l2: 68.5155\n",
      "[800]\ttraining's rmse: 6.49454\ttraining's l2: 42.179\tvalid_1's rmse: 8.24908\tvalid_1's l2: 68.0473\n",
      "[1000]\ttraining's rmse: 6.1771\ttraining's l2: 38.1565\tvalid_1's rmse: 8.2356\tvalid_1's l2: 67.825\n",
      "[1200]\ttraining's rmse: 5.891\ttraining's l2: 34.7038\tvalid_1's rmse: 8.22613\tvalid_1's l2: 67.6693\n",
      "[1400]\ttraining's rmse: 5.6261\ttraining's l2: 31.653\tvalid_1's rmse: 8.2199\tvalid_1's l2: 67.5668\n",
      "[1600]\ttraining's rmse: 5.37861\ttraining's l2: 28.9295\tvalid_1's rmse: 8.21931\tvalid_1's l2: 67.5571\n",
      "[1800]\ttraining's rmse: 5.14773\ttraining's l2: 26.4992\tvalid_1's rmse: 8.2184\tvalid_1's l2: 67.5422\n",
      "[2000]\ttraining's rmse: 4.93252\ttraining's l2: 24.3297\tvalid_1's rmse: 8.21869\tvalid_1's l2: 67.5469\n",
      "Early stopping, best iteration is:\n",
      "[1943]\ttraining's rmse: 4.99237\ttraining's l2: 24.9237\tvalid_1's rmse: 8.21638\tvalid_1's l2: 67.5089\n",
      "##### iteration  1  시작\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 8.04053\ttraining's l2: 64.6502\tvalid_1's rmse: 8.46475\tvalid_1's l2: 71.652\n",
      "[400]\ttraining's rmse: 7.35119\ttraining's l2: 54.0401\tvalid_1's rmse: 8.18565\tvalid_1's l2: 67.0049\n",
      "[600]\ttraining's rmse: 6.90252\ttraining's l2: 47.6448\tvalid_1's rmse: 8.1102\tvalid_1's l2: 65.7753\n",
      "[800]\ttraining's rmse: 6.54258\ttraining's l2: 42.8053\tvalid_1's rmse: 8.07297\tvalid_1's l2: 65.1729\n",
      "[1000]\ttraining's rmse: 6.22801\ttraining's l2: 38.7881\tvalid_1's rmse: 8.05259\tvalid_1's l2: 64.8442\n",
      "[1200]\ttraining's rmse: 5.94257\ttraining's l2: 35.3141\tvalid_1's rmse: 8.04115\tvalid_1's l2: 64.6602\n",
      "[1400]\ttraining's rmse: 5.68078\ttraining's l2: 32.2713\tvalid_1's rmse: 8.03113\tvalid_1's l2: 64.499\n",
      "[1600]\ttraining's rmse: 5.4346\ttraining's l2: 29.5349\tvalid_1's rmse: 8.02507\tvalid_1's l2: 64.4018\n",
      "[1800]\ttraining's rmse: 5.20328\ttraining's l2: 27.0741\tvalid_1's rmse: 8.02008\tvalid_1's l2: 64.3217\n",
      "[2000]\ttraining's rmse: 4.98519\ttraining's l2: 24.8521\tvalid_1's rmse: 8.02135\tvalid_1's l2: 64.3421\n",
      "Early stopping, best iteration is:\n",
      "[1812]\ttraining's rmse: 5.1896\ttraining's l2: 26.932\tvalid_1's rmse: 8.01969\tvalid_1's l2: 64.3154\n",
      "##### iteration  2  시작\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 8.01388\ttraining's l2: 64.2223\tvalid_1's rmse: 8.5216\tvalid_1's l2: 72.6177\n",
      "[400]\ttraining's rmse: 7.31769\ttraining's l2: 53.5485\tvalid_1's rmse: 8.29511\tvalid_1's l2: 68.8089\n",
      "[600]\ttraining's rmse: 6.86453\ttraining's l2: 47.1218\tvalid_1's rmse: 8.23301\tvalid_1's l2: 67.7824\n",
      "[800]\ttraining's rmse: 6.49788\ttraining's l2: 42.2224\tvalid_1's rmse: 8.20774\tvalid_1's l2: 67.367\n",
      "[1000]\ttraining's rmse: 6.18241\ttraining's l2: 38.2222\tvalid_1's rmse: 8.19432\tvalid_1's l2: 67.1469\n",
      "[1200]\ttraining's rmse: 5.89746\ttraining's l2: 34.78\tvalid_1's rmse: 8.18489\tvalid_1's l2: 66.9924\n",
      "[1400]\ttraining's rmse: 5.63253\ttraining's l2: 31.7254\tvalid_1's rmse: 8.18209\tvalid_1's l2: 66.9465\n",
      "Early stopping, best iteration is:\n",
      "[1394]\ttraining's rmse: 5.64032\ttraining's l2: 31.8132\tvalid_1's rmse: 8.18145\tvalid_1's l2: 66.9361\n",
      "##### iteration  3  시작\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 7.99275\ttraining's l2: 63.8841\tvalid_1's rmse: 8.67376\tvalid_1's l2: 75.2341\n",
      "[400]\ttraining's rmse: 7.29746\ttraining's l2: 53.2529\tvalid_1's rmse: 8.39833\tvalid_1's l2: 70.5319\n",
      "[600]\ttraining's rmse: 6.842\ttraining's l2: 46.813\tvalid_1's rmse: 8.32545\tvalid_1's l2: 69.3132\n",
      "[800]\ttraining's rmse: 6.47365\ttraining's l2: 41.9081\tvalid_1's rmse: 8.29138\tvalid_1's l2: 68.747\n",
      "[1000]\ttraining's rmse: 6.15645\ttraining's l2: 37.9019\tvalid_1's rmse: 8.27365\tvalid_1's l2: 68.4533\n",
      "[1200]\ttraining's rmse: 5.86818\ttraining's l2: 34.4356\tvalid_1's rmse: 8.26613\tvalid_1's l2: 68.3289\n",
      "[1400]\ttraining's rmse: 5.60468\ttraining's l2: 31.4124\tvalid_1's rmse: 8.26599\tvalid_1's l2: 68.3266\n",
      "Early stopping, best iteration is:\n",
      "[1251]\ttraining's rmse: 5.79874\ttraining's l2: 33.6254\tvalid_1's rmse: 8.26501\tvalid_1's l2: 68.3103\n",
      "##### iteration  4  시작\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 8.03683\ttraining's l2: 64.5906\tvalid_1's rmse: 8.48552\tvalid_1's l2: 72.004\n",
      "[400]\ttraining's rmse: 7.34224\ttraining's l2: 53.9085\tvalid_1's rmse: 8.21252\tvalid_1's l2: 67.4455\n",
      "[600]\ttraining's rmse: 6.89028\ttraining's l2: 47.476\tvalid_1's rmse: 8.1434\tvalid_1's l2: 66.315\n",
      "[800]\ttraining's rmse: 6.53383\ttraining's l2: 42.691\tvalid_1's rmse: 8.11722\tvalid_1's l2: 65.8893\n",
      "[1000]\ttraining's rmse: 6.22375\ttraining's l2: 38.7351\tvalid_1's rmse: 8.10475\tvalid_1's l2: 65.687\n",
      "[1200]\ttraining's rmse: 5.93747\ttraining's l2: 35.2535\tvalid_1's rmse: 8.09785\tvalid_1's l2: 65.5751\n",
      "[1400]\ttraining's rmse: 5.67138\ttraining's l2: 32.1645\tvalid_1's rmse: 8.09291\tvalid_1's l2: 65.4951\n",
      "[1600]\ttraining's rmse: 5.42554\ttraining's l2: 29.4365\tvalid_1's rmse: 8.09129\tvalid_1's l2: 65.4689\n",
      "Early stopping, best iteration is:\n",
      "[1539]\ttraining's rmse: 5.49877\ttraining's l2: 30.2364\tvalid_1's rmse: 8.08995\tvalid_1's l2: 65.4472\n"
     ]
    }
   ],
   "source": [
    "clf, test_preds = train_apps_all_with_oof(ftr, target, nfolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37.94921532, 42.88769799, 27.90362666, ..., 37.82912055,\n",
       "       33.07105315, 26.29963301])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDtest['age'] = test_preds ; sub = IDtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submissions_0613_jin_ha_scaled_lgbm_tun.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
