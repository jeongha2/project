{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "from matplotlib import font_manager, rc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "import matplotlib.font_manager as fm\n",
    "plt.rc('font', family=fm.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()) # for Windows OS user\n",
    "import math\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import klib\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sys, warnings\n",
    "if not sys.warnoptions: warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import ClassifierMixin\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import platform\n",
    "from itertools import combinations\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "\n",
    "import datetime\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_train = pd.read_csv(os.path.abspath(\"../input\")+'/X_train.csv', encoding='cp949')\n",
    "df_x_test = pd.read_csv(os.path.abspath(\"../input\")+'/X_test.csv', encoding='cp949')\n",
    "df_y_train = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv').age\n",
    "df_y_train_mer = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv')\n",
    "y_train = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv', encoding='cp949')\n",
    "IDtrain = pd.DataFrame({'custid': df_x_train.custid.unique()})\n",
    "IDtest = pd.DataFrame({'custid': df_x_test.custid.unique()})\n",
    "\n",
    "# df = pd.concat([df_x_train, df_x_test])\n",
    "\n",
    "eda_df = pd.merge(df_x_train , df_y_train_mer, on = 'custid')\n",
    "\n",
    "ALL = pd.read_csv(os.path.abspath(\"../input\")+'/ALL피처추가060601.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_vector = pd.read_csv(os.path.abspath(\"../input\")+'/train_mean_vector.csv')\n",
    "test_mean_vector = pd.read_csv(os.path.abspath(\"../input\")+'/test_mean_vector.csv')\n",
    "\n",
    "train_mean_vector1 = pd.read_csv(os.path.abspath(\"../input\")+'/train_mean_vector1.csv')\n",
    "test_mean_vector1 = pd.read_csv(os.path.abspath(\"../input\")+'/test_mean_vector1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.abspath(\"../input\")+'/지평오빠feature.csv')\n",
    "dft = pd.read_csv(os.path.abspath(\"../input\")+'/지평오빠feature_te.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.dtypes[df.dtypes != 'object'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-skew에만 처리를 하기 위해서 numeric_columns에서 skew와 non-skew로 구분\n",
    "skewlist = []\n",
    "numeric_columns_ns = []\n",
    "\n",
    "for i in numeric_columns:\n",
    "    if 'SKEW' in i:\n",
    "        skewlist.append(i)\n",
    "    else:\n",
    "        numeric_columns_ns.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-skew standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "dft[numeric_columns] = scaler.transform(dft[numeric_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainid = IDtrain.custid.to_list()\n",
    "X_train = ALL.query('custid in @trainid')\n",
    "\n",
    "testid = IDtest.custid.to_list()\n",
    "X_test = ALL.query('custid in @testid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('custid', axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop('custid', axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns2 = X_train.dtypes[X_train.dtypes != 'object'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-skew에만 처리를 하기 위해서 numeric_columns에서 skew와 non-skew로 구분\n",
    "skewlist2 = []\n",
    "numeric_columns_ns2 = []\n",
    "\n",
    "for i in numeric_columns2:\n",
    "    if 'SKEW' in i:\n",
    "        skewlist2.append(i)\n",
    "    else:\n",
    "        numeric_columns_ns2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-skew standard scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train[numeric_columns2] = scaler.fit_transform(X_train[numeric_columns2])\n",
    "X_test[numeric_columns2] = scaler.transform(X_test[numeric_columns2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df[numeric_columns_ns], df[skewlist], train_mean_vector , X_train[numeric_columns_ns2], X_train[skewlist2]], axis=1)\n",
    "\n",
    "test_mean_vector.index = dft.index\n",
    "X_test.index = dft.index\n",
    "data_te = pd.concat([dft[numeric_columns_ns], dft[skewlist], test_mean_vector ,  X_test[numeric_columns_ns2], X_test[skewlist2]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean_vector1.columns = train_mean_vector1.columns.astype(str) + \"_time\"\n",
    "data = pd.concat([data, train_mean_vector1], axis=1)\n",
    "\n",
    "test_mean_vector1.index = data_te.index\n",
    "test_mean_vector1.columns = test_mean_vector1.columns.astype(str) + \"_time\"\n",
    "data_te = pd.concat([data_te, test_mean_vector1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custid</th>\n",
       "      <th>CUSTID_COUNT</th>\n",
       "      <th>SALES_TIME_MIN</th>\n",
       "      <th>SALES_TIME_MAX</th>\n",
       "      <th>SALES_TIME_STD</th>\n",
       "      <th>GOODCD_NUNIQUE</th>\n",
       "      <th>IMPORT_FLG_MEAN</th>\n",
       "      <th>IMPORT_FLG_SUM</th>\n",
       "      <th>IMPORT_FLG_NUNIQUE</th>\n",
       "      <th>TOT_AMT_MIN</th>\n",
       "      <th>...</th>\n",
       "      <th>90_time</th>\n",
       "      <th>91_time</th>\n",
       "      <th>92_time</th>\n",
       "      <th>93_time</th>\n",
       "      <th>94_time</th>\n",
       "      <th>95_time</th>\n",
       "      <th>96_time</th>\n",
       "      <th>97_time</th>\n",
       "      <th>98_time</th>\n",
       "      <th>99_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.732723</td>\n",
       "      <td>-0.539868</td>\n",
       "      <td>-0.129259</td>\n",
       "      <td>0.522979</td>\n",
       "      <td>0.984931</td>\n",
       "      <td>-0.642232</td>\n",
       "      <td>2.224346</td>\n",
       "      <td>0.501115</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>0.321857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256365</td>\n",
       "      <td>0.250052</td>\n",
       "      <td>-0.588806</td>\n",
       "      <td>-0.406187</td>\n",
       "      <td>0.439367</td>\n",
       "      <td>0.185279</td>\n",
       "      <td>0.181373</td>\n",
       "      <td>0.685128</td>\n",
       "      <td>0.177597</td>\n",
       "      <td>-0.129340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.732492</td>\n",
       "      <td>-0.539868</td>\n",
       "      <td>-0.950020</td>\n",
       "      <td>0.455549</td>\n",
       "      <td>2.119481</td>\n",
       "      <td>-0.642232</td>\n",
       "      <td>-0.466244</td>\n",
       "      <td>-0.488931</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>-1.662230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042530</td>\n",
       "      <td>-0.419971</td>\n",
       "      <td>-0.239485</td>\n",
       "      <td>0.733378</td>\n",
       "      <td>-0.301544</td>\n",
       "      <td>0.995978</td>\n",
       "      <td>-0.650930</td>\n",
       "      <td>-0.216006</td>\n",
       "      <td>0.197088</td>\n",
       "      <td>0.797142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.732376</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>-0.630835</td>\n",
       "      <td>0.610638</td>\n",
       "      <td>1.087506</td>\n",
       "      <td>0.354036</td>\n",
       "      <td>-0.914675</td>\n",
       "      <td>-0.653938</td>\n",
       "      <td>-2.104596</td>\n",
       "      <td>-0.578141</td>\n",
       "      <td>...</td>\n",
       "      <td>1.116125</td>\n",
       "      <td>-0.557568</td>\n",
       "      <td>-0.064265</td>\n",
       "      <td>0.548341</td>\n",
       "      <td>1.124598</td>\n",
       "      <td>0.094009</td>\n",
       "      <td>1.270353</td>\n",
       "      <td>-0.477902</td>\n",
       "      <td>-0.803664</td>\n",
       "      <td>0.894444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.732261</td>\n",
       "      <td>-0.750325</td>\n",
       "      <td>0.838428</td>\n",
       "      <td>-2.713662</td>\n",
       "      <td>-2.247293</td>\n",
       "      <td>-0.841486</td>\n",
       "      <td>0.318512</td>\n",
       "      <td>-0.488931</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>0.335493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111864</td>\n",
       "      <td>-0.018073</td>\n",
       "      <td>0.509619</td>\n",
       "      <td>0.201731</td>\n",
       "      <td>-1.008987</td>\n",
       "      <td>0.289059</td>\n",
       "      <td>-0.123257</td>\n",
       "      <td>-1.076202</td>\n",
       "      <td>0.199374</td>\n",
       "      <td>0.672205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.732145</td>\n",
       "      <td>0.091504</td>\n",
       "      <td>-0.696699</td>\n",
       "      <td>0.401605</td>\n",
       "      <td>1.616230</td>\n",
       "      <td>0.354036</td>\n",
       "      <td>0.010215</td>\n",
       "      <td>0.336107</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>0.296404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.496266</td>\n",
       "      <td>0.330754</td>\n",
       "      <td>-0.703236</td>\n",
       "      <td>-0.781866</td>\n",
       "      <td>0.165355</td>\n",
       "      <td>0.084495</td>\n",
       "      <td>0.354446</td>\n",
       "      <td>0.697256</td>\n",
       "      <td>-0.053173</td>\n",
       "      <td>-0.477805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21582</th>\n",
       "      <td>1.732428</td>\n",
       "      <td>1.414377</td>\n",
       "      <td>-1.086814</td>\n",
       "      <td>0.408348</td>\n",
       "      <td>0.531171</td>\n",
       "      <td>2.678662</td>\n",
       "      <td>-0.395439</td>\n",
       "      <td>0.666123</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>-1.375867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.410266</td>\n",
       "      <td>0.409000</td>\n",
       "      <td>-0.294169</td>\n",
       "      <td>-0.414884</td>\n",
       "      <td>-0.443909</td>\n",
       "      <td>0.351057</td>\n",
       "      <td>-0.240866</td>\n",
       "      <td>0.028628</td>\n",
       "      <td>-0.047315</td>\n",
       "      <td>0.323812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>1.732544</td>\n",
       "      <td>-0.299345</td>\n",
       "      <td>0.012601</td>\n",
       "      <td>0.401605</td>\n",
       "      <td>1.011767</td>\n",
       "      <td>-0.509397</td>\n",
       "      <td>-0.395439</td>\n",
       "      <td>-0.323923</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>-0.010870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549509</td>\n",
       "      <td>0.009478</td>\n",
       "      <td>0.140891</td>\n",
       "      <td>0.843434</td>\n",
       "      <td>-0.524057</td>\n",
       "      <td>0.407387</td>\n",
       "      <td>-1.173958</td>\n",
       "      <td>-0.395567</td>\n",
       "      <td>0.306181</td>\n",
       "      <td>0.958077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>1.732659</td>\n",
       "      <td>-0.269280</td>\n",
       "      <td>0.063265</td>\n",
       "      <td>-0.825621</td>\n",
       "      <td>-0.765582</td>\n",
       "      <td>-0.243725</td>\n",
       "      <td>-0.668038</td>\n",
       "      <td>-0.488931</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>0.280948</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.485857</td>\n",
       "      <td>0.605453</td>\n",
       "      <td>-0.245641</td>\n",
       "      <td>-0.241068</td>\n",
       "      <td>-0.327827</td>\n",
       "      <td>0.197681</td>\n",
       "      <td>0.248651</td>\n",
       "      <td>-0.296740</td>\n",
       "      <td>-0.351735</td>\n",
       "      <td>-0.426079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>1.732775</td>\n",
       "      <td>-0.479737</td>\n",
       "      <td>0.331786</td>\n",
       "      <td>0.340918</td>\n",
       "      <td>-0.124821</td>\n",
       "      <td>-0.310143</td>\n",
       "      <td>-0.914675</td>\n",
       "      <td>-0.653938</td>\n",
       "      <td>-2.104596</td>\n",
       "      <td>0.193675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052067</td>\n",
       "      <td>0.453997</td>\n",
       "      <td>0.609971</td>\n",
       "      <td>0.394561</td>\n",
       "      <td>-0.917944</td>\n",
       "      <td>0.692990</td>\n",
       "      <td>-1.018453</td>\n",
       "      <td>-0.385621</td>\n",
       "      <td>-0.163522</td>\n",
       "      <td>0.667704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>1.732890</td>\n",
       "      <td>-0.780390</td>\n",
       "      <td>0.382450</td>\n",
       "      <td>-0.198522</td>\n",
       "      <td>1.280078</td>\n",
       "      <td>-0.907904</td>\n",
       "      <td>0.729574</td>\n",
       "      <td>-0.488931</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>0.331402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479082</td>\n",
       "      <td>-1.350079</td>\n",
       "      <td>0.031222</td>\n",
       "      <td>1.166514</td>\n",
       "      <td>0.970791</td>\n",
       "      <td>-0.375154</td>\n",
       "      <td>1.578779</td>\n",
       "      <td>0.086713</td>\n",
       "      <td>-1.556864</td>\n",
       "      <td>0.928450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21587 rows × 20391 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         custid  CUSTID_COUNT  SALES_TIME_MIN  SALES_TIME_MAX  SALES_TIME_STD  \\\n",
       "0     -1.732723     -0.539868       -0.129259        0.522979        0.984931   \n",
       "1     -1.732492     -0.539868       -0.950020        0.455549        2.119481   \n",
       "2     -1.732376      0.031373       -0.630835        0.610638        1.087506   \n",
       "3     -1.732261     -0.750325        0.838428       -2.713662       -2.247293   \n",
       "4     -1.732145      0.091504       -0.696699        0.401605        1.616230   \n",
       "...         ...           ...             ...             ...             ...   \n",
       "21582  1.732428      1.414377       -1.086814        0.408348        0.531171   \n",
       "21583  1.732544     -0.299345        0.012601        0.401605        1.011767   \n",
       "21584  1.732659     -0.269280        0.063265       -0.825621       -0.765582   \n",
       "21585  1.732775     -0.479737        0.331786        0.340918       -0.124821   \n",
       "21586  1.732890     -0.780390        0.382450       -0.198522        1.280078   \n",
       "\n",
       "       GOODCD_NUNIQUE  IMPORT_FLG_MEAN  IMPORT_FLG_SUM  IMPORT_FLG_NUNIQUE  \\\n",
       "0           -0.642232         2.224346        0.501115            0.475150   \n",
       "1           -0.642232        -0.466244       -0.488931            0.475150   \n",
       "2            0.354036        -0.914675       -0.653938           -2.104596   \n",
       "3           -0.841486         0.318512       -0.488931            0.475150   \n",
       "4            0.354036         0.010215        0.336107            0.475150   \n",
       "...               ...              ...             ...                 ...   \n",
       "21582        2.678662        -0.395439        0.666123            0.475150   \n",
       "21583       -0.509397        -0.395439       -0.323923            0.475150   \n",
       "21584       -0.243725        -0.668038       -0.488931            0.475150   \n",
       "21585       -0.310143        -0.914675       -0.653938           -2.104596   \n",
       "21586       -0.907904         0.729574       -0.488931            0.475150   \n",
       "\n",
       "       TOT_AMT_MIN  ...   90_time   91_time   92_time   93_time   94_time  \\\n",
       "0         0.321857  ...  0.256365  0.250052 -0.588806 -0.406187  0.439367   \n",
       "1        -1.662230  ...  0.042530 -0.419971 -0.239485  0.733378 -0.301544   \n",
       "2        -0.578141  ...  1.116125 -0.557568 -0.064265  0.548341  1.124598   \n",
       "3         0.335493  ... -0.111864 -0.018073  0.509619  0.201731 -1.008987   \n",
       "4         0.296404  ... -0.496266  0.330754 -0.703236 -0.781866  0.165355   \n",
       "...            ...  ...       ...       ...       ...       ...       ...   \n",
       "21582    -1.375867  ... -0.410266  0.409000 -0.294169 -0.414884 -0.443909   \n",
       "21583    -0.010870  ...  0.549509  0.009478  0.140891  0.843434 -0.524057   \n",
       "21584     0.280948  ... -0.485857  0.605453 -0.245641 -0.241068 -0.327827   \n",
       "21585     0.193675  ...  0.052067  0.453997  0.609971  0.394561 -0.917944   \n",
       "21586     0.331402  ...  0.479082 -1.350079  0.031222  1.166514  0.970791   \n",
       "\n",
       "        95_time   96_time   97_time   98_time   99_time  \n",
       "0      0.185279  0.181373  0.685128  0.177597 -0.129340  \n",
       "1      0.995978 -0.650930 -0.216006  0.197088  0.797142  \n",
       "2      0.094009  1.270353 -0.477902 -0.803664  0.894444  \n",
       "3      0.289059 -0.123257 -1.076202  0.199374  0.672205  \n",
       "4      0.084495  0.354446  0.697256 -0.053173 -0.477805  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "21582  0.351057 -0.240866  0.028628 -0.047315  0.323812  \n",
       "21583  0.407387 -1.173958 -0.395567  0.306181  0.958077  \n",
       "21584  0.197681  0.248651 -0.296740 -0.351735 -0.426079  \n",
       "21585  0.692990 -1.018453 -0.385621 -0.163522  0.667704  \n",
       "21586 -0.375154  1.578779  0.086713 -1.556864  0.928450  \n",
       "\n",
       "[21587 rows x 20391 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>custid</th>\n",
       "      <th>CUSTID_COUNT</th>\n",
       "      <th>SALES_TIME_MIN</th>\n",
       "      <th>SALES_TIME_MAX</th>\n",
       "      <th>SALES_TIME_STD</th>\n",
       "      <th>GOODCD_NUNIQUE</th>\n",
       "      <th>IMPORT_FLG_MEAN</th>\n",
       "      <th>IMPORT_FLG_SUM</th>\n",
       "      <th>IMPORT_FLG_NUNIQUE</th>\n",
       "      <th>TOT_AMT_MIN</th>\n",
       "      <th>...</th>\n",
       "      <th>90_time</th>\n",
       "      <th>91_time</th>\n",
       "      <th>92_time</th>\n",
       "      <th>93_time</th>\n",
       "      <th>94_time</th>\n",
       "      <th>95_time</th>\n",
       "      <th>96_time</th>\n",
       "      <th>97_time</th>\n",
       "      <th>98_time</th>\n",
       "      <th>99_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.733121</td>\n",
       "      <td>-0.058823</td>\n",
       "      <td>2.054371</td>\n",
       "      <td>0.388119</td>\n",
       "      <td>-1.644922</td>\n",
       "      <td>0.221200</td>\n",
       "      <td>-0.549286</td>\n",
       "      <td>-0.323923</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>0.180448</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.729876</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>-0.534627</td>\n",
       "      <td>-0.790608</td>\n",
       "      <td>0.550694</td>\n",
       "      <td>-0.203051</td>\n",
       "      <td>0.623015</td>\n",
       "      <td>0.889328</td>\n",
       "      <td>-0.021200</td>\n",
       "      <td>-0.047212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.733237</td>\n",
       "      <td>2.196075</td>\n",
       "      <td>-0.950020</td>\n",
       "      <td>0.522979</td>\n",
       "      <td>0.717249</td>\n",
       "      <td>2.080901</td>\n",
       "      <td>0.294331</td>\n",
       "      <td>3.471252</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>-0.290415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.139067</td>\n",
       "      <td>0.302104</td>\n",
       "      <td>-0.191570</td>\n",
       "      <td>-0.283908</td>\n",
       "      <td>-0.355157</td>\n",
       "      <td>0.406006</td>\n",
       "      <td>-0.136448</td>\n",
       "      <td>0.012418</td>\n",
       "      <td>0.354670</td>\n",
       "      <td>0.022581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.733352</td>\n",
       "      <td>0.542483</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.522979</td>\n",
       "      <td>0.430884</td>\n",
       "      <td>0.752543</td>\n",
       "      <td>-0.389915</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>0.080493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.170458</td>\n",
       "      <td>0.789328</td>\n",
       "      <td>0.787921</td>\n",
       "      <td>0.150189</td>\n",
       "      <td>-0.091482</td>\n",
       "      <td>-0.284138</td>\n",
       "      <td>-0.354679</td>\n",
       "      <td>-0.451833</td>\n",
       "      <td>0.404842</td>\n",
       "      <td>-0.853814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.733583</td>\n",
       "      <td>-0.840521</td>\n",
       "      <td>3.371641</td>\n",
       "      <td>0.340918</td>\n",
       "      <td>-2.525086</td>\n",
       "      <td>-1.040740</td>\n",
       "      <td>4.018072</td>\n",
       "      <td>-0.488931</td>\n",
       "      <td>-2.104596</td>\n",
       "      <td>0.346402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243847</td>\n",
       "      <td>1.097241</td>\n",
       "      <td>1.557096</td>\n",
       "      <td>-0.272380</td>\n",
       "      <td>0.635346</td>\n",
       "      <td>-0.351574</td>\n",
       "      <td>-1.010821</td>\n",
       "      <td>1.454252</td>\n",
       "      <td>1.535311</td>\n",
       "      <td>-0.746919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.733814</td>\n",
       "      <td>-0.690194</td>\n",
       "      <td>1.532529</td>\n",
       "      <td>0.543208</td>\n",
       "      <td>-0.429491</td>\n",
       "      <td>-0.708650</td>\n",
       "      <td>-0.092551</td>\n",
       "      <td>-0.488931</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>0.308220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051304</td>\n",
       "      <td>0.285071</td>\n",
       "      <td>1.738491</td>\n",
       "      <td>0.670187</td>\n",
       "      <td>0.097054</td>\n",
       "      <td>0.247967</td>\n",
       "      <td>-1.045783</td>\n",
       "      <td>-0.738167</td>\n",
       "      <td>0.132681</td>\n",
       "      <td>0.090545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14375</th>\n",
       "      <td>4.042105</td>\n",
       "      <td>-0.750325</td>\n",
       "      <td>-0.088728</td>\n",
       "      <td>-0.353611</td>\n",
       "      <td>1.230923</td>\n",
       "      <td>-0.841486</td>\n",
       "      <td>0.318512</td>\n",
       "      <td>-0.488931</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>0.339584</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.197278</td>\n",
       "      <td>0.738715</td>\n",
       "      <td>-1.700414</td>\n",
       "      <td>-1.095568</td>\n",
       "      <td>1.133009</td>\n",
       "      <td>-0.163281</td>\n",
       "      <td>0.482896</td>\n",
       "      <td>-0.430197</td>\n",
       "      <td>0.405807</td>\n",
       "      <td>-0.668299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14376</th>\n",
       "      <td>4.042336</td>\n",
       "      <td>-0.840521</td>\n",
       "      <td>1.345071</td>\n",
       "      <td>-2.356283</td>\n",
       "      <td>-2.525086</td>\n",
       "      <td>-1.040740</td>\n",
       "      <td>4.018072</td>\n",
       "      <td>-0.488931</td>\n",
       "      <td>-2.104596</td>\n",
       "      <td>0.559129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.216434</td>\n",
       "      <td>0.823960</td>\n",
       "      <td>0.246141</td>\n",
       "      <td>1.341821</td>\n",
       "      <td>-1.019822</td>\n",
       "      <td>0.616578</td>\n",
       "      <td>-0.526211</td>\n",
       "      <td>-0.412237</td>\n",
       "      <td>-0.534695</td>\n",
       "      <td>0.445136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14377</th>\n",
       "      <td>4.042567</td>\n",
       "      <td>-0.810456</td>\n",
       "      <td>2.545814</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>-1.472477</td>\n",
       "      <td>-0.974322</td>\n",
       "      <td>1.551698</td>\n",
       "      <td>-0.488931</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>0.340947</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.634492</td>\n",
       "      <td>0.613673</td>\n",
       "      <td>-1.001559</td>\n",
       "      <td>-1.402124</td>\n",
       "      <td>0.456310</td>\n",
       "      <td>-0.343922</td>\n",
       "      <td>1.446845</td>\n",
       "      <td>0.932912</td>\n",
       "      <td>-0.158417</td>\n",
       "      <td>-0.456060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14378</th>\n",
       "      <td>4.042683</td>\n",
       "      <td>-0.750325</td>\n",
       "      <td>1.988507</td>\n",
       "      <td>-0.131092</td>\n",
       "      <td>-1.412283</td>\n",
       "      <td>-0.907904</td>\n",
       "      <td>-0.914675</td>\n",
       "      <td>-0.653938</td>\n",
       "      <td>-2.104596</td>\n",
       "      <td>0.295948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175128</td>\n",
       "      <td>-0.184848</td>\n",
       "      <td>0.750435</td>\n",
       "      <td>-0.397384</td>\n",
       "      <td>0.299535</td>\n",
       "      <td>0.286670</td>\n",
       "      <td>-0.795192</td>\n",
       "      <td>0.655654</td>\n",
       "      <td>0.591079</td>\n",
       "      <td>-1.472640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14379</th>\n",
       "      <td>4.042798</td>\n",
       "      <td>-0.780390</td>\n",
       "      <td>1.846647</td>\n",
       "      <td>-0.286181</td>\n",
       "      <td>-1.114755</td>\n",
       "      <td>-0.907904</td>\n",
       "      <td>0.729574</td>\n",
       "      <td>-0.488931</td>\n",
       "      <td>0.475150</td>\n",
       "      <td>0.315311</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.597444</td>\n",
       "      <td>1.010484</td>\n",
       "      <td>-0.076435</td>\n",
       "      <td>-0.204458</td>\n",
       "      <td>0.172927</td>\n",
       "      <td>0.440275</td>\n",
       "      <td>-0.100321</td>\n",
       "      <td>0.255401</td>\n",
       "      <td>-0.312869</td>\n",
       "      <td>-0.391766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14380 rows × 20391 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         custid  CUSTID_COUNT  SALES_TIME_MIN  SALES_TIME_MAX  SALES_TIME_STD  \\\n",
       "0      1.733121     -0.058823        2.054371        0.388119       -1.644922   \n",
       "1      1.733237      2.196075       -0.950020        0.522979        0.717249   \n",
       "2      1.733352      0.542483        0.027800        0.522979        0.430884   \n",
       "3      1.733583     -0.840521        3.371641        0.340918       -2.525086   \n",
       "4      1.733814     -0.690194        1.532529        0.543208       -0.429491   \n",
       "...         ...           ...             ...             ...             ...   \n",
       "14375  4.042105     -0.750325       -0.088728       -0.353611        1.230923   \n",
       "14376  4.042336     -0.840521        1.345071       -2.356283       -2.525086   \n",
       "14377  4.042567     -0.810456        2.545814        0.003768       -1.472477   \n",
       "14378  4.042683     -0.750325        1.988507       -0.131092       -1.412283   \n",
       "14379  4.042798     -0.780390        1.846647       -0.286181       -1.114755   \n",
       "\n",
       "       GOODCD_NUNIQUE  IMPORT_FLG_MEAN  IMPORT_FLG_SUM  IMPORT_FLG_NUNIQUE  \\\n",
       "0            0.221200        -0.549286       -0.323923            0.475150   \n",
       "1            2.080901         0.294331        3.471252            0.475150   \n",
       "2            0.752543        -0.389915        0.171100            0.475150   \n",
       "3           -1.040740         4.018072       -0.488931           -2.104596   \n",
       "4           -0.708650        -0.092551       -0.488931            0.475150   \n",
       "...               ...              ...             ...                 ...   \n",
       "14375       -0.841486         0.318512       -0.488931            0.475150   \n",
       "14376       -1.040740         4.018072       -0.488931           -2.104596   \n",
       "14377       -0.974322         1.551698       -0.488931            0.475150   \n",
       "14378       -0.907904        -0.914675       -0.653938           -2.104596   \n",
       "14379       -0.907904         0.729574       -0.488931            0.475150   \n",
       "\n",
       "       TOT_AMT_MIN  ...   90_time   91_time   92_time   93_time   94_time  \\\n",
       "0         0.180448  ... -0.729876  0.690600 -0.534627 -0.790608  0.550694   \n",
       "1        -0.290415  ... -0.139067  0.302104 -0.191570 -0.283908 -0.355157   \n",
       "2         0.080493  ... -0.170458  0.789328  0.787921  0.150189 -0.091482   \n",
       "3         0.346402  ...  0.243847  1.097241  1.557096 -0.272380  0.635346   \n",
       "4         0.308220  ...  0.051304  0.285071  1.738491  0.670187  0.097054   \n",
       "...            ...  ...       ...       ...       ...       ...       ...   \n",
       "14375     0.339584  ... -0.197278  0.738715 -1.700414 -1.095568  1.133009   \n",
       "14376     0.559129  ...  0.216434  0.823960  0.246141  1.341821 -1.019822   \n",
       "14377     0.340947  ... -1.634492  0.613673 -1.001559 -1.402124  0.456310   \n",
       "14378     0.295948  ...  0.175128 -0.184848  0.750435 -0.397384  0.299535   \n",
       "14379     0.315311  ... -0.597444  1.010484 -0.076435 -0.204458  0.172927   \n",
       "\n",
       "        95_time   96_time   97_time   98_time   99_time  \n",
       "0     -0.203051  0.623015  0.889328 -0.021200 -0.047212  \n",
       "1      0.406006 -0.136448  0.012418  0.354670  0.022581  \n",
       "2     -0.284138 -0.354679 -0.451833  0.404842 -0.853814  \n",
       "3     -0.351574 -1.010821  1.454252  1.535311 -0.746919  \n",
       "4      0.247967 -1.045783 -0.738167  0.132681  0.090545  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "14375 -0.163281  0.482896 -0.430197  0.405807 -0.668299  \n",
       "14376  0.616578 -0.526211 -0.412237 -0.534695  0.445136  \n",
       "14377 -0.343922  1.446845  0.932912 -0.158417 -0.456060  \n",
       "14378  0.286670 -0.795192  0.655654  0.591079 -1.472640  \n",
       "14379  0.440275 -0.100321  0.255401 -0.312869 -0.391766  \n",
       "\n",
       "[14380 rows x 20391 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display( data, data_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21587, 20391), (14380, 20391), (21587,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = pd.read_csv(os.path.abspath(os.path.abspath(\"../input\")+'/y_train.csv', encoding='cp949').age\n",
    "data.shape, data_te.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = data.columns.astype(str)\n",
    "data_te.columns = data_te.columns.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# data = data.rename(columns = lambda x:re.sub('[^A-Za-z0-9]+', ' ', x))\n",
    "# data_te = data_te.rename(columns = lambda x:re.sub('[^A-Za-z0-9]+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.values\n",
    "data_te2 = data_te.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = data2[:, 1:]\n",
    "target = y_train\n",
    "target_log = np.log1p(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17269, 20390), (4318, 20390))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(ftr, target, test_size=0.2 , stratify = target , random_state=1000)\n",
    "train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttraining's rmse: 7.80921\ttraining's l2: 60.9838\tvalid_1's rmse: 8.30124\tvalid_1's l2: 68.9106\n",
      "[200]\ttraining's rmse: 7.07975\ttraining's l2: 50.1229\tvalid_1's rmse: 8.07777\tvalid_1's l2: 65.2504\n",
      "[300]\ttraining's rmse: 6.5901\ttraining's l2: 43.4295\tvalid_1's rmse: 8.03189\tvalid_1's l2: 64.5112\n",
      "[400]\ttraining's rmse: 6.20279\ttraining's l2: 38.4746\tvalid_1's rmse: 8.0224\tvalid_1's l2: 64.3589\n",
      "Early stopping, best iteration is:\n",
      "[434]\ttraining's rmse: 6.08606\ttraining's l2: 37.0401\tvalid_1's rmse: 8.02037\tvalid_1's l2: 64.3263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(learning_rate=0.02, max_depth=12, n_estimators=1000,\n",
       "              num_leaves=32, silent=-1, subsample=0.8, verbose=-1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "clf = LGBMRegressor(\n",
    "        n_jobs=-1,\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.02,\n",
    "        num_leaves=32,\n",
    "        subsample=0.8,\n",
    "        max_depth=12,\n",
    "        silent=-1,\n",
    "        verbose=-1\n",
    "        )\n",
    "\n",
    "clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric = 'RMSE', \n",
    "        verbose=100, early_stopping_rounds= 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection & Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = data2[:, 1:]\n",
    "target = y_train\n",
    "target_log = np.log1p(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "smf = SelectFromModel(clf, threshold='7.0*mean')\n",
    "smf.fit(ftr, target)\n",
    "X_new = smf.transform(ftr)\n",
    "X_te_new = smf.transform(data_te2[:, 1:])\n",
    "feature_idx = smf.get_support()\n",
    "#feature_name = ftr.columns[feature_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차 검증별 정확도: [-63.3759 -66.1126 -65.3234 -61.516  -62.5808]\n",
      "평균 검증 정확도: -63.7818\n",
      "RMSE: 7.986347806899532\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(clf, X_new, target, scoring='neg_mean_squared_error', cv=5)\n",
    "print('교차 검증별 정확도:', np.round(scores, 4))\n",
    "print('평균 검증 정확도:', np.round(np.mean(scores), 4))\n",
    "print('RMSE:', np.sqrt(-np.mean(scores)))\n",
    "# mean -\n",
    "# 2.0mean - \n",
    "# 3.0mean - \n",
    "# 3.5mean -       \n",
    "# 4.0mean - \n",
    "# 4.5mean - \n",
    "# 5.0mean - 7.986655255475062\n",
    "# 6.0mean - \n",
    "# 6.5mean - \n",
    "# 7.0mean - 7.986347806899532\n",
    "# 8.0mean - \n",
    "# 9.0mean - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21587, 833), (14380, 833))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape, X_te_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(X_new).to_csv('zi_ha_sel_feature.csv', index = False )\n",
    "# pd.DataFrame(X_te_new).to_csv('zi_ha_sel_feature_te.csv', index = False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper parameter Tuning_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = X_new\n",
    "y_train = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv', encoding='cp949').age\n",
    "target = y_train\n",
    "target_log = np.log1p(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21587,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15110, 833), (6477, 833))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(ftr, target, test_size=0.3, stratify = target,  random_state=1000)\n",
    "train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.063286\n",
      "0:\tlearn: 10.2722687\ttotal: 281ms\tremaining: 4m 40s\n",
      "1:\tlearn: 10.1243738\ttotal: 394ms\tremaining: 3m 16s\n",
      "2:\tlearn: 9.9847729\ttotal: 508ms\tremaining: 2m 48s\n",
      "3:\tlearn: 9.8494674\ttotal: 628ms\tremaining: 2m 36s\n",
      "4:\tlearn: 9.7277755\ttotal: 751ms\tremaining: 2m 29s\n",
      "5:\tlearn: 9.6168876\ttotal: 865ms\tremaining: 2m 23s\n",
      "6:\tlearn: 9.5190837\ttotal: 971ms\tremaining: 2m 17s\n",
      "7:\tlearn: 9.4303120\ttotal: 1.09s\tremaining: 2m 15s\n",
      "8:\tlearn: 9.3388005\ttotal: 1.21s\tremaining: 2m 12s\n",
      "9:\tlearn: 9.2571771\ttotal: 1.32s\tremaining: 2m 11s\n",
      "10:\tlearn: 9.1784009\ttotal: 1.44s\tremaining: 2m 9s\n",
      "11:\tlearn: 9.1141058\ttotal: 1.57s\tremaining: 2m 9s\n",
      "12:\tlearn: 9.0518966\ttotal: 1.7s\tremaining: 2m 8s\n",
      "13:\tlearn: 8.9961118\ttotal: 1.81s\tremaining: 2m 7s\n",
      "14:\tlearn: 8.9404305\ttotal: 1.92s\tremaining: 2m 6s\n",
      "15:\tlearn: 8.8880598\ttotal: 2.03s\tremaining: 2m 5s\n",
      "16:\tlearn: 8.8453148\ttotal: 2.15s\tremaining: 2m 4s\n",
      "17:\tlearn: 8.8014856\ttotal: 2.26s\tremaining: 2m 3s\n",
      "18:\tlearn: 8.7627664\ttotal: 2.39s\tremaining: 2m 3s\n",
      "19:\tlearn: 8.7254731\ttotal: 2.5s\tremaining: 2m 2s\n",
      "20:\tlearn: 8.6868237\ttotal: 2.61s\tremaining: 2m 1s\n",
      "21:\tlearn: 8.6507741\ttotal: 2.72s\tremaining: 2m\n",
      "22:\tlearn: 8.6181209\ttotal: 2.82s\tremaining: 1m 59s\n",
      "23:\tlearn: 8.5868627\ttotal: 2.93s\tremaining: 1m 59s\n",
      "24:\tlearn: 8.5548622\ttotal: 3.04s\tremaining: 1m 58s\n",
      "25:\tlearn: 8.5259273\ttotal: 3.15s\tremaining: 1m 57s\n",
      "26:\tlearn: 8.5019169\ttotal: 3.25s\tremaining: 1m 57s\n",
      "27:\tlearn: 8.4785449\ttotal: 3.35s\tremaining: 1m 56s\n",
      "28:\tlearn: 8.4589192\ttotal: 3.46s\tremaining: 1m 56s\n",
      "29:\tlearn: 8.4345162\ttotal: 3.59s\tremaining: 1m 56s\n",
      "30:\tlearn: 8.4140568\ttotal: 3.71s\tremaining: 1m 55s\n",
      "31:\tlearn: 8.3924872\ttotal: 3.81s\tremaining: 1m 55s\n",
      "32:\tlearn: 8.3751447\ttotal: 3.92s\tremaining: 1m 54s\n",
      "33:\tlearn: 8.3580857\ttotal: 4.02s\tremaining: 1m 54s\n",
      "34:\tlearn: 8.3418763\ttotal: 4.13s\tremaining: 1m 53s\n",
      "35:\tlearn: 8.3264895\ttotal: 4.24s\tremaining: 1m 53s\n",
      "36:\tlearn: 8.3132037\ttotal: 4.35s\tremaining: 1m 53s\n",
      "37:\tlearn: 8.2973652\ttotal: 4.48s\tremaining: 1m 53s\n",
      "38:\tlearn: 8.2819575\ttotal: 4.59s\tremaining: 1m 53s\n",
      "39:\tlearn: 8.2695841\ttotal: 4.7s\tremaining: 1m 52s\n",
      "40:\tlearn: 8.2546188\ttotal: 4.83s\tremaining: 1m 52s\n",
      "41:\tlearn: 8.2451148\ttotal: 4.93s\tremaining: 1m 52s\n",
      "42:\tlearn: 8.2350596\ttotal: 5.04s\tremaining: 1m 52s\n",
      "43:\tlearn: 8.2222065\ttotal: 5.16s\tremaining: 1m 52s\n",
      "44:\tlearn: 8.2110703\ttotal: 5.27s\tremaining: 1m 51s\n",
      "45:\tlearn: 8.2010000\ttotal: 5.38s\tremaining: 1m 51s\n",
      "46:\tlearn: 8.1879471\ttotal: 5.49s\tremaining: 1m 51s\n",
      "47:\tlearn: 8.1780041\ttotal: 5.6s\tremaining: 1m 51s\n",
      "48:\tlearn: 8.1699896\ttotal: 5.72s\tremaining: 1m 51s\n",
      "49:\tlearn: 8.1614710\ttotal: 5.83s\tremaining: 1m 50s\n",
      "50:\tlearn: 8.1517647\ttotal: 5.95s\tremaining: 1m 50s\n",
      "51:\tlearn: 8.1408265\ttotal: 6.06s\tremaining: 1m 50s\n",
      "52:\tlearn: 8.1328982\ttotal: 6.17s\tremaining: 1m 50s\n",
      "53:\tlearn: 8.1242581\ttotal: 6.28s\tremaining: 1m 50s\n",
      "54:\tlearn: 8.1162344\ttotal: 6.39s\tremaining: 1m 49s\n",
      "55:\tlearn: 8.1078284\ttotal: 6.5s\tremaining: 1m 49s\n",
      "56:\tlearn: 8.0989908\ttotal: 6.61s\tremaining: 1m 49s\n",
      "57:\tlearn: 8.0888279\ttotal: 6.72s\tremaining: 1m 49s\n",
      "58:\tlearn: 8.0814725\ttotal: 6.83s\tremaining: 1m 49s\n",
      "59:\tlearn: 8.0722102\ttotal: 6.95s\tremaining: 1m 48s\n",
      "60:\tlearn: 8.0657691\ttotal: 7.06s\tremaining: 1m 48s\n",
      "61:\tlearn: 8.0574598\ttotal: 7.17s\tremaining: 1m 48s\n",
      "62:\tlearn: 8.0512616\ttotal: 7.29s\tremaining: 1m 48s\n",
      "63:\tlearn: 8.0446953\ttotal: 7.41s\tremaining: 1m 48s\n",
      "64:\tlearn: 8.0382889\ttotal: 7.53s\tremaining: 1m 48s\n",
      "65:\tlearn: 8.0316378\ttotal: 7.65s\tremaining: 1m 48s\n",
      "66:\tlearn: 8.0235133\ttotal: 7.78s\tremaining: 1m 48s\n",
      "67:\tlearn: 8.0169703\ttotal: 7.91s\tremaining: 1m 48s\n",
      "68:\tlearn: 8.0109390\ttotal: 8.02s\tremaining: 1m 48s\n",
      "69:\tlearn: 8.0062040\ttotal: 8.13s\tremaining: 1m 48s\n",
      "70:\tlearn: 8.0004595\ttotal: 8.25s\tremaining: 1m 47s\n",
      "71:\tlearn: 7.9963170\ttotal: 8.37s\tremaining: 1m 47s\n",
      "72:\tlearn: 7.9885754\ttotal: 8.48s\tremaining: 1m 47s\n",
      "73:\tlearn: 7.9825851\ttotal: 8.59s\tremaining: 1m 47s\n",
      "74:\tlearn: 7.9772008\ttotal: 8.71s\tremaining: 1m 47s\n",
      "75:\tlearn: 7.9730546\ttotal: 8.82s\tremaining: 1m 47s\n",
      "76:\tlearn: 7.9669750\ttotal: 8.95s\tremaining: 1m 47s\n",
      "77:\tlearn: 7.9615572\ttotal: 9.06s\tremaining: 1m 47s\n",
      "78:\tlearn: 7.9564994\ttotal: 9.18s\tremaining: 1m 47s\n",
      "79:\tlearn: 7.9518001\ttotal: 9.29s\tremaining: 1m 46s\n",
      "80:\tlearn: 7.9465926\ttotal: 9.4s\tremaining: 1m 46s\n",
      "81:\tlearn: 7.9414244\ttotal: 9.51s\tremaining: 1m 46s\n",
      "82:\tlearn: 7.9355892\ttotal: 9.62s\tremaining: 1m 46s\n",
      "83:\tlearn: 7.9274620\ttotal: 9.73s\tremaining: 1m 46s\n",
      "84:\tlearn: 7.9240117\ttotal: 9.85s\tremaining: 1m 46s\n",
      "85:\tlearn: 7.9199843\ttotal: 9.97s\tremaining: 1m 45s\n",
      "86:\tlearn: 7.9154144\ttotal: 10.1s\tremaining: 1m 45s\n",
      "87:\tlearn: 7.9109004\ttotal: 10.2s\tremaining: 1m 45s\n",
      "88:\tlearn: 7.9066900\ttotal: 10.3s\tremaining: 1m 45s\n",
      "89:\tlearn: 7.9020798\ttotal: 10.4s\tremaining: 1m 45s\n",
      "90:\tlearn: 7.8965343\ttotal: 10.5s\tremaining: 1m 44s\n",
      "91:\tlearn: 7.8915601\ttotal: 10.6s\tremaining: 1m 44s\n",
      "92:\tlearn: 7.8891837\ttotal: 10.7s\tremaining: 1m 44s\n",
      "93:\tlearn: 7.8850612\ttotal: 10.8s\tremaining: 1m 44s\n",
      "94:\tlearn: 7.8820654\ttotal: 10.9s\tremaining: 1m 44s\n",
      "95:\tlearn: 7.8777360\ttotal: 11s\tremaining: 1m 43s\n",
      "96:\tlearn: 7.8732582\ttotal: 11.1s\tremaining: 1m 43s\n",
      "97:\tlearn: 7.8682470\ttotal: 11.3s\tremaining: 1m 43s\n",
      "98:\tlearn: 7.8638431\ttotal: 11.4s\tremaining: 1m 43s\n",
      "99:\tlearn: 7.8596412\ttotal: 11.5s\tremaining: 1m 43s\n",
      "100:\tlearn: 7.8554667\ttotal: 11.6s\tremaining: 1m 42s\n",
      "101:\tlearn: 7.8516419\ttotal: 11.7s\tremaining: 1m 42s\n",
      "102:\tlearn: 7.8470665\ttotal: 11.8s\tremaining: 1m 42s\n",
      "103:\tlearn: 7.8428993\ttotal: 11.9s\tremaining: 1m 42s\n",
      "104:\tlearn: 7.8380648\ttotal: 12s\tremaining: 1m 42s\n",
      "105:\tlearn: 7.8338259\ttotal: 12.1s\tremaining: 1m 41s\n",
      "106:\tlearn: 7.8293043\ttotal: 12.2s\tremaining: 1m 41s\n",
      "107:\tlearn: 7.8249162\ttotal: 12.3s\tremaining: 1m 41s\n",
      "108:\tlearn: 7.8212419\ttotal: 12.4s\tremaining: 1m 41s\n",
      "109:\tlearn: 7.8179441\ttotal: 12.5s\tremaining: 1m 41s\n",
      "110:\tlearn: 7.8141661\ttotal: 12.6s\tremaining: 1m 40s\n",
      "111:\tlearn: 7.8107788\ttotal: 12.7s\tremaining: 1m 40s\n",
      "112:\tlearn: 7.8077381\ttotal: 12.8s\tremaining: 1m 40s\n",
      "113:\tlearn: 7.8037150\ttotal: 12.9s\tremaining: 1m 40s\n",
      "114:\tlearn: 7.7996954\ttotal: 13s\tremaining: 1m 40s\n",
      "115:\tlearn: 7.7963854\ttotal: 13.1s\tremaining: 1m 40s\n",
      "116:\tlearn: 7.7930058\ttotal: 13.3s\tremaining: 1m 40s\n",
      "117:\tlearn: 7.7891864\ttotal: 13.4s\tremaining: 1m 39s\n",
      "118:\tlearn: 7.7851632\ttotal: 13.5s\tremaining: 1m 39s\n",
      "119:\tlearn: 7.7825504\ttotal: 13.6s\tremaining: 1m 39s\n",
      "120:\tlearn: 7.7788577\ttotal: 13.7s\tremaining: 1m 39s\n",
      "121:\tlearn: 7.7755473\ttotal: 13.8s\tremaining: 1m 39s\n",
      "122:\tlearn: 7.7712552\ttotal: 13.9s\tremaining: 1m 39s\n",
      "123:\tlearn: 7.7684097\ttotal: 14s\tremaining: 1m 39s\n",
      "124:\tlearn: 7.7645955\ttotal: 14.2s\tremaining: 1m 39s\n",
      "125:\tlearn: 7.7597959\ttotal: 14.3s\tremaining: 1m 39s\n",
      "126:\tlearn: 7.7568368\ttotal: 14.4s\tremaining: 1m 38s\n",
      "127:\tlearn: 7.7523389\ttotal: 14.5s\tremaining: 1m 38s\n",
      "128:\tlearn: 7.7496531\ttotal: 14.6s\tremaining: 1m 38s\n",
      "129:\tlearn: 7.7457696\ttotal: 14.7s\tremaining: 1m 38s\n",
      "130:\tlearn: 7.7429434\ttotal: 14.8s\tremaining: 1m 38s\n",
      "131:\tlearn: 7.7391785\ttotal: 15s\tremaining: 1m 38s\n",
      "132:\tlearn: 7.7357843\ttotal: 15.1s\tremaining: 1m 38s\n",
      "133:\tlearn: 7.7306676\ttotal: 15.2s\tremaining: 1m 38s\n",
      "134:\tlearn: 7.7266111\ttotal: 15.3s\tremaining: 1m 38s\n",
      "135:\tlearn: 7.7203328\ttotal: 15.4s\tremaining: 1m 38s\n",
      "136:\tlearn: 7.7171489\ttotal: 15.5s\tremaining: 1m 37s\n",
      "137:\tlearn: 7.7123827\ttotal: 15.7s\tremaining: 1m 37s\n",
      "138:\tlearn: 7.7094505\ttotal: 15.8s\tremaining: 1m 37s\n",
      "139:\tlearn: 7.7062443\ttotal: 15.9s\tremaining: 1m 37s\n",
      "140:\tlearn: 7.7032258\ttotal: 16s\tremaining: 1m 37s\n",
      "141:\tlearn: 7.6994792\ttotal: 16.1s\tremaining: 1m 37s\n",
      "142:\tlearn: 7.6962969\ttotal: 16.2s\tremaining: 1m 37s\n",
      "143:\tlearn: 7.6940732\ttotal: 16.3s\tremaining: 1m 37s\n",
      "144:\tlearn: 7.6913714\ttotal: 16.4s\tremaining: 1m 36s\n",
      "145:\tlearn: 7.6875032\ttotal: 16.5s\tremaining: 1m 36s\n",
      "146:\tlearn: 7.6838721\ttotal: 16.6s\tremaining: 1m 36s\n",
      "147:\tlearn: 7.6804108\ttotal: 16.7s\tremaining: 1m 36s\n",
      "148:\tlearn: 7.6760593\ttotal: 16.9s\tremaining: 1m 36s\n",
      "149:\tlearn: 7.6726874\ttotal: 17s\tremaining: 1m 36s\n",
      "150:\tlearn: 7.6688468\ttotal: 17.1s\tremaining: 1m 36s\n",
      "151:\tlearn: 7.6656746\ttotal: 17.2s\tremaining: 1m 35s\n",
      "152:\tlearn: 7.6616420\ttotal: 17.3s\tremaining: 1m 35s\n",
      "153:\tlearn: 7.6573704\ttotal: 17.4s\tremaining: 1m 35s\n",
      "154:\tlearn: 7.6535875\ttotal: 17.5s\tremaining: 1m 35s\n",
      "155:\tlearn: 7.6488614\ttotal: 17.6s\tremaining: 1m 35s\n",
      "156:\tlearn: 7.6459393\ttotal: 17.7s\tremaining: 1m 35s\n",
      "157:\tlearn: 7.6419875\ttotal: 17.8s\tremaining: 1m 35s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158:\tlearn: 7.6385690\ttotal: 17.9s\tremaining: 1m 34s\n",
      "159:\tlearn: 7.6352623\ttotal: 18.1s\tremaining: 1m 34s\n",
      "160:\tlearn: 7.6298776\ttotal: 18.2s\tremaining: 1m 34s\n",
      "161:\tlearn: 7.6268885\ttotal: 18.3s\tremaining: 1m 34s\n",
      "162:\tlearn: 7.6229951\ttotal: 18.4s\tremaining: 1m 34s\n",
      "163:\tlearn: 7.6175626\ttotal: 18.5s\tremaining: 1m 34s\n",
      "164:\tlearn: 7.6142392\ttotal: 18.6s\tremaining: 1m 33s\n",
      "165:\tlearn: 7.6097335\ttotal: 18.7s\tremaining: 1m 33s\n",
      "166:\tlearn: 7.6072812\ttotal: 18.8s\tremaining: 1m 33s\n",
      "167:\tlearn: 7.6060232\ttotal: 18.9s\tremaining: 1m 33s\n",
      "168:\tlearn: 7.6009440\ttotal: 19s\tremaining: 1m 33s\n",
      "169:\tlearn: 7.5976267\ttotal: 19.1s\tremaining: 1m 33s\n",
      "170:\tlearn: 7.5937460\ttotal: 19.2s\tremaining: 1m 32s\n",
      "171:\tlearn: 7.5893971\ttotal: 19.3s\tremaining: 1m 32s\n",
      "172:\tlearn: 7.5855138\ttotal: 19.4s\tremaining: 1m 32s\n",
      "173:\tlearn: 7.5818254\ttotal: 19.5s\tremaining: 1m 32s\n",
      "174:\tlearn: 7.5786640\ttotal: 19.6s\tremaining: 1m 32s\n",
      "175:\tlearn: 7.5747723\ttotal: 19.7s\tremaining: 1m 32s\n",
      "176:\tlearn: 7.5721741\ttotal: 19.8s\tremaining: 1m 31s\n",
      "177:\tlearn: 7.5664010\ttotal: 19.9s\tremaining: 1m 31s\n",
      "178:\tlearn: 7.5655255\ttotal: 20s\tremaining: 1m 31s\n",
      "179:\tlearn: 7.5636201\ttotal: 20.1s\tremaining: 1m 31s\n",
      "180:\tlearn: 7.5608426\ttotal: 20.2s\tremaining: 1m 31s\n",
      "181:\tlearn: 7.5560103\ttotal: 20.3s\tremaining: 1m 31s\n",
      "182:\tlearn: 7.5534703\ttotal: 20.4s\tremaining: 1m 30s\n",
      "183:\tlearn: 7.5480017\ttotal: 20.5s\tremaining: 1m 30s\n",
      "184:\tlearn: 7.5434664\ttotal: 20.6s\tremaining: 1m 30s\n",
      "185:\tlearn: 7.5388061\ttotal: 20.7s\tremaining: 1m 30s\n",
      "186:\tlearn: 7.5344567\ttotal: 20.8s\tremaining: 1m 30s\n",
      "187:\tlearn: 7.5303105\ttotal: 20.9s\tremaining: 1m 30s\n",
      "188:\tlearn: 7.5246519\ttotal: 21s\tremaining: 1m 29s\n",
      "189:\tlearn: 7.5209087\ttotal: 21.1s\tremaining: 1m 29s\n",
      "190:\tlearn: 7.5158947\ttotal: 21.2s\tremaining: 1m 29s\n",
      "191:\tlearn: 7.5120955\ttotal: 21.3s\tremaining: 1m 29s\n",
      "192:\tlearn: 7.5078305\ttotal: 21.4s\tremaining: 1m 29s\n",
      "193:\tlearn: 7.5045557\ttotal: 21.5s\tremaining: 1m 29s\n",
      "194:\tlearn: 7.5013520\ttotal: 21.6s\tremaining: 1m 29s\n",
      "195:\tlearn: 7.4961714\ttotal: 21.7s\tremaining: 1m 28s\n",
      "196:\tlearn: 7.4904514\ttotal: 21.8s\tremaining: 1m 28s\n",
      "197:\tlearn: 7.4885532\ttotal: 21.9s\tremaining: 1m 28s\n",
      "198:\tlearn: 7.4825381\ttotal: 22s\tremaining: 1m 28s\n",
      "199:\tlearn: 7.4779385\ttotal: 22.1s\tremaining: 1m 28s\n",
      "200:\tlearn: 7.4741568\ttotal: 22.2s\tremaining: 1m 28s\n",
      "201:\tlearn: 7.4698931\ttotal: 22.3s\tremaining: 1m 28s\n",
      "202:\tlearn: 7.4647368\ttotal: 22.4s\tremaining: 1m 28s\n",
      "203:\tlearn: 7.4589710\ttotal: 22.5s\tremaining: 1m 27s\n",
      "204:\tlearn: 7.4550208\ttotal: 22.6s\tremaining: 1m 27s\n",
      "205:\tlearn: 7.4497985\ttotal: 22.7s\tremaining: 1m 27s\n",
      "206:\tlearn: 7.4492077\ttotal: 22.9s\tremaining: 1m 27s\n",
      "207:\tlearn: 7.4454228\ttotal: 23s\tremaining: 1m 27s\n",
      "208:\tlearn: 7.4421056\ttotal: 23.1s\tremaining: 1m 27s\n",
      "209:\tlearn: 7.4381309\ttotal: 23.2s\tremaining: 1m 27s\n",
      "210:\tlearn: 7.4335332\ttotal: 23.3s\tremaining: 1m 27s\n",
      "211:\tlearn: 7.4300192\ttotal: 23.4s\tremaining: 1m 26s\n",
      "212:\tlearn: 7.4257436\ttotal: 23.5s\tremaining: 1m 26s\n",
      "213:\tlearn: 7.4223490\ttotal: 23.6s\tremaining: 1m 26s\n",
      "214:\tlearn: 7.4160221\ttotal: 23.7s\tremaining: 1m 26s\n",
      "215:\tlearn: 7.4129716\ttotal: 23.8s\tremaining: 1m 26s\n",
      "216:\tlearn: 7.4088562\ttotal: 23.9s\tremaining: 1m 26s\n",
      "217:\tlearn: 7.4054516\ttotal: 24s\tremaining: 1m 25s\n",
      "218:\tlearn: 7.4006107\ttotal: 24.1s\tremaining: 1m 25s\n",
      "219:\tlearn: 7.3971145\ttotal: 24.2s\tremaining: 1m 25s\n",
      "220:\tlearn: 7.3937958\ttotal: 24.3s\tremaining: 1m 25s\n",
      "221:\tlearn: 7.3903014\ttotal: 24.4s\tremaining: 1m 25s\n",
      "222:\tlearn: 7.3868327\ttotal: 24.5s\tremaining: 1m 25s\n",
      "223:\tlearn: 7.3820040\ttotal: 24.6s\tremaining: 1m 25s\n",
      "224:\tlearn: 7.3784228\ttotal: 24.7s\tremaining: 1m 24s\n",
      "225:\tlearn: 7.3741226\ttotal: 24.8s\tremaining: 1m 24s\n",
      "226:\tlearn: 7.3701100\ttotal: 24.9s\tremaining: 1m 24s\n",
      "227:\tlearn: 7.3659323\ttotal: 24.9s\tremaining: 1m 24s\n",
      "228:\tlearn: 7.3627855\ttotal: 25s\tremaining: 1m 24s\n",
      "229:\tlearn: 7.3574919\ttotal: 25.1s\tremaining: 1m 24s\n",
      "230:\tlearn: 7.3544568\ttotal: 25.2s\tremaining: 1m 24s\n",
      "231:\tlearn: 7.3502667\ttotal: 25.3s\tremaining: 1m 23s\n",
      "232:\tlearn: 7.3457181\ttotal: 25.4s\tremaining: 1m 23s\n",
      "233:\tlearn: 7.3410855\ttotal: 25.5s\tremaining: 1m 23s\n",
      "234:\tlearn: 7.3371152\ttotal: 25.6s\tremaining: 1m 23s\n",
      "235:\tlearn: 7.3324537\ttotal: 25.7s\tremaining: 1m 23s\n",
      "236:\tlearn: 7.3279970\ttotal: 25.8s\tremaining: 1m 23s\n",
      "237:\tlearn: 7.3233092\ttotal: 25.9s\tremaining: 1m 22s\n",
      "238:\tlearn: 7.3192639\ttotal: 26s\tremaining: 1m 22s\n",
      "239:\tlearn: 7.3147101\ttotal: 26.1s\tremaining: 1m 22s\n",
      "240:\tlearn: 7.3116487\ttotal: 26.2s\tremaining: 1m 22s\n",
      "241:\tlearn: 7.3086324\ttotal: 26.3s\tremaining: 1m 22s\n",
      "242:\tlearn: 7.3039340\ttotal: 26.4s\tremaining: 1m 22s\n",
      "243:\tlearn: 7.3009365\ttotal: 26.4s\tremaining: 1m 21s\n",
      "244:\tlearn: 7.2966799\ttotal: 26.5s\tremaining: 1m 21s\n",
      "245:\tlearn: 7.2938094\ttotal: 26.6s\tremaining: 1m 21s\n",
      "246:\tlearn: 7.2896239\ttotal: 26.7s\tremaining: 1m 21s\n",
      "247:\tlearn: 7.2861507\ttotal: 26.8s\tremaining: 1m 21s\n",
      "248:\tlearn: 7.2818750\ttotal: 26.9s\tremaining: 1m 21s\n",
      "249:\tlearn: 7.2770458\ttotal: 27s\tremaining: 1m 20s\n",
      "250:\tlearn: 7.2733139\ttotal: 27.1s\tremaining: 1m 20s\n",
      "251:\tlearn: 7.2722951\ttotal: 27.2s\tremaining: 1m 20s\n",
      "252:\tlearn: 7.2693279\ttotal: 27.3s\tremaining: 1m 20s\n",
      "253:\tlearn: 7.2686523\ttotal: 27.4s\tremaining: 1m 20s\n",
      "254:\tlearn: 7.2645902\ttotal: 27.4s\tremaining: 1m 20s\n",
      "255:\tlearn: 7.2606331\ttotal: 27.5s\tremaining: 1m 20s\n",
      "256:\tlearn: 7.2546134\ttotal: 27.6s\tremaining: 1m 19s\n",
      "257:\tlearn: 7.2504632\ttotal: 27.7s\tremaining: 1m 19s\n",
      "258:\tlearn: 7.2471195\ttotal: 27.8s\tremaining: 1m 19s\n",
      "259:\tlearn: 7.2419824\ttotal: 27.9s\tremaining: 1m 19s\n",
      "260:\tlearn: 7.2385368\ttotal: 28s\tremaining: 1m 19s\n",
      "261:\tlearn: 7.2349726\ttotal: 28.1s\tremaining: 1m 19s\n",
      "262:\tlearn: 7.2313066\ttotal: 28.2s\tremaining: 1m 19s\n",
      "263:\tlearn: 7.2284022\ttotal: 28.3s\tremaining: 1m 18s\n",
      "264:\tlearn: 7.2239343\ttotal: 28.4s\tremaining: 1m 18s\n",
      "265:\tlearn: 7.2197877\ttotal: 28.5s\tremaining: 1m 18s\n",
      "266:\tlearn: 7.2144854\ttotal: 28.6s\tremaining: 1m 18s\n",
      "267:\tlearn: 7.2111505\ttotal: 28.7s\tremaining: 1m 18s\n",
      "268:\tlearn: 7.2074639\ttotal: 28.8s\tremaining: 1m 18s\n",
      "269:\tlearn: 7.2039072\ttotal: 28.9s\tremaining: 1m 18s\n",
      "270:\tlearn: 7.2020470\ttotal: 29s\tremaining: 1m 18s\n",
      "271:\tlearn: 7.1985282\ttotal: 29.1s\tremaining: 1m 17s\n",
      "272:\tlearn: 7.1954647\ttotal: 29.2s\tremaining: 1m 17s\n",
      "273:\tlearn: 7.1918028\ttotal: 29.3s\tremaining: 1m 17s\n",
      "274:\tlearn: 7.1889296\ttotal: 29.4s\tremaining: 1m 17s\n",
      "275:\tlearn: 7.1842432\ttotal: 29.5s\tremaining: 1m 17s\n",
      "276:\tlearn: 7.1790648\ttotal: 29.7s\tremaining: 1m 17s\n",
      "277:\tlearn: 7.1749609\ttotal: 29.8s\tremaining: 1m 17s\n",
      "278:\tlearn: 7.1698630\ttotal: 29.9s\tremaining: 1m 17s\n",
      "279:\tlearn: 7.1657990\ttotal: 30s\tremaining: 1m 17s\n",
      "280:\tlearn: 7.1612082\ttotal: 30.1s\tremaining: 1m 16s\n",
      "281:\tlearn: 7.1567532\ttotal: 30.2s\tremaining: 1m 16s\n",
      "282:\tlearn: 7.1561114\ttotal: 30.3s\tremaining: 1m 16s\n",
      "283:\tlearn: 7.1519119\ttotal: 30.4s\tremaining: 1m 16s\n",
      "284:\tlearn: 7.1483905\ttotal: 30.5s\tremaining: 1m 16s\n",
      "285:\tlearn: 7.1448725\ttotal: 30.6s\tremaining: 1m 16s\n",
      "286:\tlearn: 7.1416384\ttotal: 30.7s\tremaining: 1m 16s\n",
      "287:\tlearn: 7.1384596\ttotal: 30.8s\tremaining: 1m 16s\n",
      "288:\tlearn: 7.1342736\ttotal: 30.9s\tremaining: 1m 15s\n",
      "289:\tlearn: 7.1312986\ttotal: 31s\tremaining: 1m 15s\n",
      "290:\tlearn: 7.1278802\ttotal: 31.1s\tremaining: 1m 15s\n",
      "291:\tlearn: 7.1246262\ttotal: 31.2s\tremaining: 1m 15s\n",
      "292:\tlearn: 7.1202664\ttotal: 31.3s\tremaining: 1m 15s\n",
      "293:\tlearn: 7.1164382\ttotal: 31.4s\tremaining: 1m 15s\n",
      "294:\tlearn: 7.1119800\ttotal: 31.5s\tremaining: 1m 15s\n",
      "295:\tlearn: 7.1087974\ttotal: 31.6s\tremaining: 1m 15s\n",
      "296:\tlearn: 7.1061178\ttotal: 31.7s\tremaining: 1m 14s\n",
      "297:\tlearn: 7.1027695\ttotal: 31.8s\tremaining: 1m 14s\n",
      "298:\tlearn: 7.0972822\ttotal: 31.9s\tremaining: 1m 14s\n",
      "299:\tlearn: 7.0949306\ttotal: 32s\tremaining: 1m 14s\n",
      "300:\tlearn: 7.0912104\ttotal: 32.1s\tremaining: 1m 14s\n",
      "301:\tlearn: 7.0866168\ttotal: 32.2s\tremaining: 1m 14s\n",
      "302:\tlearn: 7.0838776\ttotal: 32.3s\tremaining: 1m 14s\n",
      "303:\tlearn: 7.0800350\ttotal: 32.4s\tremaining: 1m 14s\n",
      "304:\tlearn: 7.0767113\ttotal: 32.5s\tremaining: 1m 13s\n",
      "305:\tlearn: 7.0739924\ttotal: 32.6s\tremaining: 1m 13s\n",
      "306:\tlearn: 7.0703058\ttotal: 32.7s\tremaining: 1m 13s\n",
      "307:\tlearn: 7.0698743\ttotal: 32.7s\tremaining: 1m 13s\n",
      "308:\tlearn: 7.0674433\ttotal: 32.8s\tremaining: 1m 13s\n",
      "309:\tlearn: 7.0642748\ttotal: 32.9s\tremaining: 1m 13s\n",
      "310:\tlearn: 7.0611394\ttotal: 33s\tremaining: 1m 13s\n",
      "311:\tlearn: 7.0583387\ttotal: 33.1s\tremaining: 1m 13s\n",
      "312:\tlearn: 7.0548999\ttotal: 33.2s\tremaining: 1m 12s\n",
      "313:\tlearn: 7.0511879\ttotal: 33.3s\tremaining: 1m 12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314:\tlearn: 7.0493721\ttotal: 33.4s\tremaining: 1m 12s\n",
      "315:\tlearn: 7.0450058\ttotal: 33.5s\tremaining: 1m 12s\n",
      "316:\tlearn: 7.0409368\ttotal: 33.6s\tremaining: 1m 12s\n",
      "317:\tlearn: 7.0366382\ttotal: 33.7s\tremaining: 1m 12s\n",
      "318:\tlearn: 7.0334652\ttotal: 33.8s\tremaining: 1m 12s\n",
      "319:\tlearn: 7.0305815\ttotal: 33.9s\tremaining: 1m 11s\n",
      "320:\tlearn: 7.0301526\ttotal: 33.9s\tremaining: 1m 11s\n",
      "321:\tlearn: 7.0261896\ttotal: 34s\tremaining: 1m 11s\n",
      "322:\tlearn: 7.0234660\ttotal: 34.1s\tremaining: 1m 11s\n",
      "323:\tlearn: 7.0209326\ttotal: 34.2s\tremaining: 1m 11s\n",
      "324:\tlearn: 7.0170141\ttotal: 34.3s\tremaining: 1m 11s\n",
      "325:\tlearn: 7.0140489\ttotal: 34.4s\tremaining: 1m 11s\n",
      "326:\tlearn: 7.0099417\ttotal: 34.5s\tremaining: 1m 11s\n",
      "327:\tlearn: 7.0063584\ttotal: 34.6s\tremaining: 1m 10s\n",
      "328:\tlearn: 7.0059627\ttotal: 34.7s\tremaining: 1m 10s\n",
      "329:\tlearn: 7.0028311\ttotal: 34.8s\tremaining: 1m 10s\n",
      "330:\tlearn: 6.9988291\ttotal: 34.9s\tremaining: 1m 10s\n",
      "331:\tlearn: 6.9959222\ttotal: 35s\tremaining: 1m 10s\n",
      "332:\tlearn: 6.9927883\ttotal: 35.1s\tremaining: 1m 10s\n",
      "333:\tlearn: 6.9890106\ttotal: 35.2s\tremaining: 1m 10s\n",
      "334:\tlearn: 6.9859437\ttotal: 35.3s\tremaining: 1m 10s\n",
      "335:\tlearn: 6.9833557\ttotal: 35.4s\tremaining: 1m 9s\n",
      "336:\tlearn: 6.9786899\ttotal: 35.5s\tremaining: 1m 9s\n",
      "337:\tlearn: 6.9754699\ttotal: 35.6s\tremaining: 1m 9s\n",
      "338:\tlearn: 6.9723016\ttotal: 35.7s\tremaining: 1m 9s\n",
      "339:\tlearn: 6.9691266\ttotal: 35.8s\tremaining: 1m 9s\n",
      "340:\tlearn: 6.9657738\ttotal: 35.9s\tremaining: 1m 9s\n",
      "341:\tlearn: 6.9616624\ttotal: 36s\tremaining: 1m 9s\n",
      "342:\tlearn: 6.9586837\ttotal: 36.1s\tremaining: 1m 9s\n",
      "343:\tlearn: 6.9540817\ttotal: 36.2s\tremaining: 1m 9s\n",
      "344:\tlearn: 6.9493142\ttotal: 36.3s\tremaining: 1m 8s\n",
      "345:\tlearn: 6.9458511\ttotal: 36.4s\tremaining: 1m 8s\n",
      "346:\tlearn: 6.9426314\ttotal: 36.5s\tremaining: 1m 8s\n",
      "347:\tlearn: 6.9387546\ttotal: 36.6s\tremaining: 1m 8s\n",
      "348:\tlearn: 6.9336660\ttotal: 36.7s\tremaining: 1m 8s\n",
      "349:\tlearn: 6.9297611\ttotal: 36.8s\tremaining: 1m 8s\n",
      "350:\tlearn: 6.9294621\ttotal: 36.9s\tremaining: 1m 8s\n",
      "351:\tlearn: 6.9256772\ttotal: 37s\tremaining: 1m 8s\n",
      "352:\tlearn: 6.9228369\ttotal: 37.1s\tremaining: 1m 8s\n",
      "353:\tlearn: 6.9201050\ttotal: 37.2s\tremaining: 1m 7s\n",
      "354:\tlearn: 6.9163059\ttotal: 37.4s\tremaining: 1m 7s\n",
      "355:\tlearn: 6.9117622\ttotal: 37.5s\tremaining: 1m 7s\n",
      "356:\tlearn: 6.9088327\ttotal: 37.6s\tremaining: 1m 7s\n",
      "357:\tlearn: 6.9051251\ttotal: 37.7s\tremaining: 1m 7s\n",
      "358:\tlearn: 6.9014944\ttotal: 37.8s\tremaining: 1m 7s\n",
      "359:\tlearn: 6.8984697\ttotal: 37.9s\tremaining: 1m 7s\n",
      "360:\tlearn: 6.8956303\ttotal: 38s\tremaining: 1m 7s\n",
      "361:\tlearn: 6.8934108\ttotal: 38.1s\tremaining: 1m 7s\n",
      "362:\tlearn: 6.8893295\ttotal: 38.2s\tremaining: 1m 6s\n",
      "363:\tlearn: 6.8878820\ttotal: 38.3s\tremaining: 1m 6s\n",
      "364:\tlearn: 6.8839862\ttotal: 38.4s\tremaining: 1m 6s\n",
      "365:\tlearn: 6.8804088\ttotal: 38.5s\tremaining: 1m 6s\n",
      "366:\tlearn: 6.8771801\ttotal: 38.5s\tremaining: 1m 6s\n",
      "367:\tlearn: 6.8739027\ttotal: 38.6s\tremaining: 1m 6s\n",
      "368:\tlearn: 6.8707551\ttotal: 38.7s\tremaining: 1m 6s\n",
      "369:\tlearn: 6.8666340\ttotal: 38.8s\tremaining: 1m 6s\n",
      "370:\tlearn: 6.8637708\ttotal: 38.9s\tremaining: 1m 6s\n",
      "371:\tlearn: 6.8612733\ttotal: 39s\tremaining: 1m 5s\n",
      "372:\tlearn: 6.8580699\ttotal: 39.1s\tremaining: 1m 5s\n",
      "373:\tlearn: 6.8556712\ttotal: 39.2s\tremaining: 1m 5s\n",
      "374:\tlearn: 6.8553031\ttotal: 39.3s\tremaining: 1m 5s\n",
      "375:\tlearn: 6.8523403\ttotal: 39.4s\tremaining: 1m 5s\n",
      "376:\tlearn: 6.8493226\ttotal: 39.5s\tremaining: 1m 5s\n",
      "377:\tlearn: 6.8459850\ttotal: 39.6s\tremaining: 1m 5s\n",
      "378:\tlearn: 6.8426243\ttotal: 39.7s\tremaining: 1m 5s\n",
      "379:\tlearn: 6.8388433\ttotal: 39.8s\tremaining: 1m 4s\n",
      "380:\tlearn: 6.8355510\ttotal: 39.9s\tremaining: 1m 4s\n",
      "381:\tlearn: 6.8315440\ttotal: 40s\tremaining: 1m 4s\n",
      "382:\tlearn: 6.8280745\ttotal: 40.1s\tremaining: 1m 4s\n",
      "383:\tlearn: 6.8260903\ttotal: 40.2s\tremaining: 1m 4s\n",
      "384:\tlearn: 6.8227102\ttotal: 40.3s\tremaining: 1m 4s\n",
      "385:\tlearn: 6.8197566\ttotal: 40.4s\tremaining: 1m 4s\n",
      "386:\tlearn: 6.8163705\ttotal: 40.5s\tremaining: 1m 4s\n",
      "387:\tlearn: 6.8138662\ttotal: 40.5s\tremaining: 1m 3s\n",
      "388:\tlearn: 6.8101273\ttotal: 40.6s\tremaining: 1m 3s\n",
      "389:\tlearn: 6.8064705\ttotal: 40.7s\tremaining: 1m 3s\n",
      "390:\tlearn: 6.8030313\ttotal: 40.8s\tremaining: 1m 3s\n",
      "391:\tlearn: 6.7994291\ttotal: 40.9s\tremaining: 1m 3s\n",
      "392:\tlearn: 6.7957917\ttotal: 41s\tremaining: 1m 3s\n",
      "393:\tlearn: 6.7930284\ttotal: 41.1s\tremaining: 1m 3s\n",
      "394:\tlearn: 6.7896037\ttotal: 41.2s\tremaining: 1m 3s\n",
      "395:\tlearn: 6.7892531\ttotal: 41.3s\tremaining: 1m 3s\n",
      "396:\tlearn: 6.7858248\ttotal: 41.4s\tremaining: 1m 2s\n",
      "397:\tlearn: 6.7826767\ttotal: 41.5s\tremaining: 1m 2s\n",
      "398:\tlearn: 6.7788316\ttotal: 41.6s\tremaining: 1m 2s\n",
      "399:\tlearn: 6.7755935\ttotal: 41.7s\tremaining: 1m 2s\n",
      "400:\tlearn: 6.7731763\ttotal: 41.8s\tremaining: 1m 2s\n",
      "401:\tlearn: 6.7682047\ttotal: 41.9s\tremaining: 1m 2s\n",
      "402:\tlearn: 6.7655717\ttotal: 42s\tremaining: 1m 2s\n",
      "403:\tlearn: 6.7635354\ttotal: 42.1s\tremaining: 1m 2s\n",
      "404:\tlearn: 6.7606011\ttotal: 42.2s\tremaining: 1m 1s\n",
      "405:\tlearn: 6.7573782\ttotal: 42.3s\tremaining: 1m 1s\n",
      "406:\tlearn: 6.7534851\ttotal: 42.4s\tremaining: 1m 1s\n",
      "407:\tlearn: 6.7498697\ttotal: 42.4s\tremaining: 1m 1s\n",
      "408:\tlearn: 6.7471860\ttotal: 42.5s\tremaining: 1m 1s\n",
      "409:\tlearn: 6.7441669\ttotal: 42.6s\tremaining: 1m 1s\n",
      "410:\tlearn: 6.7416604\ttotal: 42.7s\tremaining: 1m 1s\n",
      "411:\tlearn: 6.7386922\ttotal: 42.8s\tremaining: 1m 1s\n",
      "412:\tlearn: 6.7348744\ttotal: 43s\tremaining: 1m 1s\n",
      "413:\tlearn: 6.7310086\ttotal: 43.1s\tremaining: 1m\n",
      "414:\tlearn: 6.7273264\ttotal: 43.2s\tremaining: 1m\n",
      "415:\tlearn: 6.7245962\ttotal: 43.3s\tremaining: 1m\n",
      "416:\tlearn: 6.7214739\ttotal: 43.4s\tremaining: 1m\n",
      "417:\tlearn: 6.7184402\ttotal: 43.5s\tremaining: 1m\n",
      "418:\tlearn: 6.7163956\ttotal: 43.6s\tremaining: 1m\n",
      "419:\tlearn: 6.7127737\ttotal: 43.7s\tremaining: 1m\n",
      "420:\tlearn: 6.7104438\ttotal: 43.8s\tremaining: 1m\n",
      "421:\tlearn: 6.7081367\ttotal: 43.9s\tremaining: 1m\n",
      "422:\tlearn: 6.7050382\ttotal: 44s\tremaining: 60s\n",
      "423:\tlearn: 6.7011771\ttotal: 44.1s\tremaining: 59.9s\n",
      "424:\tlearn: 6.6981099\ttotal: 44.2s\tremaining: 59.8s\n",
      "425:\tlearn: 6.6945704\ttotal: 44.3s\tremaining: 59.7s\n",
      "426:\tlearn: 6.6910830\ttotal: 44.4s\tremaining: 59.6s\n",
      "427:\tlearn: 6.6889694\ttotal: 44.5s\tremaining: 59.5s\n",
      "428:\tlearn: 6.6856936\ttotal: 44.6s\tremaining: 59.4s\n",
      "429:\tlearn: 6.6830288\ttotal: 44.8s\tremaining: 59.3s\n",
      "430:\tlearn: 6.6797513\ttotal: 44.9s\tremaining: 59.3s\n",
      "431:\tlearn: 6.6766347\ttotal: 45s\tremaining: 59.2s\n",
      "432:\tlearn: 6.6743663\ttotal: 45.1s\tremaining: 59.1s\n",
      "433:\tlearn: 6.6713606\ttotal: 45.2s\tremaining: 59s\n",
      "434:\tlearn: 6.6690217\ttotal: 45.3s\tremaining: 58.9s\n",
      "435:\tlearn: 6.6659662\ttotal: 45.5s\tremaining: 58.8s\n",
      "436:\tlearn: 6.6627928\ttotal: 45.6s\tremaining: 58.7s\n",
      "437:\tlearn: 6.6593167\ttotal: 45.7s\tremaining: 58.6s\n",
      "438:\tlearn: 6.6563949\ttotal: 45.8s\tremaining: 58.5s\n",
      "439:\tlearn: 6.6538672\ttotal: 45.9s\tremaining: 58.4s\n",
      "440:\tlearn: 6.6502538\ttotal: 46s\tremaining: 58.3s\n",
      "441:\tlearn: 6.6460654\ttotal: 46.1s\tremaining: 58.2s\n",
      "442:\tlearn: 6.6423574\ttotal: 46.2s\tremaining: 58.1s\n",
      "443:\tlearn: 6.6383458\ttotal: 46.3s\tremaining: 58s\n",
      "444:\tlearn: 6.6355412\ttotal: 46.4s\tremaining: 57.9s\n",
      "445:\tlearn: 6.6332568\ttotal: 46.5s\tremaining: 57.8s\n",
      "446:\tlearn: 6.6305016\ttotal: 46.6s\tremaining: 57.7s\n",
      "447:\tlearn: 6.6280609\ttotal: 46.7s\tremaining: 57.6s\n",
      "448:\tlearn: 6.6259273\ttotal: 46.8s\tremaining: 57.5s\n",
      "449:\tlearn: 6.6235505\ttotal: 46.9s\tremaining: 57.4s\n",
      "450:\tlearn: 6.6207709\ttotal: 47s\tremaining: 57.3s\n",
      "451:\tlearn: 6.6174635\ttotal: 47.1s\tremaining: 57.2s\n",
      "452:\tlearn: 6.6141352\ttotal: 47.3s\tremaining: 57.1s\n",
      "453:\tlearn: 6.6106785\ttotal: 47.4s\tremaining: 57s\n",
      "454:\tlearn: 6.6079454\ttotal: 47.5s\tremaining: 56.8s\n",
      "455:\tlearn: 6.6047507\ttotal: 47.6s\tremaining: 56.7s\n",
      "456:\tlearn: 6.6020875\ttotal: 47.7s\tremaining: 56.6s\n",
      "457:\tlearn: 6.6000624\ttotal: 47.8s\tremaining: 56.5s\n",
      "458:\tlearn: 6.5968691\ttotal: 47.9s\tremaining: 56.4s\n",
      "459:\tlearn: 6.5944480\ttotal: 48s\tremaining: 56.3s\n",
      "460:\tlearn: 6.5916149\ttotal: 48.1s\tremaining: 56.2s\n",
      "461:\tlearn: 6.5898662\ttotal: 48.2s\tremaining: 56.1s\n",
      "462:\tlearn: 6.5856217\ttotal: 48.3s\tremaining: 56s\n",
      "463:\tlearn: 6.5832186\ttotal: 48.4s\tremaining: 55.9s\n",
      "464:\tlearn: 6.5796490\ttotal: 48.5s\tremaining: 55.8s\n",
      "465:\tlearn: 6.5770819\ttotal: 48.6s\tremaining: 55.7s\n",
      "466:\tlearn: 6.5743216\ttotal: 48.7s\tremaining: 55.6s\n",
      "467:\tlearn: 6.5740365\ttotal: 48.8s\tremaining: 55.4s\n",
      "468:\tlearn: 6.5696566\ttotal: 48.9s\tremaining: 55.3s\n",
      "469:\tlearn: 6.5670434\ttotal: 49s\tremaining: 55.2s\n",
      "470:\tlearn: 6.5633037\ttotal: 49.1s\tremaining: 55.1s\n",
      "471:\tlearn: 6.5620285\ttotal: 49.2s\tremaining: 55s\n",
      "472:\tlearn: 6.5579382\ttotal: 49.3s\tremaining: 54.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "473:\tlearn: 6.5548948\ttotal: 49.4s\tremaining: 54.8s\n",
      "474:\tlearn: 6.5518117\ttotal: 49.5s\tremaining: 54.7s\n",
      "475:\tlearn: 6.5481222\ttotal: 49.6s\tremaining: 54.6s\n",
      "476:\tlearn: 6.5456546\ttotal: 49.7s\tremaining: 54.5s\n",
      "477:\tlearn: 6.5423037\ttotal: 49.8s\tremaining: 54.4s\n",
      "478:\tlearn: 6.5396538\ttotal: 49.9s\tremaining: 54.3s\n",
      "479:\tlearn: 6.5380033\ttotal: 50s\tremaining: 54.2s\n",
      "480:\tlearn: 6.5351538\ttotal: 50.1s\tremaining: 54.1s\n",
      "481:\tlearn: 6.5312632\ttotal: 50.3s\tremaining: 54s\n",
      "482:\tlearn: 6.5284815\ttotal: 50.4s\tremaining: 53.9s\n",
      "483:\tlearn: 6.5243153\ttotal: 50.5s\tremaining: 53.8s\n",
      "484:\tlearn: 6.5214120\ttotal: 50.6s\tremaining: 53.7s\n",
      "485:\tlearn: 6.5169919\ttotal: 50.7s\tremaining: 53.6s\n",
      "486:\tlearn: 6.5146413\ttotal: 50.8s\tremaining: 53.5s\n",
      "487:\tlearn: 6.5114876\ttotal: 50.9s\tremaining: 53.4s\n",
      "488:\tlearn: 6.5086311\ttotal: 51s\tremaining: 53.3s\n",
      "489:\tlearn: 6.5061491\ttotal: 51.1s\tremaining: 53.2s\n",
      "490:\tlearn: 6.5041020\ttotal: 51.2s\tremaining: 53.1s\n",
      "491:\tlearn: 6.5014624\ttotal: 51.4s\tremaining: 53s\n",
      "492:\tlearn: 6.4984143\ttotal: 51.5s\tremaining: 52.9s\n",
      "493:\tlearn: 6.4954267\ttotal: 51.6s\tremaining: 52.8s\n",
      "494:\tlearn: 6.4933651\ttotal: 51.7s\tremaining: 52.7s\n",
      "495:\tlearn: 6.4911950\ttotal: 51.8s\tremaining: 52.6s\n",
      "496:\tlearn: 6.4882485\ttotal: 51.9s\tremaining: 52.5s\n",
      "497:\tlearn: 6.4853718\ttotal: 52s\tremaining: 52.4s\n",
      "498:\tlearn: 6.4820834\ttotal: 52.1s\tremaining: 52.3s\n",
      "499:\tlearn: 6.4787759\ttotal: 52.2s\tremaining: 52.2s\n",
      "500:\tlearn: 6.4770131\ttotal: 52.3s\tremaining: 52.1s\n",
      "501:\tlearn: 6.4742838\ttotal: 52.4s\tremaining: 52s\n",
      "502:\tlearn: 6.4710868\ttotal: 52.5s\tremaining: 51.9s\n",
      "503:\tlearn: 6.4672904\ttotal: 52.7s\tremaining: 51.8s\n",
      "504:\tlearn: 6.4643844\ttotal: 52.8s\tremaining: 51.7s\n",
      "505:\tlearn: 6.4610362\ttotal: 52.9s\tremaining: 51.6s\n",
      "506:\tlearn: 6.4587760\ttotal: 53s\tremaining: 51.5s\n",
      "507:\tlearn: 6.4553112\ttotal: 53.1s\tremaining: 51.4s\n",
      "508:\tlearn: 6.4529561\ttotal: 53.2s\tremaining: 51.3s\n",
      "509:\tlearn: 6.4507572\ttotal: 53.3s\tremaining: 51.2s\n",
      "510:\tlearn: 6.4471460\ttotal: 53.4s\tremaining: 51.1s\n",
      "511:\tlearn: 6.4446977\ttotal: 53.5s\tremaining: 51s\n",
      "512:\tlearn: 6.4412137\ttotal: 53.6s\tremaining: 50.9s\n",
      "513:\tlearn: 6.4387133\ttotal: 53.7s\tremaining: 50.8s\n",
      "514:\tlearn: 6.4362298\ttotal: 53.8s\tremaining: 50.7s\n",
      "515:\tlearn: 6.4342995\ttotal: 53.9s\tremaining: 50.6s\n",
      "516:\tlearn: 6.4323401\ttotal: 54s\tremaining: 50.5s\n",
      "517:\tlearn: 6.4288875\ttotal: 54.2s\tremaining: 50.4s\n",
      "518:\tlearn: 6.4267575\ttotal: 54.3s\tremaining: 50.3s\n",
      "519:\tlearn: 6.4237399\ttotal: 54.4s\tremaining: 50.2s\n",
      "520:\tlearn: 6.4202609\ttotal: 54.5s\tremaining: 50.1s\n",
      "521:\tlearn: 6.4171843\ttotal: 54.6s\tremaining: 50s\n",
      "522:\tlearn: 6.4145107\ttotal: 54.7s\tremaining: 49.9s\n",
      "523:\tlearn: 6.4113251\ttotal: 54.8s\tremaining: 49.8s\n",
      "524:\tlearn: 6.4089680\ttotal: 54.9s\tremaining: 49.7s\n",
      "525:\tlearn: 6.4060529\ttotal: 55s\tremaining: 49.6s\n",
      "526:\tlearn: 6.4038677\ttotal: 55.1s\tremaining: 49.5s\n",
      "527:\tlearn: 6.4012091\ttotal: 55.2s\tremaining: 49.4s\n",
      "528:\tlearn: 6.3989645\ttotal: 55.3s\tremaining: 49.3s\n",
      "529:\tlearn: 6.3960548\ttotal: 55.4s\tremaining: 49.2s\n",
      "530:\tlearn: 6.3940098\ttotal: 55.5s\tremaining: 49s\n",
      "531:\tlearn: 6.3909165\ttotal: 55.6s\tremaining: 48.9s\n",
      "532:\tlearn: 6.3879260\ttotal: 55.7s\tremaining: 48.8s\n",
      "533:\tlearn: 6.3860205\ttotal: 55.8s\tremaining: 48.7s\n",
      "534:\tlearn: 6.3833061\ttotal: 55.9s\tremaining: 48.6s\n",
      "535:\tlearn: 6.3793892\ttotal: 56s\tremaining: 48.5s\n",
      "536:\tlearn: 6.3769908\ttotal: 56.1s\tremaining: 48.4s\n",
      "537:\tlearn: 6.3735824\ttotal: 56.3s\tremaining: 48.3s\n",
      "538:\tlearn: 6.3705097\ttotal: 56.4s\tremaining: 48.2s\n",
      "539:\tlearn: 6.3678517\ttotal: 56.5s\tremaining: 48.1s\n",
      "540:\tlearn: 6.3655954\ttotal: 56.6s\tremaining: 48s\n",
      "541:\tlearn: 6.3643847\ttotal: 56.7s\tremaining: 47.9s\n",
      "542:\tlearn: 6.3610267\ttotal: 56.8s\tremaining: 47.8s\n",
      "543:\tlearn: 6.3574018\ttotal: 56.9s\tremaining: 47.7s\n",
      "544:\tlearn: 6.3544540\ttotal: 57s\tremaining: 47.6s\n",
      "545:\tlearn: 6.3515732\ttotal: 57.1s\tremaining: 47.5s\n",
      "546:\tlearn: 6.3486483\ttotal: 57.2s\tremaining: 47.4s\n",
      "547:\tlearn: 6.3452251\ttotal: 57.3s\tremaining: 47.3s\n",
      "548:\tlearn: 6.3423530\ttotal: 57.4s\tremaining: 47.2s\n",
      "549:\tlearn: 6.3395203\ttotal: 57.5s\tremaining: 47.1s\n",
      "550:\tlearn: 6.3392919\ttotal: 57.6s\tremaining: 47s\n",
      "551:\tlearn: 6.3361880\ttotal: 57.7s\tremaining: 46.9s\n",
      "552:\tlearn: 6.3323633\ttotal: 57.9s\tremaining: 46.8s\n",
      "553:\tlearn: 6.3290130\ttotal: 58s\tremaining: 46.7s\n",
      "554:\tlearn: 6.3262348\ttotal: 58.1s\tremaining: 46.6s\n",
      "555:\tlearn: 6.3232911\ttotal: 58.2s\tremaining: 46.5s\n",
      "556:\tlearn: 6.3205480\ttotal: 58.3s\tremaining: 46.4s\n",
      "557:\tlearn: 6.3173517\ttotal: 58.4s\tremaining: 46.2s\n",
      "558:\tlearn: 6.3140457\ttotal: 58.5s\tremaining: 46.1s\n",
      "559:\tlearn: 6.3116833\ttotal: 58.6s\tremaining: 46s\n",
      "560:\tlearn: 6.3084015\ttotal: 58.7s\tremaining: 46s\n",
      "561:\tlearn: 6.3064512\ttotal: 58.8s\tremaining: 45.9s\n",
      "562:\tlearn: 6.3059834\ttotal: 58.9s\tremaining: 45.7s\n",
      "563:\tlearn: 6.3026039\ttotal: 59.1s\tremaining: 45.7s\n",
      "564:\tlearn: 6.2995421\ttotal: 59.2s\tremaining: 45.6s\n",
      "565:\tlearn: 6.2968833\ttotal: 59.3s\tremaining: 45.4s\n",
      "566:\tlearn: 6.2940579\ttotal: 59.4s\tremaining: 45.3s\n",
      "567:\tlearn: 6.2916513\ttotal: 59.5s\tremaining: 45.2s\n",
      "568:\tlearn: 6.2884335\ttotal: 59.6s\tremaining: 45.1s\n",
      "569:\tlearn: 6.2855494\ttotal: 59.7s\tremaining: 45s\n",
      "570:\tlearn: 6.2834408\ttotal: 59.8s\tremaining: 44.9s\n",
      "571:\tlearn: 6.2814117\ttotal: 59.9s\tremaining: 44.8s\n",
      "572:\tlearn: 6.2788952\ttotal: 1m\tremaining: 44.7s\n",
      "573:\tlearn: 6.2767207\ttotal: 1m\tremaining: 44.6s\n",
      "574:\tlearn: 6.2734725\ttotal: 1m\tremaining: 44.5s\n",
      "575:\tlearn: 6.2705132\ttotal: 1m\tremaining: 44.4s\n",
      "576:\tlearn: 6.2676281\ttotal: 1m\tremaining: 44.3s\n",
      "577:\tlearn: 6.2645757\ttotal: 1m\tremaining: 44.2s\n",
      "578:\tlearn: 6.2611179\ttotal: 1m\tremaining: 44.1s\n",
      "579:\tlearn: 6.2595464\ttotal: 1m\tremaining: 44s\n",
      "580:\tlearn: 6.2562946\ttotal: 1m\tremaining: 43.9s\n",
      "581:\tlearn: 6.2530611\ttotal: 1m 1s\tremaining: 43.8s\n",
      "582:\tlearn: 6.2495398\ttotal: 1m 1s\tremaining: 43.7s\n",
      "583:\tlearn: 6.2469409\ttotal: 1m 1s\tremaining: 43.6s\n",
      "584:\tlearn: 6.2466445\ttotal: 1m 1s\tremaining: 43.5s\n",
      "585:\tlearn: 6.2464315\ttotal: 1m 1s\tremaining: 43.4s\n",
      "586:\tlearn: 6.2435662\ttotal: 1m 1s\tremaining: 43.3s\n",
      "587:\tlearn: 6.2413601\ttotal: 1m 1s\tremaining: 43.2s\n",
      "588:\tlearn: 6.2390044\ttotal: 1m 1s\tremaining: 43.1s\n",
      "589:\tlearn: 6.2368985\ttotal: 1m 1s\tremaining: 43s\n",
      "590:\tlearn: 6.2350189\ttotal: 1m 1s\tremaining: 42.9s\n",
      "591:\tlearn: 6.2326299\ttotal: 1m 2s\tremaining: 42.8s\n",
      "592:\tlearn: 6.2289235\ttotal: 1m 2s\tremaining: 42.7s\n",
      "593:\tlearn: 6.2263760\ttotal: 1m 2s\tremaining: 42.6s\n",
      "594:\tlearn: 6.2246561\ttotal: 1m 2s\tremaining: 42.5s\n",
      "595:\tlearn: 6.2227174\ttotal: 1m 2s\tremaining: 42.4s\n",
      "596:\tlearn: 6.2205439\ttotal: 1m 2s\tremaining: 42.3s\n",
      "597:\tlearn: 6.2178276\ttotal: 1m 2s\tremaining: 42.1s\n",
      "598:\tlearn: 6.2150587\ttotal: 1m 2s\tremaining: 42s\n",
      "599:\tlearn: 6.2131283\ttotal: 1m 2s\tremaining: 41.9s\n",
      "600:\tlearn: 6.2110500\ttotal: 1m 3s\tremaining: 41.8s\n",
      "601:\tlearn: 6.2090410\ttotal: 1m 3s\tremaining: 41.7s\n",
      "602:\tlearn: 6.2064371\ttotal: 1m 3s\tremaining: 41.6s\n",
      "603:\tlearn: 6.2062330\ttotal: 1m 3s\tremaining: 41.5s\n",
      "604:\tlearn: 6.2037024\ttotal: 1m 3s\tremaining: 41.4s\n",
      "605:\tlearn: 6.2003735\ttotal: 1m 3s\tremaining: 41.3s\n",
      "606:\tlearn: 6.1980979\ttotal: 1m 3s\tremaining: 41.2s\n",
      "607:\tlearn: 6.1941910\ttotal: 1m 3s\tremaining: 41.1s\n",
      "608:\tlearn: 6.1911295\ttotal: 1m 3s\tremaining: 41s\n",
      "609:\tlearn: 6.1884616\ttotal: 1m 3s\tremaining: 40.9s\n",
      "610:\tlearn: 6.1862367\ttotal: 1m 4s\tremaining: 40.8s\n",
      "611:\tlearn: 6.1826486\ttotal: 1m 4s\tremaining: 40.7s\n",
      "612:\tlearn: 6.1824504\ttotal: 1m 4s\tremaining: 40.5s\n",
      "613:\tlearn: 6.1806706\ttotal: 1m 4s\tremaining: 40.4s\n",
      "614:\tlearn: 6.1785854\ttotal: 1m 4s\tremaining: 40.3s\n",
      "615:\tlearn: 6.1751529\ttotal: 1m 4s\tremaining: 40.2s\n",
      "616:\tlearn: 6.1730595\ttotal: 1m 4s\tremaining: 40.1s\n",
      "617:\tlearn: 6.1716108\ttotal: 1m 4s\tremaining: 40s\n",
      "618:\tlearn: 6.1683135\ttotal: 1m 4s\tremaining: 39.9s\n",
      "619:\tlearn: 6.1662644\ttotal: 1m 5s\tremaining: 39.8s\n",
      "620:\tlearn: 6.1636054\ttotal: 1m 5s\tremaining: 39.7s\n",
      "621:\tlearn: 6.1609883\ttotal: 1m 5s\tremaining: 39.6s\n",
      "622:\tlearn: 6.1586761\ttotal: 1m 5s\tremaining: 39.5s\n",
      "623:\tlearn: 6.1559685\ttotal: 1m 5s\tremaining: 39.4s\n",
      "624:\tlearn: 6.1529547\ttotal: 1m 5s\tremaining: 39.3s\n",
      "625:\tlearn: 6.1512528\ttotal: 1m 5s\tremaining: 39.2s\n",
      "626:\tlearn: 6.1483917\ttotal: 1m 5s\tremaining: 39.1s\n",
      "627:\tlearn: 6.1449884\ttotal: 1m 5s\tremaining: 39s\n",
      "628:\tlearn: 6.1421196\ttotal: 1m 5s\tremaining: 38.9s\n",
      "629:\tlearn: 6.1403117\ttotal: 1m 6s\tremaining: 38.8s\n",
      "630:\tlearn: 6.1372630\ttotal: 1m 6s\tremaining: 38.7s\n",
      "631:\tlearn: 6.1354013\ttotal: 1m 6s\tremaining: 38.6s\n",
      "632:\tlearn: 6.1329831\ttotal: 1m 6s\tremaining: 38.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633:\tlearn: 6.1307557\ttotal: 1m 6s\tremaining: 38.4s\n",
      "634:\tlearn: 6.1276342\ttotal: 1m 6s\tremaining: 38.3s\n",
      "635:\tlearn: 6.1248088\ttotal: 1m 6s\tremaining: 38.2s\n",
      "636:\tlearn: 6.1212621\ttotal: 1m 6s\tremaining: 38.1s\n",
      "637:\tlearn: 6.1191905\ttotal: 1m 6s\tremaining: 38s\n",
      "638:\tlearn: 6.1189662\ttotal: 1m 7s\tremaining: 37.9s\n",
      "639:\tlearn: 6.1161239\ttotal: 1m 7s\tremaining: 37.8s\n",
      "640:\tlearn: 6.1140359\ttotal: 1m 7s\tremaining: 37.7s\n",
      "641:\tlearn: 6.1113350\ttotal: 1m 7s\tremaining: 37.6s\n",
      "642:\tlearn: 6.1097280\ttotal: 1m 7s\tremaining: 37.5s\n",
      "643:\tlearn: 6.1067085\ttotal: 1m 7s\tremaining: 37.4s\n",
      "644:\tlearn: 6.1034050\ttotal: 1m 7s\tremaining: 37.3s\n",
      "645:\tlearn: 6.0985909\ttotal: 1m 7s\tremaining: 37.2s\n",
      "646:\tlearn: 6.0941974\ttotal: 1m 7s\tremaining: 37.1s\n",
      "647:\tlearn: 6.0918354\ttotal: 1m 8s\tremaining: 37s\n",
      "648:\tlearn: 6.0898109\ttotal: 1m 8s\tremaining: 36.9s\n",
      "649:\tlearn: 6.0872011\ttotal: 1m 8s\tremaining: 36.8s\n",
      "650:\tlearn: 6.0834533\ttotal: 1m 8s\tremaining: 36.7s\n",
      "651:\tlearn: 6.0808624\ttotal: 1m 8s\tremaining: 36.6s\n",
      "652:\tlearn: 6.0783832\ttotal: 1m 8s\tremaining: 36.5s\n",
      "653:\tlearn: 6.0750437\ttotal: 1m 8s\tremaining: 36.3s\n",
      "654:\tlearn: 6.0727675\ttotal: 1m 8s\tremaining: 36.2s\n",
      "655:\tlearn: 6.0697910\ttotal: 1m 8s\tremaining: 36.1s\n",
      "656:\tlearn: 6.0674987\ttotal: 1m 9s\tremaining: 36s\n",
      "657:\tlearn: 6.0648871\ttotal: 1m 9s\tremaining: 35.9s\n",
      "658:\tlearn: 6.0617591\ttotal: 1m 9s\tremaining: 35.8s\n",
      "659:\tlearn: 6.0595437\ttotal: 1m 9s\tremaining: 35.7s\n",
      "660:\tlearn: 6.0593535\ttotal: 1m 9s\tremaining: 35.6s\n",
      "661:\tlearn: 6.0571871\ttotal: 1m 9s\tremaining: 35.5s\n",
      "662:\tlearn: 6.0549379\ttotal: 1m 9s\tremaining: 35.4s\n",
      "663:\tlearn: 6.0513402\ttotal: 1m 9s\tremaining: 35.3s\n",
      "664:\tlearn: 6.0493502\ttotal: 1m 9s\tremaining: 35.2s\n",
      "665:\tlearn: 6.0464960\ttotal: 1m 9s\tremaining: 35.1s\n",
      "666:\tlearn: 6.0438445\ttotal: 1m 10s\tremaining: 35s\n",
      "667:\tlearn: 6.0407722\ttotal: 1m 10s\tremaining: 34.9s\n",
      "668:\tlearn: 6.0383056\ttotal: 1m 10s\tremaining: 34.8s\n",
      "669:\tlearn: 6.0358783\ttotal: 1m 10s\tremaining: 34.7s\n",
      "670:\tlearn: 6.0332435\ttotal: 1m 10s\tremaining: 34.6s\n",
      "671:\tlearn: 6.0312173\ttotal: 1m 10s\tremaining: 34.5s\n",
      "672:\tlearn: 6.0284618\ttotal: 1m 10s\tremaining: 34.4s\n",
      "673:\tlearn: 6.0261287\ttotal: 1m 10s\tremaining: 34.2s\n",
      "674:\tlearn: 6.0236094\ttotal: 1m 10s\tremaining: 34.1s\n",
      "675:\tlearn: 6.0219195\ttotal: 1m 11s\tremaining: 34s\n",
      "676:\tlearn: 6.0195157\ttotal: 1m 11s\tremaining: 33.9s\n",
      "677:\tlearn: 6.0168751\ttotal: 1m 11s\tremaining: 33.8s\n",
      "678:\tlearn: 6.0145933\ttotal: 1m 11s\tremaining: 33.7s\n",
      "679:\tlearn: 6.0111494\ttotal: 1m 11s\tremaining: 33.6s\n",
      "680:\tlearn: 6.0082651\ttotal: 1m 11s\tremaining: 33.5s\n",
      "681:\tlearn: 6.0055891\ttotal: 1m 11s\tremaining: 33.4s\n",
      "682:\tlearn: 6.0019977\ttotal: 1m 11s\tremaining: 33.3s\n",
      "683:\tlearn: 5.9996622\ttotal: 1m 11s\tremaining: 33.2s\n",
      "684:\tlearn: 5.9970625\ttotal: 1m 11s\tremaining: 33.1s\n",
      "685:\tlearn: 5.9940661\ttotal: 1m 12s\tremaining: 33s\n",
      "686:\tlearn: 5.9907490\ttotal: 1m 12s\tremaining: 32.9s\n",
      "687:\tlearn: 5.9876794\ttotal: 1m 12s\tremaining: 32.8s\n",
      "688:\tlearn: 5.9858338\ttotal: 1m 12s\tremaining: 32.7s\n",
      "689:\tlearn: 5.9827660\ttotal: 1m 12s\tremaining: 32.6s\n",
      "690:\tlearn: 5.9799663\ttotal: 1m 12s\tremaining: 32.5s\n",
      "691:\tlearn: 5.9768911\ttotal: 1m 12s\tremaining: 32.4s\n",
      "692:\tlearn: 5.9736092\ttotal: 1m 12s\tremaining: 32.2s\n",
      "693:\tlearn: 5.9709206\ttotal: 1m 12s\tremaining: 32.1s\n",
      "694:\tlearn: 5.9684932\ttotal: 1m 13s\tremaining: 32s\n",
      "695:\tlearn: 5.9664373\ttotal: 1m 13s\tremaining: 31.9s\n",
      "696:\tlearn: 5.9648315\ttotal: 1m 13s\tremaining: 31.8s\n",
      "697:\tlearn: 5.9624636\ttotal: 1m 13s\tremaining: 31.7s\n",
      "698:\tlearn: 5.9594403\ttotal: 1m 13s\tremaining: 31.6s\n",
      "699:\tlearn: 5.9566776\ttotal: 1m 13s\tremaining: 31.5s\n",
      "700:\tlearn: 5.9540166\ttotal: 1m 13s\tremaining: 31.4s\n",
      "701:\tlearn: 5.9515854\ttotal: 1m 13s\tremaining: 31.3s\n",
      "702:\tlearn: 5.9487760\ttotal: 1m 13s\tremaining: 31.2s\n",
      "703:\tlearn: 5.9458351\ttotal: 1m 13s\tremaining: 31.1s\n",
      "704:\tlearn: 5.9421545\ttotal: 1m 14s\tremaining: 31s\n",
      "705:\tlearn: 5.9398497\ttotal: 1m 14s\tremaining: 30.9s\n",
      "706:\tlearn: 5.9374611\ttotal: 1m 14s\tremaining: 30.8s\n",
      "707:\tlearn: 5.9350059\ttotal: 1m 14s\tremaining: 30.7s\n",
      "708:\tlearn: 5.9304561\ttotal: 1m 14s\tremaining: 30.6s\n",
      "709:\tlearn: 5.9281402\ttotal: 1m 14s\tremaining: 30.5s\n",
      "710:\tlearn: 5.9261137\ttotal: 1m 14s\tremaining: 30.4s\n",
      "711:\tlearn: 5.9235201\ttotal: 1m 14s\tremaining: 30.3s\n",
      "712:\tlearn: 5.9218683\ttotal: 1m 14s\tremaining: 30.2s\n",
      "713:\tlearn: 5.9194726\ttotal: 1m 15s\tremaining: 30.1s\n",
      "714:\tlearn: 5.9170988\ttotal: 1m 15s\tremaining: 30s\n",
      "715:\tlearn: 5.9159206\ttotal: 1m 15s\tremaining: 29.9s\n",
      "716:\tlearn: 5.9141304\ttotal: 1m 15s\tremaining: 29.8s\n",
      "717:\tlearn: 5.9110356\ttotal: 1m 15s\tremaining: 29.7s\n",
      "718:\tlearn: 5.9079660\ttotal: 1m 15s\tremaining: 29.6s\n",
      "719:\tlearn: 5.9062861\ttotal: 1m 15s\tremaining: 29.5s\n",
      "720:\tlearn: 5.9036338\ttotal: 1m 15s\tremaining: 29.4s\n",
      "721:\tlearn: 5.9028865\ttotal: 1m 15s\tremaining: 29.2s\n",
      "722:\tlearn: 5.9000532\ttotal: 1m 16s\tremaining: 29.1s\n",
      "723:\tlearn: 5.8963423\ttotal: 1m 16s\tremaining: 29s\n",
      "724:\tlearn: 5.8944291\ttotal: 1m 16s\tremaining: 28.9s\n",
      "725:\tlearn: 5.8916845\ttotal: 1m 16s\tremaining: 28.8s\n",
      "726:\tlearn: 5.8900909\ttotal: 1m 16s\tremaining: 28.7s\n",
      "727:\tlearn: 5.8880443\ttotal: 1m 16s\tremaining: 28.6s\n",
      "728:\tlearn: 5.8843491\ttotal: 1m 16s\tremaining: 28.5s\n",
      "729:\tlearn: 5.8815563\ttotal: 1m 16s\tremaining: 28.4s\n",
      "730:\tlearn: 5.8803858\ttotal: 1m 16s\tremaining: 28.3s\n",
      "731:\tlearn: 5.8780242\ttotal: 1m 17s\tremaining: 28.2s\n",
      "732:\tlearn: 5.8749200\ttotal: 1m 17s\tremaining: 28.1s\n",
      "733:\tlearn: 5.8733056\ttotal: 1m 17s\tremaining: 28s\n",
      "734:\tlearn: 5.8709025\ttotal: 1m 17s\tremaining: 27.9s\n",
      "735:\tlearn: 5.8699151\ttotal: 1m 17s\tremaining: 27.8s\n",
      "736:\tlearn: 5.8672092\ttotal: 1m 17s\tremaining: 27.7s\n",
      "737:\tlearn: 5.8654136\ttotal: 1m 17s\tremaining: 27.6s\n",
      "738:\tlearn: 5.8632604\ttotal: 1m 17s\tremaining: 27.5s\n",
      "739:\tlearn: 5.8607995\ttotal: 1m 17s\tremaining: 27.4s\n",
      "740:\tlearn: 5.8587030\ttotal: 1m 18s\tremaining: 27.3s\n",
      "741:\tlearn: 5.8561375\ttotal: 1m 18s\tremaining: 27.2s\n",
      "742:\tlearn: 5.8531829\ttotal: 1m 18s\tremaining: 27.1s\n",
      "743:\tlearn: 5.8516871\ttotal: 1m 18s\tremaining: 26.9s\n",
      "744:\tlearn: 5.8480244\ttotal: 1m 18s\tremaining: 26.8s\n",
      "745:\tlearn: 5.8452944\ttotal: 1m 18s\tremaining: 26.7s\n",
      "746:\tlearn: 5.8440952\ttotal: 1m 18s\tremaining: 26.6s\n",
      "747:\tlearn: 5.8419746\ttotal: 1m 18s\tremaining: 26.5s\n",
      "748:\tlearn: 5.8405941\ttotal: 1m 18s\tremaining: 26.4s\n",
      "749:\tlearn: 5.8383298\ttotal: 1m 18s\tremaining: 26.3s\n",
      "750:\tlearn: 5.8358989\ttotal: 1m 19s\tremaining: 26.2s\n",
      "751:\tlearn: 5.8336121\ttotal: 1m 19s\tremaining: 26.1s\n",
      "752:\tlearn: 5.8307302\ttotal: 1m 19s\tremaining: 26s\n",
      "753:\tlearn: 5.8304885\ttotal: 1m 19s\tremaining: 25.9s\n",
      "754:\tlearn: 5.8284071\ttotal: 1m 19s\tremaining: 25.8s\n",
      "755:\tlearn: 5.8263381\ttotal: 1m 19s\tremaining: 25.7s\n",
      "756:\tlearn: 5.8237684\ttotal: 1m 19s\tremaining: 25.6s\n",
      "757:\tlearn: 5.8217784\ttotal: 1m 19s\tremaining: 25.5s\n",
      "758:\tlearn: 5.8194173\ttotal: 1m 19s\tremaining: 25.4s\n",
      "759:\tlearn: 5.8162718\ttotal: 1m 20s\tremaining: 25.3s\n",
      "760:\tlearn: 5.8141478\ttotal: 1m 20s\tremaining: 25.2s\n",
      "761:\tlearn: 5.8139808\ttotal: 1m 20s\tremaining: 25.1s\n",
      "762:\tlearn: 5.8113870\ttotal: 1m 20s\tremaining: 25s\n",
      "763:\tlearn: 5.8082755\ttotal: 1m 20s\tremaining: 24.8s\n",
      "764:\tlearn: 5.8064835\ttotal: 1m 20s\tremaining: 24.7s\n",
      "765:\tlearn: 5.8053030\ttotal: 1m 20s\tremaining: 24.6s\n",
      "766:\tlearn: 5.8024112\ttotal: 1m 20s\tremaining: 24.5s\n",
      "767:\tlearn: 5.7994745\ttotal: 1m 20s\tremaining: 24.4s\n",
      "768:\tlearn: 5.7969680\ttotal: 1m 20s\tremaining: 24.3s\n",
      "769:\tlearn: 5.7939671\ttotal: 1m 21s\tremaining: 24.2s\n",
      "770:\tlearn: 5.7920202\ttotal: 1m 21s\tremaining: 24.1s\n",
      "771:\tlearn: 5.7892769\ttotal: 1m 21s\tremaining: 24s\n",
      "772:\tlearn: 5.7865398\ttotal: 1m 21s\tremaining: 23.9s\n",
      "773:\tlearn: 5.7842487\ttotal: 1m 21s\tremaining: 23.8s\n",
      "774:\tlearn: 5.7813431\ttotal: 1m 21s\tremaining: 23.7s\n",
      "775:\tlearn: 5.7798921\ttotal: 1m 21s\tremaining: 23.6s\n",
      "776:\tlearn: 5.7775951\ttotal: 1m 21s\tremaining: 23.5s\n",
      "777:\tlearn: 5.7757771\ttotal: 1m 21s\tremaining: 23.4s\n",
      "778:\tlearn: 5.7733794\ttotal: 1m 22s\tremaining: 23.3s\n",
      "779:\tlearn: 5.7711741\ttotal: 1m 22s\tremaining: 23.2s\n",
      "780:\tlearn: 5.7690261\ttotal: 1m 22s\tremaining: 23.1s\n",
      "781:\tlearn: 5.7671705\ttotal: 1m 22s\tremaining: 23s\n",
      "782:\tlearn: 5.7646984\ttotal: 1m 22s\tremaining: 22.8s\n",
      "783:\tlearn: 5.7623643\ttotal: 1m 22s\tremaining: 22.7s\n",
      "784:\tlearn: 5.7600071\ttotal: 1m 22s\tremaining: 22.6s\n",
      "785:\tlearn: 5.7574436\ttotal: 1m 22s\tremaining: 22.5s\n",
      "786:\tlearn: 5.7548517\ttotal: 1m 22s\tremaining: 22.4s\n",
      "787:\tlearn: 5.7526005\ttotal: 1m 22s\tremaining: 22.3s\n",
      "788:\tlearn: 5.7505846\ttotal: 1m 23s\tremaining: 22.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789:\tlearn: 5.7480012\ttotal: 1m 23s\tremaining: 22.1s\n",
      "790:\tlearn: 5.7458688\ttotal: 1m 23s\tremaining: 22s\n",
      "791:\tlearn: 5.7439989\ttotal: 1m 23s\tremaining: 21.9s\n",
      "792:\tlearn: 5.7410715\ttotal: 1m 23s\tremaining: 21.8s\n",
      "793:\tlearn: 5.7392818\ttotal: 1m 23s\tremaining: 21.7s\n",
      "794:\tlearn: 5.7371238\ttotal: 1m 23s\tremaining: 21.6s\n",
      "795:\tlearn: 5.7348656\ttotal: 1m 23s\tremaining: 21.5s\n",
      "796:\tlearn: 5.7323028\ttotal: 1m 23s\tremaining: 21.4s\n",
      "797:\tlearn: 5.7295349\ttotal: 1m 24s\tremaining: 21.3s\n",
      "798:\tlearn: 5.7281079\ttotal: 1m 24s\tremaining: 21.2s\n",
      "799:\tlearn: 5.7254232\ttotal: 1m 24s\tremaining: 21.1s\n",
      "800:\tlearn: 5.7216037\ttotal: 1m 24s\tremaining: 21s\n",
      "801:\tlearn: 5.7191971\ttotal: 1m 24s\tremaining: 20.9s\n",
      "802:\tlearn: 5.7155905\ttotal: 1m 24s\tremaining: 20.8s\n",
      "803:\tlearn: 5.7132690\ttotal: 1m 24s\tremaining: 20.7s\n",
      "804:\tlearn: 5.7107092\ttotal: 1m 24s\tremaining: 20.6s\n",
      "805:\tlearn: 5.7083244\ttotal: 1m 24s\tremaining: 20.4s\n",
      "806:\tlearn: 5.7062749\ttotal: 1m 25s\tremaining: 20.3s\n",
      "807:\tlearn: 5.7036144\ttotal: 1m 25s\tremaining: 20.2s\n",
      "808:\tlearn: 5.7012916\ttotal: 1m 25s\tremaining: 20.1s\n",
      "809:\tlearn: 5.6981460\ttotal: 1m 25s\tremaining: 20s\n",
      "810:\tlearn: 5.6955714\ttotal: 1m 25s\tremaining: 19.9s\n",
      "811:\tlearn: 5.6932480\ttotal: 1m 25s\tremaining: 19.8s\n",
      "812:\tlearn: 5.6911425\ttotal: 1m 25s\tremaining: 19.7s\n",
      "813:\tlearn: 5.6885253\ttotal: 1m 25s\tremaining: 19.6s\n",
      "814:\tlearn: 5.6875290\ttotal: 1m 25s\tremaining: 19.5s\n",
      "815:\tlearn: 5.6843383\ttotal: 1m 26s\tremaining: 19.4s\n",
      "816:\tlearn: 5.6817426\ttotal: 1m 26s\tremaining: 19.3s\n",
      "817:\tlearn: 5.6790650\ttotal: 1m 26s\tremaining: 19.2s\n",
      "818:\tlearn: 5.6757898\ttotal: 1m 26s\tremaining: 19.1s\n",
      "819:\tlearn: 5.6725647\ttotal: 1m 26s\tremaining: 19s\n",
      "820:\tlearn: 5.6699588\ttotal: 1m 26s\tremaining: 18.9s\n",
      "821:\tlearn: 5.6678691\ttotal: 1m 26s\tremaining: 18.8s\n",
      "822:\tlearn: 5.6651812\ttotal: 1m 26s\tremaining: 18.7s\n",
      "823:\tlearn: 5.6627631\ttotal: 1m 26s\tremaining: 18.6s\n",
      "824:\tlearn: 5.6600257\ttotal: 1m 26s\tremaining: 18.4s\n",
      "825:\tlearn: 5.6597676\ttotal: 1m 27s\tremaining: 18.3s\n",
      "826:\tlearn: 5.6571371\ttotal: 1m 27s\tremaining: 18.2s\n",
      "827:\tlearn: 5.6545378\ttotal: 1m 27s\tremaining: 18.1s\n",
      "828:\tlearn: 5.6527405\ttotal: 1m 27s\tremaining: 18s\n",
      "829:\tlearn: 5.6517528\ttotal: 1m 27s\tremaining: 17.9s\n",
      "830:\tlearn: 5.6495359\ttotal: 1m 27s\tremaining: 17.8s\n",
      "831:\tlearn: 5.6472677\ttotal: 1m 27s\tremaining: 17.7s\n",
      "832:\tlearn: 5.6444004\ttotal: 1m 27s\tremaining: 17.6s\n",
      "833:\tlearn: 5.6420071\ttotal: 1m 27s\tremaining: 17.5s\n",
      "834:\tlearn: 5.6400573\ttotal: 1m 28s\tremaining: 17.4s\n",
      "835:\tlearn: 5.6374378\ttotal: 1m 28s\tremaining: 17.3s\n",
      "836:\tlearn: 5.6355895\ttotal: 1m 28s\tremaining: 17.2s\n",
      "837:\tlearn: 5.6328814\ttotal: 1m 28s\tremaining: 17.1s\n",
      "838:\tlearn: 5.6304736\ttotal: 1m 28s\tremaining: 17s\n",
      "839:\tlearn: 5.6273856\ttotal: 1m 28s\tremaining: 16.9s\n",
      "840:\tlearn: 5.6246724\ttotal: 1m 28s\tremaining: 16.8s\n",
      "841:\tlearn: 5.6222431\ttotal: 1m 28s\tremaining: 16.7s\n",
      "842:\tlearn: 5.6195877\ttotal: 1m 28s\tremaining: 16.6s\n",
      "843:\tlearn: 5.6163501\ttotal: 1m 29s\tremaining: 16.5s\n",
      "844:\tlearn: 5.6134654\ttotal: 1m 29s\tremaining: 16.3s\n",
      "845:\tlearn: 5.6114317\ttotal: 1m 29s\tremaining: 16.2s\n",
      "846:\tlearn: 5.6088843\ttotal: 1m 29s\tremaining: 16.1s\n",
      "847:\tlearn: 5.6073607\ttotal: 1m 29s\tremaining: 16s\n",
      "848:\tlearn: 5.6039551\ttotal: 1m 29s\tremaining: 15.9s\n",
      "849:\tlearn: 5.6019379\ttotal: 1m 29s\tremaining: 15.8s\n",
      "850:\tlearn: 5.5991278\ttotal: 1m 29s\tremaining: 15.7s\n",
      "851:\tlearn: 5.5976108\ttotal: 1m 29s\tremaining: 15.6s\n",
      "852:\tlearn: 5.5951462\ttotal: 1m 29s\tremaining: 15.5s\n",
      "853:\tlearn: 5.5921258\ttotal: 1m 30s\tremaining: 15.4s\n",
      "854:\tlearn: 5.5892851\ttotal: 1m 30s\tremaining: 15.3s\n",
      "855:\tlearn: 5.5868265\ttotal: 1m 30s\tremaining: 15.2s\n",
      "856:\tlearn: 5.5845048\ttotal: 1m 30s\tremaining: 15.1s\n",
      "857:\tlearn: 5.5827608\ttotal: 1m 30s\tremaining: 15s\n",
      "858:\tlearn: 5.5800930\ttotal: 1m 30s\tremaining: 14.9s\n",
      "859:\tlearn: 5.5777057\ttotal: 1m 30s\tremaining: 14.8s\n",
      "860:\tlearn: 5.5754670\ttotal: 1m 30s\tremaining: 14.7s\n",
      "861:\tlearn: 5.5732567\ttotal: 1m 30s\tremaining: 14.6s\n",
      "862:\tlearn: 5.5692497\ttotal: 1m 31s\tremaining: 14.5s\n",
      "863:\tlearn: 5.5664657\ttotal: 1m 31s\tremaining: 14.3s\n",
      "864:\tlearn: 5.5640983\ttotal: 1m 31s\tremaining: 14.2s\n",
      "865:\tlearn: 5.5601542\ttotal: 1m 31s\tremaining: 14.1s\n",
      "866:\tlearn: 5.5573624\ttotal: 1m 31s\tremaining: 14s\n",
      "867:\tlearn: 5.5554640\ttotal: 1m 31s\tremaining: 13.9s\n",
      "868:\tlearn: 5.5536691\ttotal: 1m 31s\tremaining: 13.8s\n",
      "869:\tlearn: 5.5512926\ttotal: 1m 31s\tremaining: 13.7s\n",
      "870:\tlearn: 5.5490646\ttotal: 1m 31s\tremaining: 13.6s\n",
      "871:\tlearn: 5.5464262\ttotal: 1m 32s\tremaining: 13.5s\n",
      "872:\tlearn: 5.5434148\ttotal: 1m 32s\tremaining: 13.4s\n",
      "873:\tlearn: 5.5409307\ttotal: 1m 32s\tremaining: 13.3s\n",
      "874:\tlearn: 5.5387192\ttotal: 1m 32s\tremaining: 13.2s\n",
      "875:\tlearn: 5.5361911\ttotal: 1m 32s\tremaining: 13.1s\n",
      "876:\tlearn: 5.5337358\ttotal: 1m 32s\tremaining: 13s\n",
      "877:\tlearn: 5.5298330\ttotal: 1m 32s\tremaining: 12.9s\n",
      "878:\tlearn: 5.5274073\ttotal: 1m 32s\tremaining: 12.8s\n",
      "879:\tlearn: 5.5253374\ttotal: 1m 32s\tremaining: 12.7s\n",
      "880:\tlearn: 5.5235904\ttotal: 1m 32s\tremaining: 12.6s\n",
      "881:\tlearn: 5.5216694\ttotal: 1m 33s\tremaining: 12.5s\n",
      "882:\tlearn: 5.5188931\ttotal: 1m 33s\tremaining: 12.3s\n",
      "883:\tlearn: 5.5155298\ttotal: 1m 33s\tremaining: 12.2s\n",
      "884:\tlearn: 5.5123475\ttotal: 1m 33s\tremaining: 12.1s\n",
      "885:\tlearn: 5.5121928\ttotal: 1m 33s\tremaining: 12s\n",
      "886:\tlearn: 5.5101568\ttotal: 1m 33s\tremaining: 11.9s\n",
      "887:\tlearn: 5.5076933\ttotal: 1m 33s\tremaining: 11.8s\n",
      "888:\tlearn: 5.5047622\ttotal: 1m 33s\tremaining: 11.7s\n",
      "889:\tlearn: 5.5023863\ttotal: 1m 33s\tremaining: 11.6s\n",
      "890:\tlearn: 5.5003527\ttotal: 1m 34s\tremaining: 11.5s\n",
      "891:\tlearn: 5.4984098\ttotal: 1m 34s\tremaining: 11.4s\n",
      "892:\tlearn: 5.4953417\ttotal: 1m 34s\tremaining: 11.3s\n",
      "893:\tlearn: 5.4933426\ttotal: 1m 34s\tremaining: 11.2s\n",
      "894:\tlearn: 5.4905659\ttotal: 1m 34s\tremaining: 11.1s\n",
      "895:\tlearn: 5.4879943\ttotal: 1m 34s\tremaining: 11s\n",
      "896:\tlearn: 5.4863045\ttotal: 1m 34s\tremaining: 10.9s\n",
      "897:\tlearn: 5.4845708\ttotal: 1m 34s\tremaining: 10.8s\n",
      "898:\tlearn: 5.4821120\ttotal: 1m 34s\tremaining: 10.7s\n",
      "899:\tlearn: 5.4797995\ttotal: 1m 34s\tremaining: 10.5s\n",
      "900:\tlearn: 5.4763273\ttotal: 1m 35s\tremaining: 10.4s\n",
      "901:\tlearn: 5.4741807\ttotal: 1m 35s\tremaining: 10.3s\n",
      "902:\tlearn: 5.4724825\ttotal: 1m 35s\tremaining: 10.2s\n",
      "903:\tlearn: 5.4701268\ttotal: 1m 35s\tremaining: 10.1s\n",
      "904:\tlearn: 5.4684166\ttotal: 1m 35s\tremaining: 10s\n",
      "905:\tlearn: 5.4656656\ttotal: 1m 35s\tremaining: 9.91s\n",
      "906:\tlearn: 5.4639493\ttotal: 1m 35s\tremaining: 9.8s\n",
      "907:\tlearn: 5.4626278\ttotal: 1m 35s\tremaining: 9.7s\n",
      "908:\tlearn: 5.4602617\ttotal: 1m 35s\tremaining: 9.59s\n",
      "909:\tlearn: 5.4573904\ttotal: 1m 35s\tremaining: 9.49s\n",
      "910:\tlearn: 5.4550502\ttotal: 1m 36s\tremaining: 9.38s\n",
      "911:\tlearn: 5.4525468\ttotal: 1m 36s\tremaining: 9.28s\n",
      "912:\tlearn: 5.4513117\ttotal: 1m 36s\tremaining: 9.17s\n",
      "913:\tlearn: 5.4489556\ttotal: 1m 36s\tremaining: 9.07s\n",
      "914:\tlearn: 5.4470894\ttotal: 1m 36s\tremaining: 8.96s\n",
      "915:\tlearn: 5.4446595\ttotal: 1m 36s\tremaining: 8.86s\n",
      "916:\tlearn: 5.4424031\ttotal: 1m 36s\tremaining: 8.75s\n",
      "917:\tlearn: 5.4404699\ttotal: 1m 36s\tremaining: 8.64s\n",
      "918:\tlearn: 5.4386413\ttotal: 1m 36s\tremaining: 8.54s\n",
      "919:\tlearn: 5.4365558\ttotal: 1m 36s\tremaining: 8.43s\n",
      "920:\tlearn: 5.4346504\ttotal: 1m 37s\tremaining: 8.32s\n",
      "921:\tlearn: 5.4321053\ttotal: 1m 37s\tremaining: 8.22s\n",
      "922:\tlearn: 5.4299168\ttotal: 1m 37s\tremaining: 8.12s\n",
      "923:\tlearn: 5.4279633\ttotal: 1m 37s\tremaining: 8.01s\n",
      "924:\tlearn: 5.4249962\ttotal: 1m 37s\tremaining: 7.9s\n",
      "925:\tlearn: 5.4229504\ttotal: 1m 37s\tremaining: 7.8s\n",
      "926:\tlearn: 5.4205488\ttotal: 1m 37s\tremaining: 7.69s\n",
      "927:\tlearn: 5.4185971\ttotal: 1m 37s\tremaining: 7.59s\n",
      "928:\tlearn: 5.4161213\ttotal: 1m 37s\tremaining: 7.48s\n",
      "929:\tlearn: 5.4132366\ttotal: 1m 38s\tremaining: 7.38s\n",
      "930:\tlearn: 5.4108565\ttotal: 1m 38s\tremaining: 7.27s\n",
      "931:\tlearn: 5.4084361\ttotal: 1m 38s\tremaining: 7.17s\n",
      "932:\tlearn: 5.4067388\ttotal: 1m 38s\tremaining: 7.06s\n",
      "933:\tlearn: 5.4040729\ttotal: 1m 38s\tremaining: 6.95s\n",
      "934:\tlearn: 5.4038881\ttotal: 1m 38s\tremaining: 6.85s\n",
      "935:\tlearn: 5.4018250\ttotal: 1m 38s\tremaining: 6.74s\n",
      "936:\tlearn: 5.3997263\ttotal: 1m 38s\tremaining: 6.63s\n",
      "937:\tlearn: 5.3979053\ttotal: 1m 38s\tremaining: 6.53s\n",
      "938:\tlearn: 5.3960025\ttotal: 1m 38s\tremaining: 6.42s\n",
      "939:\tlearn: 5.3935990\ttotal: 1m 38s\tremaining: 6.32s\n",
      "940:\tlearn: 5.3921309\ttotal: 1m 39s\tremaining: 6.21s\n",
      "941:\tlearn: 5.3905080\ttotal: 1m 39s\tremaining: 6.11s\n",
      "942:\tlearn: 5.3881000\ttotal: 1m 39s\tremaining: 6s\n",
      "943:\tlearn: 5.3850534\ttotal: 1m 39s\tremaining: 5.89s\n",
      "944:\tlearn: 5.3828497\ttotal: 1m 39s\tremaining: 5.79s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945:\tlearn: 5.3797756\ttotal: 1m 39s\tremaining: 5.68s\n",
      "946:\tlearn: 5.3772058\ttotal: 1m 39s\tremaining: 5.58s\n",
      "947:\tlearn: 5.3740586\ttotal: 1m 39s\tremaining: 5.47s\n",
      "948:\tlearn: 5.3726741\ttotal: 1m 39s\tremaining: 5.37s\n",
      "949:\tlearn: 5.3708495\ttotal: 1m 39s\tremaining: 5.26s\n",
      "950:\tlearn: 5.3689764\ttotal: 1m 40s\tremaining: 5.16s\n",
      "951:\tlearn: 5.3666822\ttotal: 1m 40s\tremaining: 5.05s\n",
      "952:\tlearn: 5.3652571\ttotal: 1m 40s\tremaining: 4.95s\n",
      "953:\tlearn: 5.3633997\ttotal: 1m 40s\tremaining: 4.84s\n",
      "954:\tlearn: 5.3612523\ttotal: 1m 40s\tremaining: 4.73s\n",
      "955:\tlearn: 5.3591457\ttotal: 1m 40s\tremaining: 4.63s\n",
      "956:\tlearn: 5.3565540\ttotal: 1m 40s\tremaining: 4.52s\n",
      "957:\tlearn: 5.3530074\ttotal: 1m 40s\tremaining: 4.42s\n",
      "958:\tlearn: 5.3507986\ttotal: 1m 40s\tremaining: 4.31s\n",
      "959:\tlearn: 5.3484256\ttotal: 1m 40s\tremaining: 4.21s\n",
      "960:\tlearn: 5.3459785\ttotal: 1m 41s\tremaining: 4.1s\n",
      "961:\tlearn: 5.3432129\ttotal: 1m 41s\tremaining: 3.99s\n",
      "962:\tlearn: 5.3401960\ttotal: 1m 41s\tremaining: 3.89s\n",
      "963:\tlearn: 5.3379702\ttotal: 1m 41s\tremaining: 3.78s\n",
      "964:\tlearn: 5.3369901\ttotal: 1m 41s\tremaining: 3.68s\n",
      "965:\tlearn: 5.3352915\ttotal: 1m 41s\tremaining: 3.57s\n",
      "966:\tlearn: 5.3326511\ttotal: 1m 41s\tremaining: 3.47s\n",
      "967:\tlearn: 5.3300271\ttotal: 1m 41s\tremaining: 3.36s\n",
      "968:\tlearn: 5.3279598\ttotal: 1m 41s\tremaining: 3.26s\n",
      "969:\tlearn: 5.3252954\ttotal: 1m 41s\tremaining: 3.15s\n",
      "970:\tlearn: 5.3225977\ttotal: 1m 41s\tremaining: 3.04s\n",
      "971:\tlearn: 5.3205637\ttotal: 1m 42s\tremaining: 2.94s\n",
      "972:\tlearn: 5.3183495\ttotal: 1m 42s\tremaining: 2.83s\n",
      "973:\tlearn: 5.3158292\ttotal: 1m 42s\tremaining: 2.73s\n",
      "974:\tlearn: 5.3142131\ttotal: 1m 42s\tremaining: 2.62s\n",
      "975:\tlearn: 5.3127362\ttotal: 1m 42s\tremaining: 2.52s\n",
      "976:\tlearn: 5.3102560\ttotal: 1m 42s\tremaining: 2.41s\n",
      "977:\tlearn: 5.3083642\ttotal: 1m 42s\tremaining: 2.31s\n",
      "978:\tlearn: 5.3052803\ttotal: 1m 42s\tremaining: 2.2s\n",
      "979:\tlearn: 5.3033341\ttotal: 1m 42s\tremaining: 2.1s\n",
      "980:\tlearn: 5.3014952\ttotal: 1m 42s\tremaining: 1.99s\n",
      "981:\tlearn: 5.2995389\ttotal: 1m 43s\tremaining: 1.89s\n",
      "982:\tlearn: 5.2972275\ttotal: 1m 43s\tremaining: 1.78s\n",
      "983:\tlearn: 5.2953337\ttotal: 1m 43s\tremaining: 1.68s\n",
      "984:\tlearn: 5.2936872\ttotal: 1m 43s\tremaining: 1.57s\n",
      "985:\tlearn: 5.2911664\ttotal: 1m 43s\tremaining: 1.47s\n",
      "986:\tlearn: 5.2895757\ttotal: 1m 43s\tremaining: 1.36s\n",
      "987:\tlearn: 5.2880040\ttotal: 1m 43s\tremaining: 1.26s\n",
      "988:\tlearn: 5.2862769\ttotal: 1m 43s\tremaining: 1.15s\n",
      "989:\tlearn: 5.2841181\ttotal: 1m 43s\tremaining: 1.05s\n",
      "990:\tlearn: 5.2815543\ttotal: 1m 43s\tremaining: 944ms\n",
      "991:\tlearn: 5.2802680\ttotal: 1m 44s\tremaining: 839ms\n",
      "992:\tlearn: 5.2780646\ttotal: 1m 44s\tremaining: 734ms\n",
      "993:\tlearn: 5.2753312\ttotal: 1m 44s\tremaining: 629ms\n",
      "994:\tlearn: 5.2730253\ttotal: 1m 44s\tremaining: 524ms\n",
      "995:\tlearn: 5.2717764\ttotal: 1m 44s\tremaining: 419ms\n",
      "996:\tlearn: 5.2701901\ttotal: 1m 44s\tremaining: 314ms\n",
      "997:\tlearn: 5.2688408\ttotal: 1m 44s\tremaining: 210ms\n",
      "998:\tlearn: 5.2673270\ttotal: 1m 44s\tremaining: 105ms\n",
      "999:\tlearn: 5.2645614\ttotal: 1m 44s\tremaining: 0us\n",
      "LGBMRegressor 로그 변환된 RMSE: 8.082\n",
      "CatBoostRegressor 로그 변환된 RMSE: 8.035\n",
      "XGBRegressor 로그 변환된 RMSE: 8.556\n",
      "Ridge 로그 변환된 RMSE: 8.488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.082221840096834, 8.035338980509017, 8.555654554119082, 8.487651250878368]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_rmse(model):\n",
    "    pred = model.predict(valid_x)\n",
    "    mse = mean_squared_error(valid_y , pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print('{0} 로그 변환된 RMSE: {1}'.format(model.__class__.__name__,np.round(rmse, 3)))\n",
    "    return rmse\n",
    "\n",
    "def get_rmses(models):\n",
    "    rmses = [ ]\n",
    "    for model in models:\n",
    "        rmse = get_rmse(model)\n",
    "        rmses.append(rmse)\n",
    "    return rmses\n",
    "\n",
    "\n",
    "\n",
    "lgbm = LGBMRegressor(random_state = 1000  )\n",
    "lgbm = lgbm.fit(train_x , train_y)\n",
    "\n",
    "cat = CatBoostRegressor(random_state=1000 )\n",
    "cat = cat.fit(train_x , train_y)\n",
    "\n",
    "xgb = XGBRegressor(random_state = 1000 )\n",
    "xgb.fit(train_x , train_y )\n",
    "\n",
    "reg_ridge = Ridge(random_state = 1000)\n",
    "reg_ridge.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "models = [lgbm, cat, xgb, reg_ridge ]\n",
    "get_rmses(models)\n",
    "\n",
    "# 8.082221840096834, 8.035338980509017, 8.555654554119082, 8.487651250878368]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_params = {\n",
    "    'max_depth':(8, 16),\n",
    "    'num_leaves':(24, 64),\n",
    "    'min_child_samples':(10, 200),\n",
    "    'min_child_weight':(1, 50),\n",
    "    'subsample':(0.5, 1),\n",
    "    'colsample_bytree':(0.5, 1),\n",
    "    'max_bin':(10, 500),\n",
    "    'reg_lambda':(0.001, 10),\n",
    "    'reg_alpha':(0.01, 50)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_rmse_eval(max_depth, num_leaves, min_child_samples, min_child_weight, subsample, \n",
    "                colsample_bytree, max_bin, reg_lambda, reg_alpha):\n",
    "    \n",
    "    params = {\n",
    "        \"n_estimators\":2000, \n",
    "        \"learning_rate\":0.02,\n",
    "        'max_depth':int(round(max_depth)),\n",
    "        'num_leaves':int(round(num_leaves)),\n",
    "        'min_child_samples': int(round(min_child_samples)),\n",
    "        'min_child_weight': int(round(min_child_weight)),\n",
    "        'subsample':max(min(subsample, 1), 0),\n",
    "        'colsample_bytree':max(min(colsample_bytree, 1), 0),\n",
    "        'reg_lambda': max(reg_lambda,0),\n",
    "        'reg_alpha': max(reg_alpha, 0)\n",
    "    }\n",
    "    \n",
    "    lgb_model = LGBMRegressor(**params)\n",
    "    lgb_model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'RMSE', verbose= 100, \n",
    "                early_stopping_rounds= 100)\n",
    "    valid_pred = lgb_model.predict(valid_x)\n",
    "    RMSE = np.sqrt(mean_squared_error(valid_y, valid_pred))\n",
    "    \n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... |  max_bin  | max_depth | min_ch... | min_ch... | num_le... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.82935\ttraining's l2: 61.2987\tvalid_1's rmse: 8.30778\tvalid_1's l2: 69.0192\n",
      "[200]\ttraining's rmse: 7.15116\ttraining's l2: 51.1391\tvalid_1's rmse: 8.08957\tvalid_1's l2: 65.4412\n",
      "[300]\ttraining's rmse: 6.69638\ttraining's l2: 44.8415\tvalid_1's rmse: 8.03895\tvalid_1's l2: 64.6247\n",
      "[400]\ttraining's rmse: 6.33075\ttraining's l2: 40.0784\tvalid_1's rmse: 8.01757\tvalid_1's l2: 64.2814\n",
      "[500]\ttraining's rmse: 6.01426\ttraining's l2: 36.1713\tvalid_1's rmse: 8.0095\tvalid_1's l2: 64.1521\n",
      "[600]\ttraining's rmse: 5.73295\ttraining's l2: 32.8667\tvalid_1's rmse: 8.00917\tvalid_1's l2: 64.1467\n",
      "Early stopping, best iteration is:\n",
      "[537]\ttraining's rmse: 5.90722\ttraining's l2: 34.8953\tvalid_1's rmse: 8.00649\tvalid_1's l2: 64.1039\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 8.006   \u001b[0m | \u001b[0m 0.8268  \u001b[0m | \u001b[0m 66.35   \u001b[0m | \u001b[0m 15.6    \u001b[0m | \u001b[0m 101.6   \u001b[0m | \u001b[0m 43.75   \u001b[0m | \u001b[0m 32.49   \u001b[0m | \u001b[0m 2.045   \u001b[0m | \u001b[0m 3.973   \u001b[0m | \u001b[0m 0.6166  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.4918\ttraining's l2: 56.127\tvalid_1's rmse: 8.26923\tvalid_1's l2: 68.3801\n",
      "[200]\ttraining's rmse: 6.60297\ttraining's l2: 43.5992\tvalid_1's rmse: 8.07595\tvalid_1's l2: 65.221\n",
      "[300]\ttraining's rmse: 6.00687\ttraining's l2: 36.0825\tvalid_1's rmse: 8.03433\tvalid_1's l2: 64.5505\n",
      "[400]\ttraining's rmse: 5.51532\ttraining's l2: 30.4187\tvalid_1's rmse: 8.02005\tvalid_1's l2: 64.3212\n",
      "[500]\ttraining's rmse: 5.11524\ttraining's l2: 26.1657\tvalid_1's rmse: 8.01972\tvalid_1's l2: 64.3159\n",
      "[600]\ttraining's rmse: 4.75608\ttraining's l2: 22.6203\tvalid_1's rmse: 8.01584\tvalid_1's l2: 64.2537\n",
      "[700]\ttraining's rmse: 4.43045\ttraining's l2: 19.6289\tvalid_1's rmse: 8.01535\tvalid_1's l2: 64.2459\n",
      "Early stopping, best iteration is:\n",
      "[642]\ttraining's rmse: 4.61255\ttraining's l2: 21.2756\tvalid_1's rmse: 8.01431\tvalid_1's l2: 64.2292\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 8.014   \u001b[0m | \u001b[95m 0.9209  \u001b[0m | \u001b[95m 111.5   \u001b[0m | \u001b[95m 13.94   \u001b[0m | \u001b[95m 84.51   \u001b[0m | \u001b[95m 9.931   \u001b[0m | \u001b[95m 53.74   \u001b[0m | \u001b[95m 3.488   \u001b[0m | \u001b[95m 8.853   \u001b[0m | \u001b[95m 0.9763  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.70882\ttraining's l2: 59.4258\tvalid_1's rmse: 8.28565\tvalid_1's l2: 68.652\n",
      "[200]\ttraining's rmse: 7.10125\ttraining's l2: 50.4277\tvalid_1's rmse: 8.08218\tvalid_1's l2: 65.3216\n",
      "[300]\ttraining's rmse: 6.76918\ttraining's l2: 45.8218\tvalid_1's rmse: 8.03611\tvalid_1's l2: 64.5791\n",
      "[400]\ttraining's rmse: 6.50598\ttraining's l2: 42.3277\tvalid_1's rmse: 8.01804\tvalid_1's l2: 64.2889\n",
      "[500]\ttraining's rmse: 6.26733\ttraining's l2: 39.2794\tvalid_1's rmse: 8.00772\tvalid_1's l2: 64.1236\n",
      "[600]\ttraining's rmse: 6.04528\ttraining's l2: 36.5454\tvalid_1's rmse: 8.00033\tvalid_1's l2: 64.0052\n",
      "[700]\ttraining's rmse: 5.8313\ttraining's l2: 34.004\tvalid_1's rmse: 8.00062\tvalid_1's l2: 64.0099\n",
      "Early stopping, best iteration is:\n",
      "[645]\ttraining's rmse: 5.94963\ttraining's l2: 35.3981\tvalid_1's rmse: 7.99809\tvalid_1's l2: 63.9695\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 7.998   \u001b[0m | \u001b[0m 0.9656  \u001b[0m | \u001b[0m 213.6   \u001b[0m | \u001b[0m 8.232   \u001b[0m | \u001b[0m 196.6   \u001b[0m | \u001b[0m 17.64   \u001b[0m | \u001b[0m 52.27   \u001b[0m | \u001b[0m 18.1    \u001b[0m | \u001b[0m 0.352   \u001b[0m | \u001b[0m 0.9275  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.98671\ttraining's l2: 63.7876\tvalid_1's rmse: 8.33354\tvalid_1's l2: 69.4478\n",
      "[200]\ttraining's rmse: 7.41489\ttraining's l2: 54.9806\tvalid_1's rmse: 8.09941\tvalid_1's l2: 65.6005\n",
      "[300]\ttraining's rmse: 7.05765\ttraining's l2: 49.8104\tvalid_1's rmse: 8.04087\tvalid_1's l2: 64.6556\n",
      "[400]\ttraining's rmse: 6.77029\ttraining's l2: 45.8369\tvalid_1's rmse: 8.01484\tvalid_1's l2: 64.2376\n",
      "[500]\ttraining's rmse: 6.51843\ttraining's l2: 42.4899\tvalid_1's rmse: 8.00153\tvalid_1's l2: 64.0246\n",
      "[600]\ttraining's rmse: 6.29729\ttraining's l2: 39.6558\tvalid_1's rmse: 7.99291\tvalid_1's l2: 63.8867\n",
      "[700]\ttraining's rmse: 6.08992\ttraining's l2: 37.0871\tvalid_1's rmse: 7.99601\tvalid_1's l2: 63.9362\n",
      "Early stopping, best iteration is:\n",
      "[607]\ttraining's rmse: 6.28188\ttraining's l2: 39.462\tvalid_1's rmse: 7.99214\tvalid_1's l2: 63.8743\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 7.992   \u001b[0m | \u001b[0m 0.8286  \u001b[0m | \u001b[0m 385.2   \u001b[0m | \u001b[0m 12.43   \u001b[0m | \u001b[0m 178.2   \u001b[0m | \u001b[0m 45.31   \u001b[0m | \u001b[0m 24.42   \u001b[0m | \u001b[0m 3.737   \u001b[0m | \u001b[0m 2.447   \u001b[0m | \u001b[0m 0.5667  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.96353\ttraining's l2: 63.4178\tvalid_1's rmse: 8.34464\tvalid_1's l2: 69.633\n",
      "[200]\ttraining's rmse: 7.3597\ttraining's l2: 54.1652\tvalid_1's rmse: 8.10667\tvalid_1's l2: 65.7181\n",
      "[300]\ttraining's rmse: 6.97293\ttraining's l2: 48.6217\tvalid_1's rmse: 8.0521\tvalid_1's l2: 64.8363\n",
      "[400]\ttraining's rmse: 6.65838\ttraining's l2: 44.334\tvalid_1's rmse: 8.03063\tvalid_1's l2: 64.491\n",
      "[500]\ttraining's rmse: 6.38636\ttraining's l2: 40.7857\tvalid_1's rmse: 8.01708\tvalid_1's l2: 64.2735\n",
      "[600]\ttraining's rmse: 6.14425\ttraining's l2: 37.7518\tvalid_1's rmse: 8.00962\tvalid_1's l2: 64.154\n",
      "[700]\ttraining's rmse: 5.92072\ttraining's l2: 35.0549\tvalid_1's rmse: 8.0081\tvalid_1's l2: 64.1297\n",
      "[800]\ttraining's rmse: 5.71586\ttraining's l2: 32.671\tvalid_1's rmse: 8.005\tvalid_1's l2: 64.0801\n",
      "[900]\ttraining's rmse: 5.51998\ttraining's l2: 30.4701\tvalid_1's rmse: 8.00866\tvalid_1's l2: 64.1387\n",
      "Early stopping, best iteration is:\n",
      "[805]\ttraining's rmse: 5.70554\ttraining's l2: 32.5532\tvalid_1's rmse: 8.00442\tvalid_1's l2: 64.0708\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 8.004   \u001b[0m | \u001b[0m 0.849   \u001b[0m | \u001b[0m 205.1   \u001b[0m | \u001b[0m 15.06   \u001b[0m | \u001b[0m 44.39   \u001b[0m | \u001b[0m 22.19   \u001b[0m | \u001b[0m 24.73   \u001b[0m | \u001b[0m 34.57   \u001b[0m | \u001b[0m 4.697   \u001b[0m | \u001b[0m 0.5641  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.55787\ttraining's l2: 57.1214\tvalid_1's rmse: 8.28192\tvalid_1's l2: 68.5902\n",
      "[200]\ttraining's rmse: 6.7117\ttraining's l2: 45.047\tvalid_1's rmse: 8.07362\tvalid_1's l2: 65.1834\n",
      "[300]\ttraining's rmse: 6.16766\ttraining's l2: 38.04\tvalid_1's rmse: 8.0296\tvalid_1's l2: 64.4745\n",
      "[400]\ttraining's rmse: 5.74648\ttraining's l2: 33.0221\tvalid_1's rmse: 8.01496\tvalid_1's l2: 64.2395\n",
      "[500]\ttraining's rmse: 5.39162\ttraining's l2: 29.0695\tvalid_1's rmse: 8.0099\tvalid_1's l2: 64.1585\n",
      "[600]\ttraining's rmse: 5.06929\ttraining's l2: 25.6977\tvalid_1's rmse: 8.00786\tvalid_1's l2: 64.1258\n",
      "[700]\ttraining's rmse: 4.77264\ttraining's l2: 22.7781\tvalid_1's rmse: 8.00367\tvalid_1's l2: 64.0587\n",
      "Early stopping, best iteration is:\n",
      "[677]\ttraining's rmse: 4.84178\ttraining's l2: 23.4428\tvalid_1's rmse: 8.00187\tvalid_1's l2: 64.0299\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 8.002   \u001b[0m | \u001b[0m 0.5631  \u001b[0m | \u001b[0m 104.9   \u001b[0m | \u001b[0m 9.425   \u001b[0m | \u001b[0m 24.34   \u001b[0m | \u001b[0m 1.128   \u001b[0m | \u001b[0m 47.18   \u001b[0m | \u001b[0m 9.937   \u001b[0m | \u001b[0m 4.877   \u001b[0m | \u001b[0m 0.646   \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.9209\ttraining's l2: 62.7406\tvalid_1's rmse: 8.33292\tvalid_1's l2: 69.4375\n",
      "[200]\ttraining's rmse: 7.29728\ttraining's l2: 53.2503\tvalid_1's rmse: 8.10011\tvalid_1's l2: 65.6117\n",
      "[300]\ttraining's rmse: 6.89575\ttraining's l2: 47.5514\tvalid_1's rmse: 8.04347\tvalid_1's l2: 64.6974\n",
      "[400]\ttraining's rmse: 6.57195\ttraining's l2: 43.1905\tvalid_1's rmse: 8.02002\tvalid_1's l2: 64.3206\n",
      "[500]\ttraining's rmse: 6.28976\ttraining's l2: 39.5611\tvalid_1's rmse: 8.00146\tvalid_1's l2: 64.0234\n",
      "[600]\ttraining's rmse: 6.03634\ttraining's l2: 36.4374\tvalid_1's rmse: 7.99496\tvalid_1's l2: 63.9194\n",
      "[700]\ttraining's rmse: 5.80432\ttraining's l2: 33.6901\tvalid_1's rmse: 7.99309\tvalid_1's l2: 63.8895\n",
      "[800]\ttraining's rmse: 5.58645\ttraining's l2: 31.2084\tvalid_1's rmse: 7.99332\tvalid_1's l2: 63.8932\n",
      "Early stopping, best iteration is:\n",
      "[740]\ttraining's rmse: 5.71553\ttraining's l2: 32.6673\tvalid_1's rmse: 7.99154\tvalid_1's l2: 63.8647\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 7.992   \u001b[0m | \u001b[0m 0.6164  \u001b[0m | \u001b[0m 196.7   \u001b[0m | \u001b[0m 13.94   \u001b[0m | \u001b[0m 40.92   \u001b[0m | \u001b[0m 17.82   \u001b[0m | \u001b[0m 27.72   \u001b[0m | \u001b[0m 37.33   \u001b[0m | \u001b[0m 9.548   \u001b[0m | \u001b[0m 0.7088  \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.75464\ttraining's l2: 60.1344\tvalid_1's rmse: 8.2913\tvalid_1's l2: 68.7456\n",
      "[200]\ttraining's rmse: 7.03408\ttraining's l2: 49.4782\tvalid_1's rmse: 8.07276\tvalid_1's l2: 65.1694\n",
      "[300]\ttraining's rmse: 6.54866\ttraining's l2: 42.885\tvalid_1's rmse: 8.02173\tvalid_1's l2: 64.3481\n",
      "[400]\ttraining's rmse: 6.14817\ttraining's l2: 37.7999\tvalid_1's rmse: 7.99908\tvalid_1's l2: 63.9853\n",
      "[500]\ttraining's rmse: 5.80486\ttraining's l2: 33.6964\tvalid_1's rmse: 7.99507\tvalid_1's l2: 63.9212\n",
      "[600]\ttraining's rmse: 5.50386\ttraining's l2: 30.2925\tvalid_1's rmse: 7.9948\tvalid_1's l2: 63.9169\n",
      "Early stopping, best iteration is:\n",
      "[539]\ttraining's rmse: 5.683\ttraining's l2: 32.2964\tvalid_1's rmse: 7.99414\tvalid_1's l2: 63.9062\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 7.994   \u001b[0m | \u001b[0m 0.7675  \u001b[0m | \u001b[0m 132.0   \u001b[0m | \u001b[0m 14.3    \u001b[0m | \u001b[0m 40.39   \u001b[0m | \u001b[0m 44.76   \u001b[0m | \u001b[0m 35.85   \u001b[0m | \u001b[0m 33.16   \u001b[0m | \u001b[0m 4.179   \u001b[0m | \u001b[0m 0.9905  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.43944\ttraining's l2: 55.3453\tvalid_1's rmse: 8.24821\tvalid_1's l2: 68.033\n",
      "[200]\ttraining's rmse: 6.62144\ttraining's l2: 43.8435\tvalid_1's rmse: 8.05628\tvalid_1's l2: 64.9037\n",
      "[300]\ttraining's rmse: 6.18949\ttraining's l2: 38.3098\tvalid_1's rmse: 8.02023\tvalid_1's l2: 64.324\n",
      "[400]\ttraining's rmse: 5.87931\ttraining's l2: 34.5662\tvalid_1's rmse: 8.00666\tvalid_1's l2: 64.1065\n",
      "[500]\ttraining's rmse: 5.59439\ttraining's l2: 31.2972\tvalid_1's rmse: 7.99668\tvalid_1's l2: 63.9469\n",
      "[600]\ttraining's rmse: 5.32977\ttraining's l2: 28.4064\tvalid_1's rmse: 7.99105\tvalid_1's l2: 63.8568\n",
      "[700]\ttraining's rmse: 5.05697\ttraining's l2: 25.573\tvalid_1's rmse: 7.9919\tvalid_1's l2: 63.8705\n",
      "Early stopping, best iteration is:\n",
      "[661]\ttraining's rmse: 5.16301\ttraining's l2: 26.6567\tvalid_1's rmse: 7.98922\tvalid_1's l2: 63.8276\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 7.989   \u001b[0m | \u001b[0m 0.5909  \u001b[0m | \u001b[0m 151.4   \u001b[0m | \u001b[0m 8.126   \u001b[0m | \u001b[0m 67.71   \u001b[0m | \u001b[0m 12.18   \u001b[0m | \u001b[0m 57.97   \u001b[0m | \u001b[0m 5.113   \u001b[0m | \u001b[0m 2.963   \u001b[0m | \u001b[0m 0.5074  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.90597\ttraining's l2: 62.5043\tvalid_1's rmse: 8.32703\tvalid_1's l2: 69.3395\n",
      "[200]\ttraining's rmse: 7.28252\ttraining's l2: 53.035\tvalid_1's rmse: 8.10294\tvalid_1's l2: 65.6576\n",
      "[300]\ttraining's rmse: 6.87807\ttraining's l2: 47.3078\tvalid_1's rmse: 8.05229\tvalid_1's l2: 64.8394\n",
      "[400]\ttraining's rmse: 6.54574\ttraining's l2: 42.8467\tvalid_1's rmse: 8.02581\tvalid_1's l2: 64.4136\n",
      "[500]\ttraining's rmse: 6.25307\ttraining's l2: 39.1008\tvalid_1's rmse: 8.01763\tvalid_1's l2: 64.2823\n",
      "[600]\ttraining's rmse: 5.9917\ttraining's l2: 35.9005\tvalid_1's rmse: 8.01744\tvalid_1's l2: 64.2793\n",
      "Early stopping, best iteration is:\n",
      "[536]\ttraining's rmse: 6.15594\ttraining's l2: 37.8956\tvalid_1's rmse: 8.01475\tvalid_1's l2: 64.2362\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m 8.015   \u001b[0m | \u001b[95m 0.9254  \u001b[0m | \u001b[95m 416.7   \u001b[0m | \u001b[95m 10.77   \u001b[0m | \u001b[95m 126.9   \u001b[0m | \u001b[95m 26.11   \u001b[0m | \u001b[95m 29.61   \u001b[0m | \u001b[95m 48.7    \u001b[0m | \u001b[95m 7.307   \u001b[0m | \u001b[95m 0.5611  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.86497\ttraining's l2: 61.8578\tvalid_1's rmse: 8.31388\tvalid_1's l2: 69.1205\n",
      "[200]\ttraining's rmse: 7.20727\ttraining's l2: 51.9448\tvalid_1's rmse: 8.08449\tvalid_1's l2: 65.359\n",
      "[300]\ttraining's rmse: 6.7841\ttraining's l2: 46.024\tvalid_1's rmse: 8.03131\tvalid_1's l2: 64.502\n",
      "[400]\ttraining's rmse: 6.43651\ttraining's l2: 41.4286\tvalid_1's rmse: 8.01003\tvalid_1's l2: 64.1605\n",
      "[500]\ttraining's rmse: 6.1278\ttraining's l2: 37.55\tvalid_1's rmse: 8.00132\tvalid_1's l2: 64.0212\n",
      "[600]\ttraining's rmse: 5.85573\ttraining's l2: 34.2895\tvalid_1's rmse: 7.99769\tvalid_1's l2: 63.963\n",
      "[700]\ttraining's rmse: 5.60531\ttraining's l2: 31.4195\tvalid_1's rmse: 7.99772\tvalid_1's l2: 63.9635\n",
      "[800]\ttraining's rmse: 5.36958\ttraining's l2: 28.8324\tvalid_1's rmse: 7.99876\tvalid_1's l2: 63.9802\n",
      "Early stopping, best iteration is:\n",
      "[750]\ttraining's rmse: 5.48634\ttraining's l2: 30.0999\tvalid_1's rmse: 7.99672\tvalid_1's l2: 63.9476\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 7.997   \u001b[0m | \u001b[0m 0.5141  \u001b[0m | \u001b[0m 254.3   \u001b[0m | \u001b[0m 15.33   \u001b[0m | \u001b[0m 103.2   \u001b[0m | \u001b[0m 46.9    \u001b[0m | \u001b[0m 32.34   \u001b[0m | \u001b[0m 12.22   \u001b[0m | \u001b[0m 9.777   \u001b[0m | \u001b[0m 0.6989  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.81249\ttraining's l2: 61.035\tvalid_1's rmse: 8.29929\tvalid_1's l2: 68.8782\n",
      "[200]\ttraining's rmse: 7.13089\ttraining's l2: 50.8497\tvalid_1's rmse: 8.06626\tvalid_1's l2: 65.0646\n",
      "[300]\ttraining's rmse: 6.68136\ttraining's l2: 44.6405\tvalid_1's rmse: 8.02054\tvalid_1's l2: 64.3291\n",
      "[400]\ttraining's rmse: 6.30808\ttraining's l2: 39.7919\tvalid_1's rmse: 7.99934\tvalid_1's l2: 63.9894\n",
      "[500]\ttraining's rmse: 5.98096\ttraining's l2: 35.7719\tvalid_1's rmse: 7.99022\tvalid_1's l2: 63.8437\n",
      "[600]\ttraining's rmse: 5.69123\ttraining's l2: 32.3901\tvalid_1's rmse: 7.9872\tvalid_1's l2: 63.7953\n",
      "[700]\ttraining's rmse: 5.42457\ttraining's l2: 29.4259\tvalid_1's rmse: 7.99007\tvalid_1's l2: 63.8413\n",
      "Early stopping, best iteration is:\n",
      "[610]\ttraining's rmse: 5.66356\ttraining's l2: 32.0759\tvalid_1's rmse: 7.98639\tvalid_1's l2: 63.7824\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 7.986   \u001b[0m | \u001b[0m 0.5715  \u001b[0m | \u001b[0m 420.3   \u001b[0m | \u001b[0m 15.04   \u001b[0m | \u001b[0m 125.0   \u001b[0m | \u001b[0m 3.524   \u001b[0m | \u001b[0m 35.44   \u001b[0m | \u001b[0m 22.55   \u001b[0m | \u001b[0m 3.946   \u001b[0m | \u001b[0m 0.9637  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.70738\ttraining's l2: 59.4037\tvalid_1's rmse: 8.29395\tvalid_1's l2: 68.7897\n",
      "[200]\ttraining's rmse: 6.95203\ttraining's l2: 48.3307\tvalid_1's rmse: 8.07378\tvalid_1's l2: 65.1859\n",
      "[300]\ttraining's rmse: 6.43838\ttraining's l2: 41.4527\tvalid_1's rmse: 8.01978\tvalid_1's l2: 64.3168\n",
      "[400]\ttraining's rmse: 6.01729\ttraining's l2: 36.2078\tvalid_1's rmse: 8.00097\tvalid_1's l2: 64.0156\n",
      "[500]\ttraining's rmse: 5.65668\ttraining's l2: 31.998\tvalid_1's rmse: 7.9888\tvalid_1's l2: 63.821\n",
      "[600]\ttraining's rmse: 5.33915\ttraining's l2: 28.5065\tvalid_1's rmse: 7.98331\tvalid_1's l2: 63.7332\n",
      "[700]\ttraining's rmse: 5.05586\ttraining's l2: 25.5618\tvalid_1's rmse: 7.97972\tvalid_1's l2: 63.6759\n",
      "[800]\ttraining's rmse: 4.79158\ttraining's l2: 22.9592\tvalid_1's rmse: 7.98047\tvalid_1's l2: 63.6879\n",
      "Early stopping, best iteration is:\n",
      "[748]\ttraining's rmse: 4.92582\ttraining's l2: 24.2637\tvalid_1's rmse: 7.97864\tvalid_1's l2: 63.6587\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 7.979   \u001b[0m | \u001b[0m 0.5793  \u001b[0m | \u001b[0m 255.2   \u001b[0m | \u001b[0m 13.55   \u001b[0m | \u001b[0m 12.51   \u001b[0m | \u001b[0m 29.43   \u001b[0m | \u001b[0m 39.82   \u001b[0m | \u001b[0m 33.96   \u001b[0m | \u001b[0m 5.653   \u001b[0m | \u001b[0m 0.8581  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.60174\ttraining's l2: 57.7864\tvalid_1's rmse: 8.28254\tvalid_1's l2: 68.6004\n",
      "[200]\ttraining's rmse: 6.80592\ttraining's l2: 46.3206\tvalid_1's rmse: 8.07189\tvalid_1's l2: 65.1555\n",
      "[300]\ttraining's rmse: 6.28035\ttraining's l2: 39.4428\tvalid_1's rmse: 8.0304\tvalid_1's l2: 64.4873\n",
      "[400]\ttraining's rmse: 5.86367\ttraining's l2: 34.3826\tvalid_1's rmse: 8.01486\tvalid_1's l2: 64.238\n",
      "[500]\ttraining's rmse: 5.49939\ttraining's l2: 30.2433\tvalid_1's rmse: 8.01213\tvalid_1's l2: 64.1942\n",
      "[600]\ttraining's rmse: 5.15384\ttraining's l2: 26.5621\tvalid_1's rmse: 8.0089\tvalid_1's l2: 64.1425\n",
      "[700]\ttraining's rmse: 4.8443\ttraining's l2: 23.4672\tvalid_1's rmse: 8.01195\tvalid_1's l2: 64.1913\n",
      "Early stopping, best iteration is:\n",
      "[640]\ttraining's rmse: 5.02931\ttraining's l2: 25.294\tvalid_1's rmse: 8.00755\tvalid_1's l2: 64.1208\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 8.008   \u001b[0m | \u001b[0m 0.6464  \u001b[0m | \u001b[0m 212.8   \u001b[0m | \u001b[0m 9.076   \u001b[0m | \u001b[0m 32.07   \u001b[0m | \u001b[0m 22.8    \u001b[0m | \u001b[0m 47.68   \u001b[0m | \u001b[0m 48.94   \u001b[0m | \u001b[0m 5.883   \u001b[0m | \u001b[0m 0.8768  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.54675\ttraining's l2: 56.9535\tvalid_1's rmse: 8.26265\tvalid_1's l2: 68.2713\n",
      "[200]\ttraining's rmse: 6.70501\ttraining's l2: 44.9571\tvalid_1's rmse: 8.05106\tvalid_1's l2: 64.8195\n",
      "[300]\ttraining's rmse: 6.11956\ttraining's l2: 37.4491\tvalid_1's rmse: 8.00668\tvalid_1's l2: 64.1069\n",
      "[400]\ttraining's rmse: 5.63438\ttraining's l2: 31.7462\tvalid_1's rmse: 7.99548\tvalid_1's l2: 63.9276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\ttraining's rmse: 5.21249\ttraining's l2: 27.1701\tvalid_1's rmse: 7.98705\tvalid_1's l2: 63.7929\n",
      "[600]\ttraining's rmse: 4.83784\ttraining's l2: 23.4047\tvalid_1's rmse: 7.98179\tvalid_1's l2: 63.709\n",
      "[700]\ttraining's rmse: 4.49471\ttraining's l2: 20.2024\tvalid_1's rmse: 7.9857\tvalid_1's l2: 63.7715\n",
      "Early stopping, best iteration is:\n",
      "[618]\ttraining's rmse: 4.77329\ttraining's l2: 22.7843\tvalid_1's rmse: 7.9802\tvalid_1's l2: 63.6836\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 7.98    \u001b[0m | \u001b[0m 0.6659  \u001b[0m | \u001b[0m 318.9   \u001b[0m | \u001b[0m 14.7    \u001b[0m | \u001b[0m 89.29   \u001b[0m | \u001b[0m 7.679   \u001b[0m | \u001b[0m 54.4    \u001b[0m | \u001b[0m 41.98   \u001b[0m | \u001b[0m 4.397   \u001b[0m | \u001b[0m 0.6464  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.41937\ttraining's l2: 55.047\tvalid_1's rmse: 8.26974\tvalid_1's l2: 68.3886\n",
      "[200]\ttraining's rmse: 6.52574\ttraining's l2: 42.5853\tvalid_1's rmse: 8.0731\tvalid_1's l2: 65.175\n",
      "[300]\ttraining's rmse: 5.92428\ttraining's l2: 35.0971\tvalid_1's rmse: 8.03735\tvalid_1's l2: 64.599\n",
      "[400]\ttraining's rmse: 5.44183\ttraining's l2: 29.6135\tvalid_1's rmse: 8.01934\tvalid_1's l2: 64.3098\n",
      "[500]\ttraining's rmse: 5.01193\ttraining's l2: 25.1194\tvalid_1's rmse: 8.01229\tvalid_1's l2: 64.1968\n",
      "[600]\ttraining's rmse: 4.63699\ttraining's l2: 21.5017\tvalid_1's rmse: 8.0113\tvalid_1's l2: 64.1809\n",
      "[700]\ttraining's rmse: 4.29465\ttraining's l2: 18.444\tvalid_1's rmse: 8.01279\tvalid_1's l2: 64.2049\n",
      "Early stopping, best iteration is:\n",
      "[656]\ttraining's rmse: 4.44329\ttraining's l2: 19.7428\tvalid_1's rmse: 8.00885\tvalid_1's l2: 64.1417\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 8.009   \u001b[0m | \u001b[0m 0.8912  \u001b[0m | \u001b[0m 226.8   \u001b[0m | \u001b[0m 9.587   \u001b[0m | \u001b[0m 45.26   \u001b[0m | \u001b[0m 38.36   \u001b[0m | \u001b[0m 58.79   \u001b[0m | \u001b[0m 48.62   \u001b[0m | \u001b[0m 2.709   \u001b[0m | \u001b[0m 0.9832  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.84404\ttraining's l2: 61.529\tvalid_1's rmse: 8.31939\tvalid_1's l2: 69.2123\n",
      "[200]\ttraining's rmse: 7.17951\ttraining's l2: 51.5453\tvalid_1's rmse: 8.09364\tvalid_1's l2: 65.507\n",
      "[300]\ttraining's rmse: 6.74971\ttraining's l2: 45.5586\tvalid_1's rmse: 8.04491\tvalid_1's l2: 64.7206\n",
      "[400]\ttraining's rmse: 6.40186\ttraining's l2: 40.9839\tvalid_1's rmse: 8.01898\tvalid_1's l2: 64.304\n",
      "[500]\ttraining's rmse: 6.09334\ttraining's l2: 37.1287\tvalid_1's rmse: 8.00525\tvalid_1's l2: 64.084\n",
      "[600]\ttraining's rmse: 5.81607\ttraining's l2: 33.8267\tvalid_1's rmse: 8.0005\tvalid_1's l2: 64.0079\n",
      "Early stopping, best iteration is:\n",
      "[557]\ttraining's rmse: 5.93037\ttraining's l2: 35.1693\tvalid_1's rmse: 7.99997\tvalid_1's l2: 63.9995\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 8.0     \u001b[0m | \u001b[0m 0.9319  \u001b[0m | \u001b[0m 265.1   \u001b[0m | \u001b[0m 8.635   \u001b[0m | \u001b[0m 55.92   \u001b[0m | \u001b[0m 4.254   \u001b[0m | \u001b[0m 32.04   \u001b[0m | \u001b[0m 37.83   \u001b[0m | \u001b[0m 6.108   \u001b[0m | \u001b[0m 0.9044  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.56828\ttraining's l2: 57.2788\tvalid_1's rmse: 8.28047\tvalid_1's l2: 68.5661\n",
      "[200]\ttraining's rmse: 6.72985\ttraining's l2: 45.2909\tvalid_1's rmse: 8.07478\tvalid_1's l2: 65.2021\n",
      "[300]\ttraining's rmse: 6.16894\ttraining's l2: 38.0559\tvalid_1's rmse: 8.03235\tvalid_1's l2: 64.5186\n",
      "[400]\ttraining's rmse: 5.71432\ttraining's l2: 32.6535\tvalid_1's rmse: 8.0118\tvalid_1's l2: 64.1889\n",
      "[500]\ttraining's rmse: 5.31686\ttraining's l2: 28.269\tvalid_1's rmse: 8.004\tvalid_1's l2: 64.064\n",
      "[600]\ttraining's rmse: 4.96179\ttraining's l2: 24.6193\tvalid_1's rmse: 8.00185\tvalid_1's l2: 64.0296\n",
      "Early stopping, best iteration is:\n",
      "[584]\ttraining's rmse: 5.01766\ttraining's l2: 25.1769\tvalid_1's rmse: 8.00108\tvalid_1's l2: 64.0174\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 8.001   \u001b[0m | \u001b[0m 0.8401  \u001b[0m | \u001b[0m 218.0   \u001b[0m | \u001b[0m 10.83   \u001b[0m | \u001b[0m 18.53   \u001b[0m | \u001b[0m 44.08   \u001b[0m | \u001b[0m 48.28   \u001b[0m | \u001b[0m 31.39   \u001b[0m | \u001b[0m 5.161   \u001b[0m | \u001b[0m 0.8321  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.42564\ttraining's l2: 55.1401\tvalid_1's rmse: 8.25777\tvalid_1's l2: 68.1907\n",
      "[200]\ttraining's rmse: 6.52118\ttraining's l2: 42.5257\tvalid_1's rmse: 8.06626\tvalid_1's l2: 65.0646\n",
      "[300]\ttraining's rmse: 5.93247\ttraining's l2: 35.1942\tvalid_1's rmse: 8.02848\tvalid_1's l2: 64.4564\n",
      "[400]\ttraining's rmse: 5.45778\ttraining's l2: 29.7874\tvalid_1's rmse: 8.013\tvalid_1's l2: 64.2081\n",
      "[500]\ttraining's rmse: 5.0425\ttraining's l2: 25.4268\tvalid_1's rmse: 8.00817\tvalid_1's l2: 64.1307\n",
      "[600]\ttraining's rmse: 4.68781\ttraining's l2: 21.9756\tvalid_1's rmse: 8.0051\tvalid_1's l2: 64.0816\n",
      "Early stopping, best iteration is:\n",
      "[590]\ttraining's rmse: 4.7235\ttraining's l2: 22.3115\tvalid_1's rmse: 8.00399\tvalid_1's l2: 64.0639\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 8.004   \u001b[0m | \u001b[0m 0.7178  \u001b[0m | \u001b[0m 174.7   \u001b[0m | \u001b[0m 10.29   \u001b[0m | \u001b[0m 36.12   \u001b[0m | \u001b[0m 11.2    \u001b[0m | \u001b[0m 59.42   \u001b[0m | \u001b[0m 23.53   \u001b[0m | \u001b[0m 9.78    \u001b[0m | \u001b[0m 0.5628  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.93775\ttraining's l2: 63.0078\tvalid_1's rmse: 8.3214\tvalid_1's l2: 69.2457\n",
      "[200]\ttraining's rmse: 7.34011\ttraining's l2: 53.8772\tvalid_1's rmse: 8.08443\tvalid_1's l2: 65.358\n",
      "[300]\ttraining's rmse: 6.95514\ttraining's l2: 48.3739\tvalid_1's rmse: 8.02885\tvalid_1's l2: 64.4624\n",
      "[400]\ttraining's rmse: 6.63532\ttraining's l2: 44.0275\tvalid_1's rmse: 8.00313\tvalid_1's l2: 64.0502\n",
      "[500]\ttraining's rmse: 6.35585\ttraining's l2: 40.3969\tvalid_1's rmse: 7.99626\tvalid_1's l2: 63.9402\n",
      "[600]\ttraining's rmse: 6.09995\ttraining's l2: 37.2094\tvalid_1's rmse: 7.99287\tvalid_1's l2: 63.886\n",
      "Early stopping, best iteration is:\n",
      "[558]\ttraining's rmse: 6.20356\ttraining's l2: 38.4842\tvalid_1's rmse: 7.99214\tvalid_1's l2: 63.8744\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 7.992   \u001b[0m | \u001b[0m 0.8023  \u001b[0m | \u001b[0m 372.3   \u001b[0m | \u001b[0m 13.22   \u001b[0m | \u001b[0m 189.7   \u001b[0m | \u001b[0m 29.8    \u001b[0m | \u001b[0m 29.28   \u001b[0m | \u001b[0m 46.45   \u001b[0m | \u001b[0m 6.261   \u001b[0m | \u001b[0m 0.5583  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.6482\ttraining's l2: 58.495\tvalid_1's rmse: 8.28219\tvalid_1's l2: 68.5946\n",
      "[200]\ttraining's rmse: 6.87076\ttraining's l2: 47.2074\tvalid_1's rmse: 8.07244\tvalid_1's l2: 65.1643\n",
      "[300]\ttraining's rmse: 6.35266\ttraining's l2: 40.3564\tvalid_1's rmse: 8.02758\tvalid_1's l2: 64.4421\n",
      "[400]\ttraining's rmse: 5.92757\ttraining's l2: 35.1361\tvalid_1's rmse: 8.014\tvalid_1's l2: 64.2242\n",
      "[500]\ttraining's rmse: 5.55863\ttraining's l2: 30.8984\tvalid_1's rmse: 8.00285\tvalid_1's l2: 64.0457\n",
      "Early stopping, best iteration is:\n",
      "[485]\ttraining's rmse: 5.6108\ttraining's l2: 31.4811\tvalid_1's rmse: 8.00263\tvalid_1's l2: 64.0422\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 8.003   \u001b[0m | \u001b[0m 0.6121  \u001b[0m | \u001b[0m 56.01   \u001b[0m | \u001b[0m 13.32   \u001b[0m | \u001b[0m 86.02   \u001b[0m | \u001b[0m 2.535   \u001b[0m | \u001b[0m 43.43   \u001b[0m | \u001b[0m 12.8    \u001b[0m | \u001b[0m 3.448   \u001b[0m | \u001b[0m 0.5243  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.63448\ttraining's l2: 58.2853\tvalid_1's rmse: 8.2646\tvalid_1's l2: 68.3036\n",
      "[200]\ttraining's rmse: 6.84273\ttraining's l2: 46.8229\tvalid_1's rmse: 8.05077\tvalid_1's l2: 64.8149\n",
      "[300]\ttraining's rmse: 6.30817\ttraining's l2: 39.793\tvalid_1's rmse: 8.00878\tvalid_1's l2: 64.1406\n",
      "[400]\ttraining's rmse: 5.86858\ttraining's l2: 34.4402\tvalid_1's rmse: 7.9954\tvalid_1's l2: 63.9265\n",
      "[500]\ttraining's rmse: 5.48333\ttraining's l2: 30.0669\tvalid_1's rmse: 7.99744\tvalid_1's l2: 63.959\n",
      "Early stopping, best iteration is:\n",
      "[444]\ttraining's rmse: 5.69197\ttraining's l2: 32.3985\tvalid_1's rmse: 7.99432\tvalid_1's l2: 63.9092\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 7.994   \u001b[0m | \u001b[0m 0.6732  \u001b[0m | \u001b[0m 294.8   \u001b[0m | \u001b[0m 15.04   \u001b[0m | \u001b[0m 117.9   \u001b[0m | \u001b[0m 26.82   \u001b[0m | \u001b[0m 46.07   \u001b[0m | \u001b[0m 13.76   \u001b[0m | \u001b[0m 3.302   \u001b[0m | \u001b[0m 0.5363  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.7177\ttraining's l2: 59.5629\tvalid_1's rmse: 8.28928\tvalid_1's l2: 68.7122\n",
      "[200]\ttraining's rmse: 6.97738\ttraining's l2: 48.6838\tvalid_1's rmse: 8.07227\tvalid_1's l2: 65.1615\n",
      "[300]\ttraining's rmse: 6.47632\ttraining's l2: 41.9427\tvalid_1's rmse: 8.0203\tvalid_1's l2: 64.3252\n",
      "[400]\ttraining's rmse: 6.06521\ttraining's l2: 36.7868\tvalid_1's rmse: 7.99425\tvalid_1's l2: 63.908\n",
      "[500]\ttraining's rmse: 5.71314\ttraining's l2: 32.64\tvalid_1's rmse: 7.98797\tvalid_1's l2: 63.8077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600]\ttraining's rmse: 5.39813\ttraining's l2: 29.1398\tvalid_1's rmse: 7.98598\tvalid_1's l2: 63.7759\n",
      "Early stopping, best iteration is:\n",
      "[571]\ttraining's rmse: 5.48647\ttraining's l2: 30.1013\tvalid_1's rmse: 7.98456\tvalid_1's l2: 63.7532\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 7.985   \u001b[0m | \u001b[0m 0.819   \u001b[0m | \u001b[0m 33.68   \u001b[0m | \u001b[0m 14.31   \u001b[0m | \u001b[0m 81.77   \u001b[0m | \u001b[0m 30.16   \u001b[0m | \u001b[0m 39.27   \u001b[0m | \u001b[0m 19.79   \u001b[0m | \u001b[0m 7.493   \u001b[0m | \u001b[0m 0.6173  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.60431\ttraining's l2: 57.8256\tvalid_1's rmse: 8.27664\tvalid_1's l2: 68.5028\n",
      "[200]\ttraining's rmse: 6.7916\ttraining's l2: 46.1258\tvalid_1's rmse: 8.06245\tvalid_1's l2: 65.003\n",
      "[300]\ttraining's rmse: 6.23353\ttraining's l2: 38.8568\tvalid_1's rmse: 8.01667\tvalid_1's l2: 64.267\n",
      "[400]\ttraining's rmse: 5.77155\ttraining's l2: 33.3108\tvalid_1's rmse: 8.00322\tvalid_1's l2: 64.0515\n",
      "[500]\ttraining's rmse: 5.376\ttraining's l2: 28.9013\tvalid_1's rmse: 7.99383\tvalid_1's l2: 63.9013\n",
      "[600]\ttraining's rmse: 5.02397\ttraining's l2: 25.2403\tvalid_1's rmse: 7.99363\tvalid_1's l2: 63.898\n",
      "Early stopping, best iteration is:\n",
      "[540]\ttraining's rmse: 5.23143\ttraining's l2: 27.3678\tvalid_1's rmse: 7.99241\tvalid_1's l2: 63.8786\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 7.992   \u001b[0m | \u001b[0m 0.7314  \u001b[0m | \u001b[0m 322.5   \u001b[0m | \u001b[0m 14.67   \u001b[0m | \u001b[0m 29.11   \u001b[0m | \u001b[0m 46.03   \u001b[0m | \u001b[0m 47.01   \u001b[0m | \u001b[0m 44.83   \u001b[0m | \u001b[0m 5.787   \u001b[0m | \u001b[0m 0.6073  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.58206\ttraining's l2: 57.4876\tvalid_1's rmse: 8.28418\tvalid_1's l2: 68.6276\n",
      "[200]\ttraining's rmse: 6.74484\ttraining's l2: 45.4928\tvalid_1's rmse: 8.07714\tvalid_1's l2: 65.2402\n",
      "[300]\ttraining's rmse: 6.16075\ttraining's l2: 37.9548\tvalid_1's rmse: 8.03734\tvalid_1's l2: 64.5988\n",
      "[400]\ttraining's rmse: 5.68369\ttraining's l2: 32.3044\tvalid_1's rmse: 8.01931\tvalid_1's l2: 64.3093\n",
      "[500]\ttraining's rmse: 5.28092\ttraining's l2: 27.8881\tvalid_1's rmse: 8.01391\tvalid_1's l2: 64.2227\n",
      "Early stopping, best iteration is:\n",
      "[468]\ttraining's rmse: 5.40186\ttraining's l2: 29.1801\tvalid_1's rmse: 8.01112\tvalid_1's l2: 64.178\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 8.011   \u001b[0m | \u001b[0m 0.9712  \u001b[0m | \u001b[0m 489.0   \u001b[0m | \u001b[0m 15.25   \u001b[0m | \u001b[0m 49.44   \u001b[0m | \u001b[0m 26.52   \u001b[0m | \u001b[0m 47.24   \u001b[0m | \u001b[0m 39.95   \u001b[0m | \u001b[0m 2.888   \u001b[0m | \u001b[0m 0.8058  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.91899\ttraining's l2: 62.7104\tvalid_1's rmse: 8.32261\tvalid_1's l2: 69.2659\n",
      "[200]\ttraining's rmse: 7.30873\ttraining's l2: 53.4175\tvalid_1's rmse: 8.09533\tvalid_1's l2: 65.5343\n",
      "[300]\ttraining's rmse: 6.91591\ttraining's l2: 47.8298\tvalid_1's rmse: 8.03767\tvalid_1's l2: 64.6042\n",
      "[400]\ttraining's rmse: 6.59495\ttraining's l2: 43.4933\tvalid_1's rmse: 8.01168\tvalid_1's l2: 64.187\n",
      "[500]\ttraining's rmse: 6.31475\ttraining's l2: 39.8761\tvalid_1's rmse: 7.99822\tvalid_1's l2: 63.9716\n",
      "[600]\ttraining's rmse: 6.06017\ttraining's l2: 36.7257\tvalid_1's rmse: 7.99768\tvalid_1's l2: 63.9629\n",
      "Early stopping, best iteration is:\n",
      "[561]\ttraining's rmse: 6.15729\ttraining's l2: 37.9122\tvalid_1's rmse: 7.99524\tvalid_1's l2: 63.9239\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 7.995   \u001b[0m | \u001b[0m 0.7876  \u001b[0m | \u001b[0m 240.7   \u001b[0m | \u001b[0m 13.27   \u001b[0m | \u001b[0m 133.4   \u001b[0m | \u001b[0m 33.93   \u001b[0m | \u001b[0m 28.29   \u001b[0m | \u001b[0m 39.58   \u001b[0m | \u001b[0m 1.11    \u001b[0m | \u001b[0m 0.7515  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.74937\ttraining's l2: 60.0527\tvalid_1's rmse: 8.28664\tvalid_1's l2: 68.6684\n",
      "[200]\ttraining's rmse: 7.03284\ttraining's l2: 49.4609\tvalid_1's rmse: 8.06597\tvalid_1's l2: 65.0599\n",
      "[300]\ttraining's rmse: 6.5611\ttraining's l2: 43.048\tvalid_1's rmse: 8.0208\tvalid_1's l2: 64.3332\n",
      "[400]\ttraining's rmse: 6.17429\ttraining's l2: 38.1219\tvalid_1's rmse: 8.00159\tvalid_1's l2: 64.0254\n",
      "[500]\ttraining's rmse: 5.83931\ttraining's l2: 34.0976\tvalid_1's rmse: 7.99224\tvalid_1's l2: 63.8759\n",
      "[600]\ttraining's rmse: 5.53662\ttraining's l2: 30.6542\tvalid_1's rmse: 7.99409\tvalid_1's l2: 63.9055\n",
      "Early stopping, best iteration is:\n",
      "[535]\ttraining's rmse: 5.73088\ttraining's l2: 32.843\tvalid_1's rmse: 7.99035\tvalid_1's l2: 63.8457\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 7.99    \u001b[0m | \u001b[0m 0.5948  \u001b[0m | \u001b[0m 65.55   \u001b[0m | \u001b[0m 13.15   \u001b[0m | \u001b[0m 102.1   \u001b[0m | \u001b[0m 8.454   \u001b[0m | \u001b[0m 37.71   \u001b[0m | \u001b[0m 8.496   \u001b[0m | \u001b[0m 8.393   \u001b[0m | \u001b[0m 0.7137  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.70946\ttraining's l2: 59.4358\tvalid_1's rmse: 8.29133\tvalid_1's l2: 68.7462\n",
      "[200]\ttraining's rmse: 6.95908\ttraining's l2: 48.4287\tvalid_1's rmse: 8.07525\tvalid_1's l2: 65.2096\n",
      "[300]\ttraining's rmse: 6.46316\ttraining's l2: 41.7724\tvalid_1's rmse: 8.02416\tvalid_1's l2: 64.3871\n",
      "[400]\ttraining's rmse: 6.06776\ttraining's l2: 36.8177\tvalid_1's rmse: 8.01293\tvalid_1's l2: 64.207\n",
      "[500]\ttraining's rmse: 5.70892\ttraining's l2: 32.5917\tvalid_1's rmse: 8.0072\tvalid_1's l2: 64.1152\n",
      "[600]\ttraining's rmse: 5.38403\ttraining's l2: 28.9878\tvalid_1's rmse: 8.00507\tvalid_1's l2: 64.0811\n",
      "Early stopping, best iteration is:\n",
      "[513]\ttraining's rmse: 5.66453\ttraining's l2: 32.0869\tvalid_1's rmse: 8.0041\tvalid_1's l2: 64.0656\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 8.004   \u001b[0m | \u001b[0m 0.5543  \u001b[0m | \u001b[0m 266.6   \u001b[0m | \u001b[0m 12.65   \u001b[0m | \u001b[0m 147.4   \u001b[0m | \u001b[0m 13.2    \u001b[0m | \u001b[0m 45.91   \u001b[0m | \u001b[0m 23.86   \u001b[0m | \u001b[0m 8.469   \u001b[0m | \u001b[0m 0.9726  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.67462\ttraining's l2: 58.8998\tvalid_1's rmse: 8.29135\tvalid_1's l2: 68.7465\n",
      "[200]\ttraining's rmse: 6.90917\ttraining's l2: 47.7366\tvalid_1's rmse: 8.08239\tvalid_1's l2: 65.3249\n",
      "[300]\ttraining's rmse: 6.41352\ttraining's l2: 41.1333\tvalid_1's rmse: 8.03113\tvalid_1's l2: 64.499\n",
      "[400]\ttraining's rmse: 6.02354\ttraining's l2: 36.2831\tvalid_1's rmse: 8.01151\tvalid_1's l2: 64.1843\n",
      "[500]\ttraining's rmse: 5.68364\ttraining's l2: 32.3038\tvalid_1's rmse: 8.00372\tvalid_1's l2: 64.0595\n",
      "[600]\ttraining's rmse: 5.38236\ttraining's l2: 28.9698\tvalid_1's rmse: 8.00154\tvalid_1's l2: 64.0246\n",
      "[700]\ttraining's rmse: 5.10434\ttraining's l2: 26.0543\tvalid_1's rmse: 7.99955\tvalid_1's l2: 63.9928\n",
      "Early stopping, best iteration is:\n",
      "[650]\ttraining's rmse: 5.23786\ttraining's l2: 27.4352\tvalid_1's rmse: 7.99847\tvalid_1's l2: 63.9755\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 7.998   \u001b[0m | \u001b[0m 0.6567  \u001b[0m | \u001b[0m 433.3   \u001b[0m | \u001b[0m 9.49    \u001b[0m | \u001b[0m 18.65   \u001b[0m | \u001b[0m 34.59   \u001b[0m | \u001b[0m 40.31   \u001b[0m | \u001b[0m 9.121   \u001b[0m | \u001b[0m 5.538   \u001b[0m | \u001b[0m 0.5815  \u001b[0m |\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 7.89933\ttraining's l2: 62.3994\tvalid_1's rmse: 8.31436\tvalid_1's l2: 69.1287\n",
      "[200]\ttraining's rmse: 7.27854\ttraining's l2: 52.9771\tvalid_1's rmse: 8.08118\tvalid_1's l2: 65.3055\n",
      "[300]\ttraining's rmse: 6.89094\ttraining's l2: 47.485\tvalid_1's rmse: 8.02765\tvalid_1's l2: 64.4432\n",
      "[400]\ttraining's rmse: 6.57358\ttraining's l2: 43.212\tvalid_1's rmse: 8.00445\tvalid_1's l2: 64.0712\n",
      "[500]\ttraining's rmse: 6.28824\ttraining's l2: 39.5419\tvalid_1's rmse: 7.99671\tvalid_1's l2: 63.9473\n",
      "[600]\ttraining's rmse: 6.03655\ttraining's l2: 36.4399\tvalid_1's rmse: 7.99278\tvalid_1's l2: 63.8845\n",
      "Early stopping, best iteration is:\n",
      "[557]\ttraining's rmse: 6.14087\ttraining's l2: 37.7103\tvalid_1's rmse: 7.992\tvalid_1's l2: 63.8721\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 7.992   \u001b[0m | \u001b[0m 0.6299  \u001b[0m | \u001b[0m 460.8   \u001b[0m | \u001b[0m 9.51    \u001b[0m | \u001b[0m 107.5   \u001b[0m | \u001b[0m 27.24   \u001b[0m | \u001b[0m 29.0    \u001b[0m | \u001b[0m 36.78   \u001b[0m | \u001b[0m 0.7092  \u001b[0m | \u001b[0m 0.9752  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "lgbBO = BayesianOptimization(f=lgb_rmse_eval, pbounds=bayesian_params, random_state=1000)\n",
    "lgbBO.maximize(init_points=5, n_iter=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 8.006492932036826,\n",
       "  'params': {'colsample_bytree': 0.8267947927323047,\n",
       "   'max_bin': 66.35340213095881,\n",
       "   'max_depth': 15.602262914792195,\n",
       "   'min_child_samples': 101.61636627131966,\n",
       "   'min_child_weight': 43.75125222391973,\n",
       "   'num_leaves': 32.4933072369088,\n",
       "   'reg_alpha': 2.045074142206765,\n",
       "   'reg_lambda': 3.9725474189957124,\n",
       "   'subsample': 0.61656609867419}},\n",
       " {'target': 8.014310481643134,\n",
       "  'params': {'colsample_bytree': 0.9208703621265308,\n",
       "   'max_bin': 111.47034874950597,\n",
       "   'max_depth': 13.93975626865927,\n",
       "   'min_child_samples': 84.50928428367985,\n",
       "   'min_child_weight': 9.930569477666833,\n",
       "   'num_leaves': 53.7415765836856,\n",
       "   'reg_alpha': 3.488408227688028,\n",
       "   'reg_lambda': 8.853486706603126,\n",
       "   'subsample': 0.9763221996107709}},\n",
       " {'target': 7.998092920982338,\n",
       "  'params': {'colsample_bytree': 0.9655717173387555,\n",
       "   'max_bin': 213.56116699200484,\n",
       "   'max_depth': 8.23185327509497,\n",
       "   'min_child_samples': 196.58522207365755,\n",
       "   'min_child_weight': 17.64224649812914,\n",
       "   'num_leaves': 52.26748775555215,\n",
       "   'reg_alpha': 18.100234609702014,\n",
       "   'reg_lambda': 0.35202387361566295,\n",
       "   'subsample': 0.9275291266202039}},\n",
       " {'target': 7.992138851168561,\n",
       "  'params': {'colsample_bytree': 0.8286267540702462,\n",
       "   'max_bin': 385.1846671311669,\n",
       "   'max_depth': 12.432697904035154,\n",
       "   'min_child_samples': 178.16765795601393,\n",
       "   'min_child_weight': 45.305683154470756,\n",
       "   'num_leaves': 24.416868022358166,\n",
       "   'reg_alpha': 3.737091239759974,\n",
       "   'reg_lambda': 2.447047468388093,\n",
       "   'subsample': 0.5666523762257424}},\n",
       " {'target': 8.004424567294615,\n",
       "  'params': {'colsample_bytree': 0.8489625502048597,\n",
       "   'max_bin': 205.1203929277082,\n",
       "   'max_depth': 15.064977531233142,\n",
       "   'min_child_samples': 44.39142695200357,\n",
       "   'min_child_weight': 22.19245941780866,\n",
       "   'num_leaves': 24.725728110186907,\n",
       "   'reg_alpha': 34.57497870582523,\n",
       "   'reg_lambda': 4.697436829706333,\n",
       "   'subsample': 0.5641110948268239}},\n",
       " {'target': 8.001867692720612,\n",
       "  'params': {'colsample_bytree': 0.5630911528025806,\n",
       "   'max_bin': 104.8937050495689,\n",
       "   'max_depth': 9.425393621078042,\n",
       "   'min_child_samples': 24.34145471856184,\n",
       "   'min_child_weight': 1.127600204282425,\n",
       "   'num_leaves': 47.18425372076874,\n",
       "   'reg_alpha': 9.937199169157793,\n",
       "   'reg_lambda': 4.877047817864935,\n",
       "   'subsample': 0.6459642918325579}},\n",
       " {'target': 7.991538986549537,\n",
       "  'params': {'colsample_bytree': 0.6164036678268915,\n",
       "   'max_bin': 196.70876574127576,\n",
       "   'max_depth': 13.94258671515849,\n",
       "   'min_child_samples': 40.918893780211235,\n",
       "   'min_child_weight': 17.823696879157698,\n",
       "   'num_leaves': 27.717336993922093,\n",
       "   'reg_alpha': 37.328820883580114,\n",
       "   'reg_lambda': 9.547534827759183,\n",
       "   'subsample': 0.7087766229313908}},\n",
       " {'target': 7.994137335015508,\n",
       "  'params': {'colsample_bytree': 0.7675419859658956,\n",
       "   'max_bin': 132.0199627366569,\n",
       "   'max_depth': 14.298915241781739,\n",
       "   'min_child_samples': 40.3924686618707,\n",
       "   'min_child_weight': 44.75925548935725,\n",
       "   'num_leaves': 35.85435623811703,\n",
       "   'reg_alpha': 33.161736698845125,\n",
       "   'reg_lambda': 4.179346930104301,\n",
       "   'subsample': 0.9904558712794339}},\n",
       " {'target': 7.98921599431001,\n",
       "  'params': {'colsample_bytree': 0.5909121889605802,\n",
       "   'max_bin': 151.4451283058841,\n",
       "   'max_depth': 8.12570510974261,\n",
       "   'min_child_samples': 67.70982537785662,\n",
       "   'min_child_weight': 12.179556446382666,\n",
       "   'num_leaves': 57.97492807929662,\n",
       "   'reg_alpha': 5.11327135472758,\n",
       "   'reg_lambda': 2.9634057731939567,\n",
       "   'subsample': 0.5073924938486567}},\n",
       " {'target': 8.014749385657526,\n",
       "  'params': {'colsample_bytree': 0.9254172005439811,\n",
       "   'max_bin': 416.7208648802382,\n",
       "   'max_depth': 10.76574953773526,\n",
       "   'min_child_samples': 126.91007769267303,\n",
       "   'min_child_weight': 26.11112160742515,\n",
       "   'num_leaves': 29.613949340778454,\n",
       "   'reg_alpha': 48.70271304115722,\n",
       "   'reg_lambda': 7.307286495379972,\n",
       "   'subsample': 0.5610568992324143}},\n",
       " {'target': 7.99672422875902,\n",
       "  'params': {'colsample_bytree': 0.5141043094390911,\n",
       "   'max_bin': 254.3407418874025,\n",
       "   'max_depth': 15.332770159742921,\n",
       "   'min_child_samples': 103.21393354412879,\n",
       "   'min_child_weight': 46.90043392010393,\n",
       "   'num_leaves': 32.33623155988221,\n",
       "   'reg_alpha': 12.21846097676822,\n",
       "   'reg_lambda': 9.777330191074164,\n",
       "   'subsample': 0.6988643735876281}},\n",
       " {'target': 7.986388461450105,\n",
       "  'params': {'colsample_bytree': 0.5714645329357839,\n",
       "   'max_bin': 420.3449035602966,\n",
       "   'max_depth': 15.043879875899249,\n",
       "   'min_child_samples': 124.99880880459827,\n",
       "   'min_child_weight': 3.5244762071364786,\n",
       "   'num_leaves': 35.44420386327446,\n",
       "   'reg_alpha': 22.54827686645599,\n",
       "   'reg_lambda': 3.945893499815971,\n",
       "   'subsample': 0.9636697925128256}},\n",
       " {'target': 7.97863800882115,\n",
       "  'params': {'colsample_bytree': 0.579276254301057,\n",
       "   'max_bin': 255.18517768650602,\n",
       "   'max_depth': 13.550001654403957,\n",
       "   'min_child_samples': 12.505066102402521,\n",
       "   'min_child_weight': 29.42773011515404,\n",
       "   'num_leaves': 39.820389083540235,\n",
       "   'reg_alpha': 33.960034049410424,\n",
       "   'reg_lambda': 5.65260345331489,\n",
       "   'subsample': 0.8581227529538189}},\n",
       " {'target': 8.007549383603234,\n",
       "  'params': {'colsample_bytree': 0.6464371516415328,\n",
       "   'max_bin': 212.7951882260217,\n",
       "   'max_depth': 9.075548813868783,\n",
       "   'min_child_samples': 32.06927980062517,\n",
       "   'min_child_weight': 22.79731494793803,\n",
       "   'num_leaves': 47.67936927865554,\n",
       "   'reg_alpha': 48.938112708376934,\n",
       "   'reg_lambda': 5.882906904286742,\n",
       "   'subsample': 0.8768332875895308}},\n",
       " {'target': 7.980198319418749,\n",
       "  'params': {'colsample_bytree': 0.66587818349067,\n",
       "   'max_bin': 318.85670293585764,\n",
       "   'max_depth': 14.696083656856331,\n",
       "   'min_child_samples': 89.28919948696489,\n",
       "   'min_child_weight': 7.6786440348103335,\n",
       "   'num_leaves': 54.40354378771569,\n",
       "   'reg_alpha': 41.984699177480934,\n",
       "   'reg_lambda': 4.397204203719935,\n",
       "   'subsample': 0.6464426489126807}},\n",
       " {'target': 8.008850627579472,\n",
       "  'params': {'colsample_bytree': 0.8911768943126078,\n",
       "   'max_bin': 226.82335694733166,\n",
       "   'max_depth': 9.58652048927745,\n",
       "   'min_child_samples': 45.26117358341472,\n",
       "   'min_child_weight': 38.35804601447657,\n",
       "   'num_leaves': 58.79479371344334,\n",
       "   'reg_alpha': 48.616770778588304,\n",
       "   'reg_lambda': 2.7092744689647326,\n",
       "   'subsample': 0.9831742043197643}},\n",
       " {'target': 7.9999683736445135,\n",
       "  'params': {'colsample_bytree': 0.9318807646090632,\n",
       "   'max_bin': 265.05132830154224,\n",
       "   'max_depth': 8.634833232254739,\n",
       "   'min_child_samples': 55.92302516802116,\n",
       "   'min_child_weight': 4.254390351086849,\n",
       "   'num_leaves': 32.03886139159377,\n",
       "   'reg_alpha': 37.83364228095505,\n",
       "   'reg_lambda': 6.107782167557087,\n",
       "   'subsample': 0.9043824557585871}},\n",
       " {'target': 8.001084702668269,\n",
       "  'params': {'colsample_bytree': 0.8400676823446944,\n",
       "   'max_bin': 218.03126694416892,\n",
       "   'max_depth': 10.828374957564478,\n",
       "   'min_child_samples': 18.53296578103854,\n",
       "   'min_child_weight': 44.08053706636261,\n",
       "   'num_leaves': 48.2828624584605,\n",
       "   'reg_alpha': 31.386355289971423,\n",
       "   'reg_lambda': 5.16125396023365,\n",
       "   'subsample': 0.8320839011056689}},\n",
       " {'target': 8.00399353959004,\n",
       "  'params': {'colsample_bytree': 0.7178480127649094,\n",
       "   'max_bin': 174.6944105583218,\n",
       "   'max_depth': 10.291625851974237,\n",
       "   'min_child_samples': 36.11565074404746,\n",
       "   'min_child_weight': 11.2039887326501,\n",
       "   'num_leaves': 59.424618627709016,\n",
       "   'reg_alpha': 23.53333846758369,\n",
       "   'reg_lambda': 9.779720391939048,\n",
       "   'subsample': 0.562822235810886}},\n",
       " {'target': 7.992143567446837,\n",
       "  'params': {'colsample_bytree': 0.8023267797378124,\n",
       "   'max_bin': 372.2860100070965,\n",
       "   'max_depth': 13.219571070044436,\n",
       "   'min_child_samples': 189.73023488549583,\n",
       "   'min_child_weight': 29.798620436536304,\n",
       "   'num_leaves': 29.276672554717873,\n",
       "   'reg_alpha': 46.44571088697003,\n",
       "   'reg_lambda': 6.261069677877012,\n",
       "   'subsample': 0.5583173239443255}},\n",
       " {'target': 8.002634994010915,\n",
       "  'params': {'colsample_bytree': 0.6120817815183712,\n",
       "   'max_bin': 56.0127776812391,\n",
       "   'max_depth': 13.317409740581677,\n",
       "   'min_child_samples': 86.02488861817797,\n",
       "   'min_child_weight': 2.5349663365974218,\n",
       "   'num_leaves': 43.42729499707772,\n",
       "   'reg_alpha': 12.803004993547578,\n",
       "   'reg_lambda': 3.4478154743231246,\n",
       "   'subsample': 0.5242887645884554}},\n",
       " {'target': 7.994321259369537,\n",
       "  'params': {'colsample_bytree': 0.6732251624430192,\n",
       "   'max_bin': 294.7874607558146,\n",
       "   'max_depth': 15.03557802324086,\n",
       "   'min_child_samples': 117.94571860407643,\n",
       "   'min_child_weight': 26.818627877552583,\n",
       "   'num_leaves': 46.0687492784547,\n",
       "   'reg_alpha': 13.764074735933072,\n",
       "   'reg_lambda': 3.301926621834465,\n",
       "   'subsample': 0.536326218637151}},\n",
       " {'target': 7.984557215921472,\n",
       "  'params': {'colsample_bytree': 0.8190266681526392,\n",
       "   'max_bin': 33.68191484561372,\n",
       "   'max_depth': 14.314513862874048,\n",
       "   'min_child_samples': 81.77454047856418,\n",
       "   'min_child_weight': 30.156229899215923,\n",
       "   'num_leaves': 39.26811414744852,\n",
       "   'reg_alpha': 19.79003701633323,\n",
       "   'reg_lambda': 7.492847592955431,\n",
       "   'subsample': 0.6173422050108449}},\n",
       " {'target': 7.992411960303955,\n",
       "  'params': {'colsample_bytree': 0.7313688492739636,\n",
       "   'max_bin': 322.5275403491443,\n",
       "   'max_depth': 14.673684562656568,\n",
       "   'min_child_samples': 29.114738453076193,\n",
       "   'min_child_weight': 46.030863317824036,\n",
       "   'num_leaves': 47.0053495458919,\n",
       "   'reg_alpha': 44.834265453778464,\n",
       "   'reg_lambda': 5.78718670439186,\n",
       "   'subsample': 0.6073219592234658}},\n",
       " {'target': 8.011117912147114,\n",
       "  'params': {'colsample_bytree': 0.9711690837725098,\n",
       "   'max_bin': 489.01892794129435,\n",
       "   'max_depth': 15.25460690968541,\n",
       "   'min_child_samples': 49.441701762561564,\n",
       "   'min_child_weight': 26.524651705286505,\n",
       "   'num_leaves': 47.23669174089605,\n",
       "   'reg_alpha': 39.94907265936229,\n",
       "   'reg_lambda': 2.888339333527107,\n",
       "   'subsample': 0.8058092781209045}},\n",
       " {'target': 7.995241113707441,\n",
       "  'params': {'colsample_bytree': 0.7875849687909198,\n",
       "   'max_bin': 240.73276061806476,\n",
       "   'max_depth': 13.271184231558605,\n",
       "   'min_child_samples': 133.44942208159318,\n",
       "   'min_child_weight': 33.92950087465731,\n",
       "   'num_leaves': 28.29136702239342,\n",
       "   'reg_alpha': 39.57648860320042,\n",
       "   'reg_lambda': 1.1096346827494934,\n",
       "   'subsample': 0.7514903797268531}},\n",
       " {'target': 7.990347875112145,\n",
       "  'params': {'colsample_bytree': 0.5948136651099992,\n",
       "   'max_bin': 65.54879996738464,\n",
       "   'max_depth': 13.147136974234437,\n",
       "   'min_child_samples': 102.1230118149357,\n",
       "   'min_child_weight': 8.454065086716847,\n",
       "   'num_leaves': 37.71286374411538,\n",
       "   'reg_alpha': 8.495603759047018,\n",
       "   'reg_lambda': 8.3932459613762,\n",
       "   'subsample': 0.7136850800625171}},\n",
       " {'target': 8.004099657215061,\n",
       "  'params': {'colsample_bytree': 0.5543446579385094,\n",
       "   'max_bin': 266.60902033120465,\n",
       "   'max_depth': 12.646496395874314,\n",
       "   'min_child_samples': 147.43609706902322,\n",
       "   'min_child_weight': 13.197385976374632,\n",
       "   'num_leaves': 45.91016035048342,\n",
       "   'reg_alpha': 23.860024134242312,\n",
       "   'reg_lambda': 8.46904710281589,\n",
       "   'subsample': 0.9725842699472822}},\n",
       " {'target': 7.998469381383123,\n",
       "  'params': {'colsample_bytree': 0.6566524967268413,\n",
       "   'max_bin': 433.3057760989586,\n",
       "   'max_depth': 9.490104821932347,\n",
       "   'min_child_samples': 18.647514843077314,\n",
       "   'min_child_weight': 34.592646399786794,\n",
       "   'num_leaves': 40.311724206988984,\n",
       "   'reg_alpha': 9.1208781474659,\n",
       "   'reg_lambda': 5.538063770411299,\n",
       "   'subsample': 0.5814532301163122}},\n",
       " {'target': 7.992004901349113,\n",
       "  'params': {'colsample_bytree': 0.629860556156569,\n",
       "   'max_bin': 460.82332141706166,\n",
       "   'max_depth': 9.509502228957304,\n",
       "   'min_child_samples': 107.51085863131226,\n",
       "   'min_child_weight': 27.23714172016,\n",
       "   'num_leaves': 29.000040122066785,\n",
       "   'reg_alpha': 36.779227385182146,\n",
       "   'reg_lambda': 0.7092204670193314,\n",
       "   'subsample': 0.9752444932102324}}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbBO.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.006492932036826, 8.014310481643134, 7.998092920982338, 7.992138851168561, 8.004424567294615, 8.001867692720612, 7.991538986549537, 7.994137335015508, 7.98921599431001, 8.014749385657526, 7.99672422875902, 7.986388461450105, 7.97863800882115, 8.007549383603234, 7.980198319418749, 8.008850627579472, 7.9999683736445135, 8.001084702668269, 8.00399353959004, 7.992143567446837, 8.002634994010915, 7.994321259369537, 7.984557215921472, 7.992411960303955, 8.011117912147114, 7.995241113707441, 7.990347875112145, 8.004099657215061, 7.998469381383123, 7.992004901349113]\n",
      "maximum target index: 12\n"
     ]
    }
   ],
   "source": [
    "# dictionary에 있는 target값을 모두 추출\n",
    "target_list = []\n",
    "for result in lgbBO.res:\n",
    "    target = result['target']\n",
    "    target_list.append(target)\n",
    "print(target_list)\n",
    "# 가장 큰 target 값을 가지는 순번(index)를 추출\n",
    "print('maximum target index:', np.argmin(np.array(target_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': 7.97863800882115, 'params': {'colsample_bytree': 0.579276254301057, 'max_bin': 255.18517768650602, 'max_depth': 13.550001654403957, 'min_child_samples': 12.505066102402521, 'min_child_weight': 29.42773011515404, 'num_leaves': 39.820389083540235, 'reg_alpha': 33.960034049410424, 'reg_lambda': 5.65260345331489, 'subsample': 0.8581227529538189}}\n"
     ]
    }
   ],
   "source": [
    "# 가장 큰 target값을 가지는 index값을 기준으로 res에서 해당 parameter 추출. \n",
    "max_dict = lgbBO.res[np.argmin(np.array(target_list))]\n",
    "print(max_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftr = DataFrame(X_new)\n",
    "y_train = pd.read_csv(os.path.abspath(\"../input\")+'/y_train.csv', encoding='cp949').age\n",
    "target = y_train\n",
    "target_log = np.log1p(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_apps_all_with_oof(ftr, target, nfolds=5):\n",
    "    ftr = ftr\n",
    "    target = target\n",
    "\n",
    "    # nfolds 개의 cross validatin fold set을 가지는 KFold 생성 \n",
    "    folds = KFold(n_splits=nfolds, shuffle=True, random_state=0)\n",
    "    \n",
    "    # Out of Folds로 학습된 모델의 validation set을 예측하여 결과 확률을 담을 array 생성.\n",
    "    # validation set가 n_split갯수만큼 있으므로 크기는 ftr_app의 크기가 되어야 함. \n",
    "    oof_preds = np.zeros((ftr.shape[0],))  \n",
    "    \n",
    "    # Ouf of Folds로 학습된 모델의 test dataset을 예측하여 결과 확률을 담을 array 생성. \n",
    "    test_preds = np.zeros(((X_te_new.shape[0],)))\n",
    "    \n",
    "    # n_estimators를 4000까지 확대. \n",
    "    clf = LGBMRegressor(\n",
    "                nthread=4,\n",
    "                n_estimators=4000,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=11,\n",
    "                num_leaves=35,\n",
    "                colsample_bytree=0.8385,\n",
    "                subsample=0.760,\n",
    "                max_bin=251,\n",
    "                reg_alpha=33.817,\n",
    "                reg_lambda=8.207,\n",
    "                min_child_weight=26,\n",
    "                min_child_samples=12,\n",
    "                silent=-1,\n",
    "                verbose=-1,\n",
    "                )\n",
    "\n",
    "    # nfolds 번 cross validation Iteration 반복하면서 OOF 방식으로 학습 및 테스트 데이터 예측\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(folds.split(ftr)):\n",
    "        print('##### iteration ', fold_idx, ' 시작')\n",
    "        # 학습용 데이터 세트의 인덱스와 검증용 데이터 세트의 인덱스 추출하여 이를 기반으로 학습/검증 데이터 추출\n",
    "        train_x = ftr.iloc[train_idx, :]\n",
    "        train_y = target.iloc[train_idx]\n",
    "        valid_x = ftr.iloc[valid_idx, :]\n",
    "        valid_y = target.iloc[valid_idx]\n",
    "        \n",
    "        # 추출된 학습/검증 데이터 세트로 모델 학습. early_stopping은 200으로 증가. \n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'RMSE', verbose= 200, \n",
    "                early_stopping_rounds= 200)\n",
    "        # 검증 데이터 세트로 예측된 확률 저장. 사용되지는 않음. \n",
    "        #oof_preds[valid_idx] = clf.predict(valid_x, num_iteration=clf.best_iteration_)       \n",
    "        # 학습된 모델로 테스트 데이터 세트에 예측 확률 계산. \n",
    "        # nfolds 번 반복 실행하므로 평균 확률을 구하기 위해 개별 수행시 마다 수행 횟수로 나눈 확률을 추후에 더해서 최종 평균 확률 계산. \n",
    "        test_preds += clf.predict(X_te_new, num_iteration=clf.best_iteration_)/folds.n_splits\n",
    "        \n",
    "        \n",
    "    return clf, test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### iteration  0  시작\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 7.8261\ttraining's l2: 61.2478\tvalid_1's rmse: 8.35968\tvalid_1's l2: 69.8842\n",
      "[400]\ttraining's rmse: 7.16539\ttraining's l2: 51.3428\tvalid_1's rmse: 8.1451\tvalid_1's l2: 66.3427\n",
      "[600]\ttraining's rmse: 6.73373\ttraining's l2: 45.3431\tvalid_1's rmse: 8.08987\tvalid_1's l2: 65.4461\n",
      "[800]\ttraining's rmse: 6.37603\ttraining's l2: 40.6537\tvalid_1's rmse: 8.06218\tvalid_1's l2: 64.9988\n",
      "[1000]\ttraining's rmse: 6.06347\ttraining's l2: 36.7657\tvalid_1's rmse: 8.05183\tvalid_1's l2: 64.832\n",
      "[1200]\ttraining's rmse: 5.78837\ttraining's l2: 33.5052\tvalid_1's rmse: 8.04658\tvalid_1's l2: 64.7475\n",
      "[1400]\ttraining's rmse: 5.5382\ttraining's l2: 30.6717\tvalid_1's rmse: 8.0442\tvalid_1's l2: 64.7092\n",
      "[1600]\ttraining's rmse: 5.30689\ttraining's l2: 28.1631\tvalid_1's rmse: 8.04275\tvalid_1's l2: 64.6858\n",
      "[1800]\ttraining's rmse: 5.0875\ttraining's l2: 25.8827\tvalid_1's rmse: 8.04221\tvalid_1's l2: 64.6772\n",
      "Early stopping, best iteration is:\n",
      "[1769]\ttraining's rmse: 5.12045\ttraining's l2: 26.219\tvalid_1's rmse: 8.04073\tvalid_1's l2: 64.6533\n",
      "##### iteration  1  시작\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 7.85429\ttraining's l2: 61.6898\tvalid_1's rmse: 8.23988\tvalid_1's l2: 67.8957\n",
      "[400]\ttraining's rmse: 7.18421\ttraining's l2: 51.6128\tvalid_1's rmse: 7.99533\tvalid_1's l2: 63.9254\n",
      "[600]\ttraining's rmse: 6.75026\ttraining's l2: 45.566\tvalid_1's rmse: 7.93119\tvalid_1's l2: 62.9038\n",
      "[800]\ttraining's rmse: 6.39368\ttraining's l2: 40.8791\tvalid_1's rmse: 7.90153\tvalid_1's l2: 62.4342\n",
      "[1000]\ttraining's rmse: 6.09003\ttraining's l2: 37.0885\tvalid_1's rmse: 7.88608\tvalid_1's l2: 62.1902\n",
      "[1200]\ttraining's rmse: 5.81706\ttraining's l2: 33.8382\tvalid_1's rmse: 7.87339\tvalid_1's l2: 61.9903\n",
      "[1400]\ttraining's rmse: 5.5642\ttraining's l2: 30.9604\tvalid_1's rmse: 7.86223\tvalid_1's l2: 61.8146\n",
      "[1600]\ttraining's rmse: 5.32908\ttraining's l2: 28.3991\tvalid_1's rmse: 7.85858\tvalid_1's l2: 61.7573\n",
      "[1800]\ttraining's rmse: 5.10845\ttraining's l2: 26.0963\tvalid_1's rmse: 7.85302\tvalid_1's l2: 61.6699\n",
      "[2000]\ttraining's rmse: 4.90295\ttraining's l2: 24.0389\tvalid_1's rmse: 7.84893\tvalid_1's l2: 61.6056\n",
      "[2200]\ttraining's rmse: 4.70965\ttraining's l2: 22.1808\tvalid_1's rmse: 7.84974\tvalid_1's l2: 61.6184\n",
      "Early stopping, best iteration is:\n",
      "[2067]\ttraining's rmse: 4.83606\ttraining's l2: 23.3875\tvalid_1's rmse: 7.84739\tvalid_1's l2: 61.5815\n",
      "##### iteration  2  시작\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 7.83806\ttraining's l2: 61.4352\tvalid_1's rmse: 8.30881\tvalid_1's l2: 69.0364\n",
      "[400]\ttraining's rmse: 7.16836\ttraining's l2: 51.3853\tvalid_1's rmse: 8.08063\tvalid_1's l2: 65.2965\n",
      "[600]\ttraining's rmse: 6.73211\ttraining's l2: 45.3212\tvalid_1's rmse: 8.02223\tvalid_1's l2: 64.3562\n",
      "[800]\ttraining's rmse: 6.37186\ttraining's l2: 40.6006\tvalid_1's rmse: 7.99944\tvalid_1's l2: 63.991\n",
      "[1000]\ttraining's rmse: 6.06144\ttraining's l2: 36.7411\tvalid_1's rmse: 7.99111\tvalid_1's l2: 63.8578\n",
      "[1200]\ttraining's rmse: 5.7861\ttraining's l2: 33.479\tvalid_1's rmse: 7.98471\tvalid_1's l2: 63.7556\n",
      "[1400]\ttraining's rmse: 5.53532\ttraining's l2: 30.6398\tvalid_1's rmse: 7.9823\tvalid_1's l2: 63.7171\n",
      "[1600]\ttraining's rmse: 5.30105\ttraining's l2: 28.1011\tvalid_1's rmse: 7.981\tvalid_1's l2: 63.6964\n",
      "[1800]\ttraining's rmse: 5.08255\ttraining's l2: 25.8323\tvalid_1's rmse: 7.98172\tvalid_1's l2: 63.7078\n",
      "Early stopping, best iteration is:\n",
      "[1628]\ttraining's rmse: 5.27027\ttraining's l2: 27.7757\tvalid_1's rmse: 7.98022\tvalid_1's l2: 63.684\n",
      "##### iteration  3  시작\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 7.79062\ttraining's l2: 60.6937\tvalid_1's rmse: 8.47864\tvalid_1's l2: 71.8873\n",
      "[400]\ttraining's rmse: 7.13131\ttraining's l2: 50.8556\tvalid_1's rmse: 8.23623\tvalid_1's l2: 67.8355\n",
      "[600]\ttraining's rmse: 6.69963\ttraining's l2: 44.8851\tvalid_1's rmse: 8.16237\tvalid_1's l2: 66.6242\n",
      "[800]\ttraining's rmse: 6.34272\ttraining's l2: 40.2301\tvalid_1's rmse: 8.13566\tvalid_1's l2: 66.1889\n",
      "[1000]\ttraining's rmse: 6.03419\ttraining's l2: 36.4114\tvalid_1's rmse: 8.12499\tvalid_1's l2: 66.0155\n",
      "[1200]\ttraining's rmse: 5.75733\ttraining's l2: 33.1469\tvalid_1's rmse: 8.11756\tvalid_1's l2: 65.8947\n",
      "[1400]\ttraining's rmse: 5.50162\ttraining's l2: 30.2679\tvalid_1's rmse: 8.11529\tvalid_1's l2: 65.8579\n",
      "[1600]\ttraining's rmse: 5.26466\ttraining's l2: 27.7167\tvalid_1's rmse: 8.11013\tvalid_1's l2: 65.7742\n",
      "[1800]\ttraining's rmse: 5.0453\ttraining's l2: 25.4551\tvalid_1's rmse: 8.10539\tvalid_1's l2: 65.6974\n",
      "[2000]\ttraining's rmse: 4.83833\ttraining's l2: 23.4094\tvalid_1's rmse: 8.10301\tvalid_1's l2: 65.6588\n",
      "[2200]\ttraining's rmse: 4.64119\ttraining's l2: 21.5407\tvalid_1's rmse: 8.10039\tvalid_1's l2: 65.6163\n",
      "[2400]\ttraining's rmse: 4.45668\ttraining's l2: 19.862\tvalid_1's rmse: 8.10397\tvalid_1's l2: 65.6744\n",
      "Early stopping, best iteration is:\n",
      "[2210]\ttraining's rmse: 4.63167\ttraining's l2: 21.4524\tvalid_1's rmse: 8.09981\tvalid_1's l2: 65.6069\n",
      "##### iteration  4  시작\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=-1, nthread=4 will be ignored. Current value: num_threads=-1\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's rmse: 7.85209\ttraining's l2: 61.6553\tvalid_1's rmse: 8.32164\tvalid_1's l2: 69.2498\n",
      "[400]\ttraining's rmse: 7.19222\ttraining's l2: 51.728\tvalid_1's rmse: 8.06234\tvalid_1's l2: 65.0014\n",
      "[600]\ttraining's rmse: 6.75778\ttraining's l2: 45.6676\tvalid_1's rmse: 7.98939\tvalid_1's l2: 63.8304\n",
      "[800]\ttraining's rmse: 6.39565\ttraining's l2: 40.9043\tvalid_1's rmse: 7.94712\tvalid_1's l2: 63.1567\n",
      "[1000]\ttraining's rmse: 6.08858\ttraining's l2: 37.0708\tvalid_1's rmse: 7.92799\tvalid_1's l2: 62.8531\n",
      "[1200]\ttraining's rmse: 5.80893\ttraining's l2: 33.7437\tvalid_1's rmse: 7.91695\tvalid_1's l2: 62.6781\n",
      "[1400]\ttraining's rmse: 5.55118\ttraining's l2: 30.8156\tvalid_1's rmse: 7.91041\tvalid_1's l2: 62.5746\n",
      "[1600]\ttraining's rmse: 5.31186\ttraining's l2: 28.2159\tvalid_1's rmse: 7.9052\tvalid_1's l2: 62.4922\n",
      "[1800]\ttraining's rmse: 5.08943\ttraining's l2: 25.9023\tvalid_1's rmse: 7.90351\tvalid_1's l2: 62.4655\n",
      "Early stopping, best iteration is:\n",
      "[1679]\ttraining's rmse: 5.22195\ttraining's l2: 27.2688\tvalid_1's rmse: 7.90323\tvalid_1's l2: 62.461\n"
     ]
    }
   ],
   "source": [
    "clf, test_preds = train_apps_all_with_oof(ftr, target, nfolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42.02959286, 41.67688766, 25.54189981, ..., 36.42954787,\n",
       "       33.38099716, 28.72781476])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDtest['age'] = test_preds ; sub = IDtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submissions_0613_zi_ha_scaled_lgbm_tun.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
